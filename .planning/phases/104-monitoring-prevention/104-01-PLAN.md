---
phase: 104-monitoring-prevention
plan: 01
type: execute
domain: backend
---

<objective>
Add storage size API and historical tracking to enable monitoring of database growth.

Purpose: Provide real disk usage data (not just row counts) and track size over time for trend analysis.
Output: API endpoint for current sizes, StorageSample model, scheduled sampling job.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/103-data-migration-validation/103-01-SUMMARY.md

**Existing infrastructure:**
@src/api/routes/cleanup.py - existing cleanup API with row counts
@src/services/cleanup.py - cleanup service with get_data_stats
@src/db/models/cleanup_run.py - CleanupRun model pattern
@src/scheduling/jobs.py - scheduled job patterns
@src/scheduling/scheduler.py - scheduler configuration

**Key decisions from prior phases:**
- PostgreSQL with SQLAlchemy 2.0 async
- Pydantic v2 with frozen models
- structlog for logging
- APScheduler for scheduled jobs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add storage size API endpoint</name>
  <files>src/api/routes/storage.py, src/api/schemas/storage.py, src/api/app.py</files>
  <action>
Create new storage API module:

1. Create src/api/schemas/storage.py:
   - TableSize: table_name, size_bytes, size_human (formatted like "12.5 GB"), row_count
   - StorageSizes: tables (list[TableSize]), total_bytes, total_human, measured_at

2. Create src/api/routes/storage.py:
   - GET /api/storage/sizes endpoint
   - Query PostgreSQL system tables using raw SQL:
     ```sql
     SELECT
       relname as table_name,
       pg_total_relation_size(quote_ident(relname)) as size_bytes,
       pg_size_pretty(pg_total_relation_size(quote_ident(relname))) as size_human,
       n_live_tup as row_count
     FROM pg_stat_user_tables
     ORDER BY pg_total_relation_size(quote_ident(relname)) DESC
     ```
   - Also get total database size: pg_database_size(current_database())
   - Use text() from sqlalchemy for raw SQL execution
   - Return StorageSizes response

3. Register router in src/api/app.py

Use established patterns:
- AsyncSession dependency injection from src/api/dependencies.py
- structlog for logging
- Pydantic response models
  </action>
  <verify>curl http://localhost:8000/api/storage/sizes returns JSON with table sizes in bytes and human-readable format</verify>
  <done>GET /api/storage/sizes returns table sizes with size_bytes, size_human, row_count for each table, plus database total</done>
</task>

<task type="auto">
  <name>Task 2: Add storage history model and sampling job</name>
  <files>src/db/models/storage_sample.py, src/db/models/__init__.py, src/scheduling/jobs.py, src/scheduling/scheduler.py, alembic/versions/xxx_add_storage_samples.py</files>
  <action>
1. Create src/db/models/storage_sample.py:
   - StorageSample model with:
     - id: primary key
     - sampled_at: datetime with server_default=func.now()
     - total_bytes: int (total database size)
     - table_sizes: JSON (dict mapping table_name -> size_bytes)
   - Follow CleanupRun pattern for model structure

2. Export in src/db/models/__init__.py

3. Create Alembic migration:
   - Run: alembic revision --autogenerate -m "add storage_samples"
   - Verify migration creates storage_samples table correctly

4. Add sample_storage_sizes() job to src/scheduling/jobs.py:
   - Query same pg_total_relation_size as API
   - Create StorageSample record
   - Log sampling completion with structlog
   - Keep last 90 samples (one per day) - delete older

5. Register job in src/scheduling/scheduler.py:
   - Run daily at 3 AM UTC (after cleanup at 2 AM)
   - Use IntervalTrigger with hours=24

6. Add GET /api/storage/history endpoint:
   - Return last N samples (default 30)
   - Include StorageSampleResponse schema
  </action>
  <verify>
1. alembic upgrade head succeeds
2. Manual test: Create sample by calling sample_storage_sizes() directly
3. GET /api/storage/history returns sample records
  </verify>
  <done>
- StorageSample model created and migrated
- Sampling job scheduled for 3 AM daily
- GET /api/storage/history returns historical samples
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] GET /api/storage/sizes returns current table sizes with human-readable format
- [ ] StorageSample table exists in database
- [ ] GET /api/storage/history returns historical samples
- [ ] Sampling job registered in scheduler (check scheduler config)
- [ ] No TypeScript/Python errors
</verification>

<success_criteria>

- All tasks completed
- Storage size API returns accurate disk usage from PostgreSQL
- Historical sampling infrastructure ready for trending
- No errors introduced
</success_criteria>

<output>
After completion, create `.planning/phases/104-monitoring-prevention/104-01-SUMMARY.md`
</output>
