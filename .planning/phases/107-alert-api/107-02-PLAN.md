---
phase: 107-alert-api
plan: 02
type: execute
domain: backend
---

<objective>
Create Pydantic schemas, API endpoints for risk alerts CRUD, and extend write_handler to persist alerts.

Purpose: Complete the alert storage and retrieval layer for the Risk Monitoring feature.
Output: Working API endpoints for alerts with persistence from scraping pipeline.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/105-investigation-schema/DISCOVERY.md
@.planning/phases/107-alert-api/107-01-SUMMARY.md

# Key files:
@src/api/schemas/storage.py (pattern reference)
@src/api/routes/storage.py (pattern reference)
@src/storage/write_queue.py (RiskAlertData DTO)
@src/storage/write_handler.py (extend for alert persistence)
@src/db/models/risk_alert.py (created in Plan 01)

# Prior phase context:
# - Plan 107-01: Created RiskAlert model, AlertStatus enum, Settings additions
# - Phase 106: RiskAlertData DTO defined, detect_risk_alerts() returns list[RiskAlertData]

# Tech stack available:
# - FastAPI with async endpoints
# - Pydantic v2 with ConfigDict, to_camel alias generator
# - SQLAlchemy 2.0 async with select(), desc()

# Established patterns:
# - from_attributes=True for ORM model validation
# - alias_generator=to_camel for frontend compatibility
# - Deferred imports in routes to avoid circular dependencies
# - response_model on route decorators
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Pydantic schemas for risk alerts</name>
  <files>src/api/schemas/alerts.py, src/api/schemas/__init__.py</files>
  <action>
Create new file `src/api/schemas/alerts.py` following storage.py patterns:

1. Import: datetime, BaseModel, ConfigDict, Field, to_camel

2. Create RiskAlertResponse schema (from_attributes=True):
   - id: int
   - event_id: int = Field(alias="eventId")
   - bookmaker_slug: str
   - market_id: str
   - market_name: str
   - line: float | None
   - outcome_name: str | None
   - alert_type: str
   - severity: str
   - change_percent: float
   - old_value: float | None
   - new_value: float | None
   - competitor_direction: str | None
   - detected_at: datetime
   - acknowledged_at: datetime | None
   - status: str
   - event_kickoff: datetime

3. Create RiskAlertsResponse (list response):
   - alerts: list[RiskAlertResponse]
   - total: int
   - new_count: int
   - acknowledged_count: int
   - past_count: int

4. Create RiskAlertStatsResponse (summary statistics):
   - total: int
   - by_status: dict[str, int]  # new, acknowledged, past
   - by_severity: dict[str, int]  # warning, elevated, critical
   - by_type: dict[str, int]  # price_change, direction_disagreement, availability

5. Create AcknowledgeAlertRequest (for PATCH):
   - acknowledged: bool = Field(description="Set to true to acknowledge alert")

6. Update __init__.py to export all schemas

Include PEP 257 docstrings for all classes.
  </action>
  <verify>python -c "from src.api.schemas.alerts import RiskAlertResponse, RiskAlertsResponse, RiskAlertStatsResponse; print('Schemas OK')"</verify>
  <done>alerts.py created with RiskAlertResponse, RiskAlertsResponse, RiskAlertStatsResponse, AcknowledgeAlertRequest schemas, exported from __init__.py</done>
</task>

<task type="auto">
  <name>Task 2: Create alerts API routes</name>
  <files>src/api/routes/alerts.py, src/api/routes/__init__.py, src/api/main.py</files>
  <action>
Create new file `src/api/routes/alerts.py` with FastAPI router:

1. Create router: APIRouter(prefix="/alerts", tags=["alerts"])

2. GET /alerts - List alerts with filters:
   - Query params: status (Optional[str]), severity (Optional[str]), event_id (Optional[int]), limit (int=50), offset (int=0)
   - Returns RiskAlertsResponse with counts by status
   - Order by detected_at DESC
   - Use deferred import for RiskAlert model

3. GET /alerts/stats - Get summary statistics:
   - Returns RiskAlertStatsResponse
   - Group by status, severity, alert_type
   - Use SQLAlchemy func.count() with group_by

4. GET /alerts/{alert_id} - Get single alert:
   - Returns RiskAlertResponse
   - Raise HTTPException 404 if not found

5. PATCH /alerts/{alert_id} - Acknowledge/unacknowledge alert:
   - Body: AcknowledgeAlertRequest
   - If acknowledged=True: set status="acknowledged", acknowledged_at=now
   - If acknowledged=False: set status="new", acknowledged_at=None
   - Return updated RiskAlertResponse
   - Raise HTTPException 404 if not found, 400 if alert is PAST

6. GET /events/{event_id}/alerts - Alerts for specific event (add to events.py):
   - Returns RiskAlertsResponse filtered by event_id
   - Include in events router at /events/{event_id}/alerts

7. Register router in src/api/routes/__init__.py
8. Include router in src/api/main.py

Follow existing route patterns from storage.py.
  </action>
  <verify>curl -s http://localhost:8000/api/alerts | head -50 (or check via Swagger at /docs)</verify>
  <done>Alerts API routes created: GET /alerts, GET /alerts/stats, GET /alerts/{id}, PATCH /alerts/{id}, plus /events/{id}/alerts</done>
</task>

<task type="auto">
  <name>Task 3: Extend WriteBatch and write_handler for alert persistence</name>
  <files>src/storage/write_queue.py, src/storage/write_handler.py, src/scraping/event_coordinator.py</files>
  <action>
1. Update WriteBatch in write_queue.py:
   Add alerts field after unavailable_competitor:
   ```python
   alerts: tuple[RiskAlertData, ...] = ()
   ```

2. Update handle_write_batch() in write_handler.py:
   - Import RiskAlert from src.db.models.risk_alert
   - After processing unavailable markets, add alert persistence:
   ```python
   # --- Step 7: INSERT risk alerts ---
   if batch.alerts:
       for alert_data in batch.alerts:
           alert = RiskAlert(
               event_id=alert_data.event_id,
               bookmaker_slug=alert_data.bookmaker_slug,
               market_id=alert_data.market_id,
               market_name=alert_data.market_name,
               line=alert_data.line,
               outcome_name=alert_data.outcome_name,
               alert_type=alert_data.alert_type,
               severity=alert_data.severity,
               change_percent=alert_data.change_percent,
               old_value=alert_data.old_value,
               new_value=alert_data.new_value,
               competitor_direction=alert_data.competitor_direction,
               detected_at=alert_data.detected_at,
               event_kickoff=alert_data.event_kickoff,
               status="new",
           )
           session.add(alert)
       alerts_inserted = len(batch.alerts)
   else:
       alerts_inserted = 0
   ```
   - Add alerts_inserted to returned stats dict

3. Update store_batch_results() in event_coordinator.py:
   - After detect_risk_alerts() call, pass alerts to WriteBatch:
   ```python
   write_batch = WriteBatch(
       # ... existing fields ...
       alerts=tuple(detected_alerts),
   )
   ```
   - Log alert count in the batch enqueue log message

4. Update _process_with_retry logging in write_queue.py to include alerts_inserted if present.
  </action>
  <verify>Run scraper, check logs for "alerts_inserted" in write_batch_processed messages</verify>
  <done>WriteBatch has alerts field, write_handler persists RiskAlert rows, event_coordinator passes detected alerts to write batch</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] GET /api/alerts returns empty list (no alerts yet)
- [ ] GET /api/alerts/stats returns zero counts
- [ ] Trigger a scrape cycle (or wait for scheduled scrape)
- [ ] Verify alerts appear in GET /api/alerts after scrape with large odds movements
- [ ] PATCH /api/alerts/{id} successfully acknowledges an alert
- [ ] Database has risk_alerts table with inserted rows
</verification>

<success_criteria>

- Pydantic schemas created for alert responses
- API endpoints working: GET /alerts, GET /alerts/stats, GET /alerts/{id}, PATCH /alerts/{id}
- WriteBatch extended with alerts field
- write_handler persists RiskAlert rows from RiskAlertData DTOs
- event_coordinator passes alerts from detect_risk_alerts() to write batch
- End-to-end flow: scrape detects alerts → enqueue → persist → API returns
</success_criteria>

<output>
After completion, create `.planning/phases/107-alert-api/107-02-SUMMARY.md`
</output>
