---
phase: 42-validation-cleanup
plan: 01
type: execute
domain: scraping
---

<objective>
Integrate EventCoordinator into scheduler and API, remove legacy ScrapingOrchestrator

Purpose: Complete the v1.7 Scraping Architecture Overhaul by switching production scraping to the new event-centric EventCoordinator and removing the legacy sequential platform-by-platform ScrapingOrchestrator.
Output: Scheduler and API use EventCoordinator.run_full_cycle(), legacy orchestrator removed.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase summaries (v1.7 Scraping Architecture)
@.planning/phases/36-investigation-architecture-design/36-01-SUMMARY.md
@.planning/phases/37-event-coordination-layer/37-01-SUMMARY.md
@.planning/phases/38-sr-id-parallel-scraping/38-01-SUMMARY.md
@.planning/phases/39-batch-db-storage/39-01-SUMMARY.md
@.planning/phases/40-concurrency-tuning-metrics/40-01-SUMMARY.md
@.planning/phases/41-on-demand-api/41-01-SUMMARY.md

# Key source files
@src/scheduling/jobs.py
@src/api/routes/scrape.py
@src/scraping/event_coordinator.py
@src/scraping/orchestrator.py

**Architecture context:**
- **EventCoordinator** (new): Event-centric parallel scraping, all platforms simultaneously per event
  - `discover_events()` -> parallel discovery from all platforms
  - `build_priority_queue()` -> kickoff urgency + coverage based sorting
  - `scrape_batch()` -> parallel platform scraping per event
  - `store_batch_results()` -> bulk DB inserts
  - `run_full_cycle()` -> orchestrates complete scrape cycle, yields SSE-compatible progress events
  - `from_settings()` -> factory method for configurable initialization

- **ScrapingOrchestrator** (legacy): Sequential platform-by-platform scraping
  - BetPawa completes fully, then SportyBet, then Bet9ja
  - Events scraped minutes apart across platforms
  - Used by: jobs.py:scrape_all_platforms(), scrape.py:trigger_scrape()

**Key differences:**
| Aspect | Legacy (ScrapingOrchestrator) | New (EventCoordinator) |
|--------|------------------------------|------------------------|
| Execution | Sequential by platform | Parallel all platforms per event |
| Progress | OrchestratorProgress objects | Dict-based SSE events |
| Timing gap | Minutes between platforms | Simultaneous (~ms gap) |
| Status tracking | ScrapeRun only | ScrapeRun + EventScrapeStatus |

**Established patterns:**
- EventCoordinator.from_settings() for configurable tuning
- run_full_cycle() yields dict progress events (CYCLE_START, DISCOVERY_COMPLETE, etc.)
- TournamentDiscoveryService still needed for discovering new tournaments
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update scheduler to use EventCoordinator</name>
  <files>src/scheduling/jobs.py</files>
  <action>
Replace ScrapingOrchestrator with EventCoordinator in `scrape_all_platforms()`:

1. **Update imports:**
   ```python
   # Remove:
   from src.scraping.orchestrator import ScrapingOrchestrator

   # Add:
   from src.scraping.event_coordinator import EventCoordinator
   ```

2. **Replace orchestrator creation and execution (lines ~122-175):**

   Current code uses:
   ```python
   orchestrator = ScrapingOrchestrator(
       sportybet_client=sportybet_client,
       betpawa_client=betpawa_client,
       bet9ja_client=bet9ja_client,
       competitor_service=competitor_service,
   )
   async for progress in orchestrator.scrape_with_progress(...):
       await broadcaster.publish(progress)
   ```

   Replace with:
   ```python
   # Create EventCoordinator from settings
   coordinator = EventCoordinator.from_settings(
       betpawa_client=betpawa_client,
       sportybet_client=sportybet_client,
       bet9ja_client=bet9ja_client,
       settings=settings,
   )

   # Execute scrape with progress streaming
   async for progress_event in coordinator.run_full_cycle(
       db=db,
       scrape_run_id=scrape_run_id,
   ):
       # Convert dict events to ScrapeProgress for broadcaster compatibility
       # Or update broadcaster to handle dict events directly
       from src.scraping.schemas import ScrapeProgress

       event_type = progress_event.get("event_type", "")

       if event_type == "CYCLE_START":
           await broadcaster.publish(ScrapeProgress(
               platform=None,
               phase="starting",
               current=0,
               total=3,
               message="Starting event-centric scrape cycle",
           ))
       elif event_type == "DISCOVERY_COMPLETE":
           total = progress_event.get("total_events", 0)
           await broadcaster.publish(ScrapeProgress(
               platform=None,
               phase="discovery",
               current=1,
               total=3,
               message=f"Discovered {total} events across all platforms",
               events_count=total,
           ))
       elif event_type == "BATCH_COMPLETE":
           processed = progress_event.get("total_processed", 0)
           queue_size = progress_event.get("queue_remaining", 0) + processed
           await broadcaster.publish(ScrapeProgress(
               platform=None,
               phase="scraping",
               current=2,
               total=3,
               message=f"Processed {processed}/{queue_size} events",
               events_count=processed,
           ))
       elif event_type == "CYCLE_COMPLETE":
           stats = progress_event.get("stats", {})
           total_events = stats.get("total_events", 0)
           await broadcaster.publish(ScrapeProgress(
               platform=None,
               phase="completed",
               current=3,
               total=3,
               message=f"Completed: {total_events} events scraped",
               events_count=total_events,
           ))
           # Update metrics for scrape_run
           total_events = stats.get("total_events", 0)
   ```

3. **Update ScrapeRun completion:**
   - Extract stats from CYCLE_COMPLETE event
   - Map to existing ScrapeRun fields (events_scraped, events_failed, platform_timings)

4. **Keep TournamentDiscoveryService** call before EventCoordinator - it handles discovering NEW tournaments (EventCoordinator discovers events within known tournaments).

5. **Remove CompetitorEventScrapingService** creation - EventCoordinator replaces this too.
  </action>
  <verify>python -c "from src.scheduling.jobs import scrape_all_platforms; print('Import OK')"</verify>
  <done>jobs.py uses EventCoordinator, imports successfully</done>
</task>

<task type="auto">
  <name>Task 2: Update API trigger_scrape to use EventCoordinator</name>
  <files>src/api/routes/scrape.py</files>
  <action>
Update POST /scrape endpoint to use EventCoordinator:

1. **Update imports:**
   ```python
   # Remove:
   from src.scraping.orchestrator import ScrapingOrchestrator

   # Add:
   from src.scraping.event_coordinator import EventCoordinator
   from src.db.models.settings import Settings
   ```

2. **In trigger_scrape() function (~lines 55-100):**

   Replace:
   ```python
   orchestrator = ScrapingOrchestrator(sportybet, betpawa, bet9ja)
   result = await orchestrator.scrape_all(...)
   ```

   With:
   ```python
   # Get settings for EventCoordinator tuning
   settings_result = await db.execute(select(Settings).where(Settings.id == 1))
   settings = settings_result.scalar_one_or_none()

   # Create coordinator
   coordinator = EventCoordinator.from_settings(
       betpawa_client=betpawa,
       sportybet_client=sportybet,
       bet9ja_client=bet9ja,
       settings=settings,
   )

   # Run cycle and collect results
   total_events = 0
   async for progress in coordinator.run_full_cycle(db=db, scrape_run_id=scrape_run_id):
       if progress.get("event_type") == "CYCLE_COMPLETE":
           total_events = progress.get("stats", {}).get("total_events", 0)
   ```

3. **Update scrape_with_progress endpoint if it uses ScrapingOrchestrator** (check lines ~200-300).

4. **Keep remaining routes that don't use orchestrator** (scrape_single_event already uses inline logic).
  </action>
  <verify>python -c "from src.api.routes.scrape import router; print('Router OK')"</verify>
  <done>scrape.py routes use EventCoordinator where applicable</done>
</task>

<task type="auto">
  <name>Task 3: Remove legacy ScrapingOrchestrator</name>
  <files>src/scraping/orchestrator.py, src/scraping/__init__.py</files>
  <action>
Remove the legacy orchestrator file:

1. **Delete orchestrator.py:**
   - Remove `src/scraping/orchestrator.py` entirely
   - This file is no longer used after Tasks 1-2

2. **Update scraping package exports (if any):**
   - Check `src/scraping/__init__.py` for ScrapingOrchestrator exports
   - Remove any references

3. **Search for any remaining imports:**
   - Grep for `from src.scraping.orchestrator` or `import orchestrator`
   - Fix any remaining references (should be none after Tasks 1-2)

4. **Verify no runtime errors:**
   - Run `python -c "from src.api.routes.scrape import router; from src.scheduling.jobs import scrape_all_platforms; print('All imports clean')"
  </action>
  <verify>
1. python -c "import os; assert not os.path.exists('src/scraping/orchestrator.py'), 'File still exists'"
2. python -c "from src.api.routes.scrape import router; from src.scheduling.jobs import scrape_all_platforms; print('Clean')"
  </verify>
  <done>orchestrator.py deleted, no remaining references, all imports clean</done>
</task>

<task type="auto">
  <name>Task 4: Clean up unused imports and dependencies</name>
  <files>src/scheduling/jobs.py, src/api/routes/scrape.py</files>
  <action>
Remove unused imports after orchestrator removal:

1. **In jobs.py:**
   - Remove `from src.scraping.competitor_events import CompetitorEventScrapingService` if no longer used
   - Verify all remaining imports are actually used

2. **In scrape.py:**
   - Remove any unused imports (OrchestratorProgress, etc.)
   - Verify all imports are actually used

3. **Run linter check:**
   - `ruff check src/scheduling/jobs.py src/api/routes/scrape.py --select F401`
   - Fix any unused import warnings
  </action>
  <verify>ruff check src/scheduling/jobs.py src/api/routes/scrape.py --select F401 --ignore E501 || echo "Ruff not installed, skipping lint"</verify>
  <done>No unused imports in modified files</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `python -c "from src.scheduling.jobs import scrape_all_platforms"` succeeds
- [ ] `python -c "from src.api.routes.scrape import router"` succeeds
- [ ] `src/scraping/orchestrator.py` no longer exists
- [ ] No grep hits for `ScrapingOrchestrator` in src/ directory
- [ ] No grep hits for `from src.scraping.orchestrator` in src/ directory
</verification>

<success_criteria>

- EventCoordinator used by scheduler's scrape_all_platforms()
- EventCoordinator used by API's trigger_scrape() endpoint
- Legacy ScrapingOrchestrator file deleted
- No remaining imports or references to orchestrator
- All imports clean, no runtime errors
</success_criteria>

<output>
After completion, create `.planning/phases/42-validation-cleanup/42-01-SUMMARY.md`:

# Phase 42 Plan 01: Validation & Cleanup Summary

**[Substantive one-liner - what shipped]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `path/to/file.ts` - Description

## Files Deleted

- `src/scraping/orchestrator.py` - Legacy sequential orchestrator

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Milestone Completion

v1.7 Scraping Architecture Overhaul COMPLETE - Ready to ship!
</output>
