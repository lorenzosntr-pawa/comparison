---
phase: 42-validation-cleanup
plan: 01-FIX
type: fix
---

<objective>
Fix 3 UAT issues from plan 42-01 discovered during verify-work.

Source: 42-01-ISSUES.md
Priority: 2 blockers, 1 major
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md

**Issues being fixed:**
@.planning/phases/42-validation-cleanup/42-01-ISSUES.md

**Original plan for reference:**
@.planning/phases/42-validation-cleanup/42-01-PLAN.md

**Key files:**
@src/scheduling/jobs.py
@src/scraping/event_coordinator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix UAT-001 - Remove redundant ScrapeProgress import</name>
  <files>src/scheduling/jobs.py</files>
  <action>
Remove the redundant local import at line 208:
```python
from src.scraping.schemas import ScrapeProgress
```

This import inside the exception handler causes Python to treat all references to `ScrapeProgress` in the function as local variables. Since `ScrapeProgress` is already imported at module level (line 15), the local import is unnecessary and creates an UnboundLocalError when the variable is referenced before the import executes.

Simply delete line 208. The module-level import at line 15 will be used instead.
  </action>
  <verify>
1. Run: `python -c "from src.scheduling.jobs import scrape_all_platforms"`
2. No import errors should occur
3. Grep for `from src.scraping.schemas import ScrapeProgress` in jobs.py - should only appear once at top
  </verify>
  <done>
- Line 208 removed
- Only one import of ScrapeProgress at module level
- No Python import/syntax errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix UAT-002 - Debug and fix BetPawa event discovery</name>
  <files>src/scraping/event_coordinator.py</files>
  <action>
The BetPawa discovery (`_discover_betpawa`) returns 0 events while SportyBet and Bet9ja work fine.

Investigation steps:
1. Add info-level logging to trace the discovery flow:
   - Log the raw response from `fetch_categories()` structure
   - Log whether `regions` key exists and its length
   - Log competition IDs found
   - Log events found per competition

2. Check if the API response structure has changed:
   - The code expects `data["regions"][n]["competitions"][m]["id"]`
   - The BetPawa API might return a different structure

3. Common issues to check:
   - `regions` key might be at a different path in the response
   - Competition structure might differ
   - The `fetch_events` response structure with `eventLists` might have changed

Likely fix: Adjust the response parsing to match actual BetPawa API structure. Add defensive logging at each step to identify where the chain breaks.

Change logger.debug calls to logger.info for discovery steps so they're visible in production logs.
  </action>
  <verify>
1. Trigger a scrape and check logs for BetPawa discovery messages
2. Look for log lines showing:
   - "Discovering BetPawa events"
   - "Found BetPawa competitions" with count > 0
   - "Discovered BetPawa events" with count > 0
3. Check "Event discovery complete" shows betpawa > 0
  </verify>
  <done>
- BetPawa discovery returns > 0 events
- Logs show discovery flow progressing
- "Event discovery complete" log shows betpawa count matching bet9ja/sportybet
  </done>
</task>

<task type="auto">
  <name>Task 3: Fix UAT-003 - Optimize batch storage performance</name>
  <files>src/scraping/event_coordinator.py</files>
  <action>
Current batch storage takes 30-40 seconds per 50 events. The bottleneck is in `store_batch_results`:

1. **Individual flush calls** (lines 1017, 1025):
   ```python
   await db.flush()  # Get snapshot ID - called per snapshot!
   ```
   With ~100 snapshots per batch, this is ~100 round trips.

2. **Fix approach - Use add_all and single flush**:
   - Collect all status_records, add them with `db.add_all(status_records)`
   - For snapshots, use a single flush after adding all snapshots, then use returned IDs
   - Or better: use SQLAlchemy's `returning` clause with bulk inserts

3. **Simpler optimization (lower risk)**:
   - Use `db.add_all()` for status_records instead of loop
   - Move the commit to after all adds are done (already done)
   - Consider using `executemany` pattern for markets

4. **Quick win**: Change flush pattern to batch flushes:
   - Add all snapshots first
   - Single flush to get all IDs
   - Then add all markets with snapshot IDs
   - Single commit at end

Target: < 10 seconds per batch (3-4x improvement)
  </action>
  <verify>
1. Trigger a scrape
2. Observe "Batch storage complete" log timestamps
3. Time between consecutive batches should be < 15 seconds (was 30-40)
4. No errors in batch storage
  </verify>
  <done>
- Batch storage completes in < 15 seconds
- No increase in errors
- All data still stored correctly
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] UAT-001: No UnboundLocalError when scraping
- [ ] UAT-002: BetPawa events discovered (betpawa > 0 in logs)
- [ ] UAT-003: Batch processing faster (< 15s per batch)
- [ ] Full scrape completes without errors
- [ ] All platforms scraped successfully
</verification>

<success_criteria>
- All 3 UAT issues from 42-01-ISSUES.md addressed
- Scrape runs complete without errors
- Ready for re-verification with /gsd:verify-work
</success_criteria>

<output>
After completion, create `.planning/phases/42-validation-cleanup/42-01-FIX-SUMMARY.md`

Update 42-01-ISSUES.md to move fixed issues to "Resolved Issues" section.
</output>
