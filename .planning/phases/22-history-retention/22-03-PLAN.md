---
phase: 22-history-retention
plan: 03
type: execute
---

<objective>
Add scheduled cleanup job to APScheduler and create REST API endpoints for cleanup operations.

Purpose: Enable automatic background cleanup on configurable schedule and provide API for manual cleanup control.
Output: Cleanup scheduler job and /api/cleanup endpoints.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/22-history-retention/22-CONTEXT.md
@.planning/phases/22-history-retention/22-01-PLAN.md
@.planning/phases/22-history-retention/22-02-PLAN.md

# Key files
@src/scheduling/scheduler.py
@src/scheduling/jobs.py
@src/api/routes/settings.py

**From 22-CONTEXT.md:**
- Scheduled cleanup job runs on configurable frequency (cleanup_frequency_hours)
- Wait for any active scraping to finish before running
- Shows status indicator when cleanup is in progress
- Detailed logging of what was deleted

**Established patterns:**
- APScheduler with IntervalTrigger for scheduled jobs
- sync_settings_on_startup pattern for loading DB settings
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add cleanup scheduler job with scraping coordination</name>
  <files>src/scheduling/scheduler.py, src/scheduling/jobs.py</files>
  <action>
Update src/scheduling/jobs.py - add cleanup job function:
```python
async def cleanup_old_data():
    """Scheduled cleanup job that removes old data based on retention settings."""
    logger = structlog.get_logger()

    # Check if scraping is in progress - wait or skip
    if is_scraping_active():
        logger.info("cleanup_skipped", reason="scraping_in_progress")
        return

    async with async_session_factory() as session:
        # Get current settings
        settings = await get_settings(session)

        # Execute cleanup
        from src.services.cleanup import execute_cleanup
        result = await execute_cleanup(
            session,
            odds_days=settings.odds_retention_days,
            match_days=settings.match_retention_days
        )

        logger.info("cleanup_completed",
            odds_deleted=result.odds_deleted,
            matches_deleted=result.matches_deleted,
            tournaments_deleted=result.tournaments_deleted
        )
```

Add scraping status tracking to jobs.py:
- Add module-level `_scraping_active = False` flag
- Set True at start of scrape_all_platforms, False at end (in finally block)
- Add `is_scraping_active()` function

Update src/scheduling/scheduler.py:
1. Add DEFAULT_CLEANUP_HOURS = 24 constant
2. In configure_scheduler(), add cleanup job:
```python
scheduler.add_job(
    cleanup_old_data,
    trigger=IntervalTrigger(hours=DEFAULT_CLEANUP_HOURS),
    id="cleanup_old_data",
    replace_existing=True,
    misfire_grace_time=3600,  # 1 hour grace for cleanup
    coalesce=True,
)
```

3. Add update_cleanup_interval(hours: int) function similar to update_scheduler_interval

4. Update sync_settings_on_startup to also sync cleanup frequency from DB
  </action>
  <verify>python -m py_compile src/scheduling/scheduler.py && python -m py_compile src/scheduling/jobs.py</verify>
  <done>Cleanup job added to scheduler with configurable frequency. Scraping coordination prevents conflicts.</done>
</task>

<task type="auto">
  <name>Task 2: Create cleanup API endpoints</name>
  <files>src/api/routes/cleanup.py, src/api/app.py</files>
  <action>
Create src/api/routes/cleanup.py with endpoints:

1. GET /api/cleanup/stats
   - Returns DataStats from cleanup service
   - No parameters

2. GET /api/cleanup/preview
   - Query params: odds_days (optional), match_days (optional)
   - If not provided, use current settings values
   - Returns CleanupPreview

3. POST /api/cleanup/execute
   - Body: { oddsRetentionDays?: number, matchRetentionDays?: number }
   - If not provided, use current settings values
   - Executes cleanup and returns CleanupResult
   - Records as "manual" trigger in cleanup_runs

4. GET /api/cleanup/history
   - Query params: limit (default 10)
   - Returns list of recent CleanupRun records

5. GET /api/cleanup/status
   - Returns { isCleanupRunning: boolean, isScrapingActive: boolean }
   - Used by frontend for status indicators

Register router in src/api/app.py:
```python
from src.api.routes.cleanup import router as cleanup_router
app.include_router(cleanup_router, prefix="/api/cleanup", tags=["cleanup"])
```

Add Pydantic response models to src/api/schemas/cleanup.py:
- CleanupStatsResponse, CleanupPreviewResponse, CleanupExecuteResponse
- CleanupHistoryResponse (list of runs)
- CleanupStatusResponse
  </action>
  <verify>python -m py_compile src/api/routes/cleanup.py</verify>
  <done>Cleanup API endpoints created and registered. All CRUD operations available.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Cleanup job added to APScheduler with configurable interval
- [ ] Scraping coordination prevents cleanup during active scraping
- [ ] sync_settings_on_startup syncs cleanup frequency
- [ ] All 5 API endpoints created and functional
- [ ] Router registered in app.py
- [ ] Response schemas match frontend expectations (camelCase)
- [ ] All Python files compile without errors
</verification>

<success_criteria>

- Cleanup runs automatically on schedule from settings
- Manual cleanup available via API
- Status endpoint shows cleanup/scraping activity
- History endpoint returns past cleanup runs
- No conflicts between scraping and cleanup jobs
</success_criteria>

<output>
After completion, create `.planning/phases/22-history-retention/22-03-SUMMARY.md`
</output>
