---
phase: 22-history-retention
plan: 02
type: execute
---

<objective>
Create cleanup service with data analysis and deletion capabilities, plus cleanup run history tracking.

Purpose: Backend logic to analyze data usage, preview what would be deleted, and execute cleanup with detailed logging.
Output: Cleanup service module and cleanup_runs database table.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/22-history-retention/22-CONTEXT.md
@.planning/phases/22-history-retention/22-01-PLAN.md

# Key files for understanding data model
@src/db/models/odds.py
@src/db/models/match.py
@src/db/models/scrape_run.py
@src/db/models/tournament.py
@src/db/models/settings.py

**From 22-CONTEXT.md:**
- odds_retention_days: Delete odds snapshots older than X days (by snapshot timestamp). Also applies to scrape run history.
- match_retention_days: Delete matches older than Y days (by kickoff time). Cascades to their odds.
- Tournaments auto-delete when they have no remaining matches
- Batch deletion to avoid long locks
- Detailed logging: records deleted per table, date range cleaned
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create cleanup service with stats/preview/execute</name>
  <files>src/services/cleanup.py</files>
  <action>
Create src/services/cleanup.py with async functions:

1. `async def get_data_stats(session: AsyncSession) -> DataStats`:
   - Count records per table (odds_snapshots, matches, tournaments, scrape_runs)
   - Get date ranges (oldest/newest record per table)
   - Group counts by platform where applicable
   - Return structured DataStats object

2. `async def preview_cleanup(session: AsyncSession, odds_days: int, match_days: int) -> CleanupPreview`:
   - Calculate cutoff dates: now - odds_days, now - match_days
   - Count odds_snapshots older than odds cutoff
   - Count scrape_runs older than odds cutoff
   - Count matches with kickoff older than match cutoff
   - Count orphaned tournaments (would have no matches after cleanup)
   - Return CleanupPreview with counts and affected date ranges

3. `async def execute_cleanup(session: AsyncSession, odds_days: int, match_days: int) -> CleanupResult`:
   - Delete in correct order to respect foreign keys:
     a. Delete odds_snapshots older than odds cutoff (batch: 1000 at a time)
     b. Delete scrape_runs older than odds cutoff (batch)
     c. Delete matches older than match cutoff (cascades to their odds)
     d. Delete tournaments with no remaining matches
   - Log each step with structlog
   - Return CleanupResult with deleted counts per table

Use batch deletion pattern:
```python
while True:
    subq = select(Model.id).where(condition).limit(1000)
    result = await session.execute(delete(Model).where(Model.id.in_(subq)))
    if result.rowcount == 0:
        break
    await session.commit()
    total_deleted += result.rowcount
```

Define Pydantic models in src/api/schemas/cleanup.py:
- DataStats, CleanupPreview, CleanupResult with appropriate fields
  </action>
  <verify>python -m py_compile src/services/cleanup.py</verify>
  <done>Cleanup service created with get_data_stats, preview_cleanup, execute_cleanup functions. Schema models defined.</done>
</task>

<task type="auto">
  <name>Task 2: Add cleanup run history table</name>
  <files>src/db/models/cleanup_run.py, src/db/models/__init__.py, alembic/versions/xxxx_add_cleanup_runs.py</files>
  <action>
Create src/db/models/cleanup_run.py:
```python
class CleanupRun(Base):
    __tablename__ = "cleanup_runs"

    id: Mapped[int] = mapped_column(primary_key=True)
    started_at: Mapped[datetime] = mapped_column(server_default=func.now())
    completed_at: Mapped[datetime | None] = mapped_column(nullable=True)
    trigger: Mapped[str] = mapped_column()  # "scheduled" or "manual"

    # Settings used
    odds_retention_days: Mapped[int]
    match_retention_days: Mapped[int]

    # Results
    odds_deleted: Mapped[int] = mapped_column(default=0)
    scrape_runs_deleted: Mapped[int] = mapped_column(default=0)
    matches_deleted: Mapped[int] = mapped_column(default=0)
    tournaments_deleted: Mapped[int] = mapped_column(default=0)

    # Date range cleaned
    oldest_odds_date: Mapped[datetime | None] = mapped_column(nullable=True)
    oldest_match_date: Mapped[datetime | None] = mapped_column(nullable=True)

    status: Mapped[str] = mapped_column(default="running")  # running, completed, failed
    error_message: Mapped[str | None] = mapped_column(nullable=True)
```

Update src/db/models/__init__.py to export CleanupRun.

Create migration for cleanup_runs table.

Update execute_cleanup to create CleanupRun record before starting and update it on completion.
  </action>
  <verify>python -m py_compile src/db/models/cleanup_run.py</verify>
  <done>CleanupRun model created, migration ready, cleanup service records run history.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Cleanup service module exists with all three functions
- [ ] DataStats, CleanupPreview, CleanupResult schemas defined
- [ ] CleanupRun model created with all tracking fields
- [ ] Migration for cleanup_runs table ready
- [ ] execute_cleanup creates and updates CleanupRun records
- [ ] Batch deletion pattern implemented to avoid long locks
- [ ] All Python files compile without errors
</verification>

<success_criteria>

- Cleanup service provides data analysis, preview, and execution
- Cleanup runs are tracked in database with full details
- Batch deletion prevents long database locks
- All functions properly async with session handling
</success_criteria>

<output>
After completion, create `.planning/phases/22-history-retention/22-02-SUMMARY.md`
</output>
