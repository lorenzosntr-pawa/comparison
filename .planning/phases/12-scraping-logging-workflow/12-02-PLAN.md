---
phase: 14-scraping-logging-workflow
plan: 02
type: execute
---

<objective>
Enhance scrape progress streaming with granular phase tracking, structured logging, and persistent state.

Purpose: Provide real-time visibility into scrape workflow with audit trail and structured logs for debugging.
Output: Enhanced ScrapeProgress schema, orchestrator with phase emissions, structured logging throughout, state persistence.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-scraping-logging-workflow/14-CONTEXT.md
@.planning/phases/14-scraping-logging-workflow/14-RESEARCH.md

# Key source files:
@src/scraping/orchestrator.py
@src/scraping/schemas.py
@src/db/models/scrape.py

# Prior plan in this phase:
@.planning/phases/14-scraping-logging-workflow/14-01-SUMMARY.md

**Tech stack available:**
- structlog (from 14-01)
- ScrapePhase, PlatformStatus enums (from 14-01)
- ScrapePhaseLog model (from 14-01)

**Established patterns:**
- AsyncGenerator for SSE streaming
- asyncio.gather with return_exceptions=True
- contextvars for async context propagation

**Key insight from RESEARCH.md:**
- Update DB state BEFORE broadcasting SSE (avoids race conditions)
- Bind context explicitly at start of each scrape job (APScheduler runs in different context)
- Log phase transitions with structured data, not just strings
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enhance ScrapeProgress schema with rich phase/error data</name>
  <files>src/scraping/schemas.py, src/api/schemas/scheduler.py</files>
  <action>
1. Update ScrapeProgress in src/scraping/schemas.py:
   - Change phase: str to phase: ScrapePhase (use the enum)
   - Add scrape_run_id: int | None = None (for tracking which run)
   - Add elapsed_ms: int | None = None (time since platform start)
   - Add error: ScrapeErrorContext | None = None (structured error info)

2. Create ScrapeErrorContext class in src/scraping/schemas.py:
   - error_type: str (e.g., "timeout", "network", "parse", "storage")
   - error_message: str
   - platform: str | None = None
   - recoverable: bool = False

3. Update src/api/schemas/scheduler.py:
   - Add ScrapePhaseLogResponse for API:
     - id: int
     - platform: str | None
     - phase: str
     - started_at: datetime
     - ended_at: datetime | None
     - events_processed: int | None
     - message: str | None
   - Add phase_logs: list[ScrapePhaseLogResponse] | None to ScrapeRunDetailResponse (optional, only populated if requested)

Do NOT break existing SSE consumers - the schema changes must be backward compatible.
The phase field changing to ScrapePhase is OK because StrEnum serializes to string.
  </action>
  <verify>python -c "from src.scraping.schemas import ScrapeProgress, ScrapeErrorContext, ScrapePhase; p = ScrapeProgress(phase=ScrapePhase.SCRAPING, current=0, total=3); print(p.model_dump_json())"</verify>
  <done>ScrapeProgress enhanced with scrape_run_id, elapsed_ms, error context; ScrapeErrorContext defined; API schemas updated</done>
</task>

<task type="auto">
  <name>Task 2: Update orchestrator with granular phase emissions and structured logging</name>
  <files>src/scraping/orchestrator.py</files>
  <action>
1. Add imports at top:
   - import structlog
   - from src.scraping.logging import configure_logging (not needed if configured in main.py)
   - from src.scraping.schemas import ScrapePhase, PlatformStatus, ScrapeErrorContext
   - from src.db.models.scrape import ScrapePhaseLog, ScrapeRun
   - from sqlalchemy import update

2. Replace logger = logging.getLogger(__name__) with:
   - logger = structlog.get_logger(__name__)

3. Add _emit_phase() helper method to ScrapingOrchestrator:
```python
async def _emit_phase(
    self,
    db: AsyncSession | None,
    scrape_run_id: int | None,
    platform: Platform | None,
    phase: ScrapePhase,
    message: str,
    events_count: int = 0,
    error: ScrapeErrorContext | None = None,
) -> ScrapeProgress:
    """Emit phase transition: update DB, log, return progress object."""
    # 1. Update ScrapeRun current state in DB
    if db and scrape_run_id:
        await db.execute(
            update(ScrapeRun)
            .where(ScrapeRun.id == scrape_run_id)
            .values(
                current_phase=phase.value,
                current_platform=platform.value if platform else None,
            )
        )
        # Don't commit - batch with other operations

    # 2. Log phase transition with structlog
    log = logger.bind(
        scrape_run_id=scrape_run_id,
        platform=platform.value if platform else None,
        phase=phase.value,
        events_count=events_count,
    )
    if error:
        log.error("phase_transition", error_type=error.error_type, error_message=error.error_message)
    else:
        log.info("phase_transition", message=message)

    # 3. Return progress object for SSE
    return ScrapeProgress(
        scrape_run_id=scrape_run_id,
        platform=platform,
        phase=phase,
        current=0,  # Will be set by caller
        total=0,    # Will be set by caller
        events_count=events_count,
        message=message,
        error=error,
    )
```

4. Add _log_phase_history() helper:
```python
async def _log_phase_history(
    self,
    db: AsyncSession,
    scrape_run_id: int,
    platform: Platform | None,
    phase: ScrapePhase,
    message: str,
    events_count: int = 0,
    error: ScrapeErrorContext | None = None,
) -> None:
    """Persist phase transition to history table."""
    phase_log = ScrapePhaseLog(
        scrape_run_id=scrape_run_id,
        platform=platform.value if platform else None,
        phase=phase.value,
        events_processed=events_count,
        message=message,
        error_details=error.model_dump() if error else None,
    )
    db.add(phase_log)
    # Don't commit - batch with other operations
```

5. Update scrape_with_progress() to use granular phases:
   - Bind contextvars at start: structlog.contextvars.clear_contextvars(); structlog.contextvars.bind_contextvars(scrape_run_id=scrape_run_id)
   - Replace phase="starting" with phase=ScrapePhase.INITIALIZING
   - Replace phase="scraping" with phase=ScrapePhase.SCRAPING
   - Replace phase="storing" with phase=ScrapePhase.STORING
   - Replace phase="completed" with phase=ScrapePhase.COMPLETED
   - Replace phase="failed" with phase=ScrapePhase.FAILED
   - Call _log_phase_history() for each phase transition when db is available

6. Update _scrape_platform() logging:
   - Use structured logger calls: logger.info("platform_scrape_start", platform=platform.value)
   - Log at key points: scrape start, scrape complete, timeout, error

7. Update error handling in scrape_with_progress():
   - When catching exceptions, create ScrapeErrorContext:
     - For TimeoutError: error_type="timeout", recoverable=True
     - For network errors: error_type="network", recoverable=True
     - For parse errors: error_type="parse", recoverable=False
     - For other errors: error_type="unknown", recoverable=False
   - Include error context in the yielded ScrapeProgress

Do NOT change scrape_all() signature or behavior - this plan focuses on scrape_with_progress() (used for SSE streaming).
Keep backward compatibility - existing code calling orchestrator should still work.
  </action>
  <verify>python -c "from src.scraping.orchestrator import ScrapingOrchestrator; print('Orchestrator loads')"</verify>
  <done>Orchestrator emits granular ScrapePhase transitions, uses structured logging, persists phase history to DB</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from src.scraping.schemas import ScrapeProgress, ScrapePhase"` succeeds
- [ ] `python -c "from src.scraping.orchestrator import ScrapingOrchestrator"` succeeds
- [ ] Start backend (`uvicorn src.main:app --reload`) and trigger scrape - logs show structured JSON
- [ ] SSE stream still works with enhanced ScrapeProgress
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- ScrapeProgress uses ScrapePhase enum (serializes as string for SSE)
- Orchestrator emits granular phase transitions
- Structured logging with context (scrape_run_id, platform, phase)
- Phase history persisted to ScrapePhaseLog table
- Existing SSE consumers still work (backward compatible)
</success_criteria>

<output>
After completion, create `.planning/phases/14-scraping-logging-workflow/14-02-SUMMARY.md`
</output>
