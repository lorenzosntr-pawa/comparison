---
phase: 02-database-schema
plan: 03
type: execute
---

<objective>
Set up Alembic for async migrations and create initial schema migration with partitioning.

Purpose: Enable database schema versioning and create the actual PostgreSQL tables with partitioning for odds_snapshots.
Output: Working Alembic setup with async env.py and initial migration that creates all tables with pg_partman partitioning.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-database-schema/02-RESEARCH.md
@.planning/phases/02-database-schema/02-01-PLAN.md
@.planning/phases/02-database-schema/02-02-PLAN.md

**From 02-01 and 02-02:** All models defined in src/db/models/

**From RESEARCH.md - MUST follow:**
- Async Alembic env.py pattern (use `run_sync()` wrapper)
- Can use sync psycopg driver for migrations (simpler than full async)
- pg_partman for automated partition management
- Daily partitions with 30-day retention

**Partitioning approach:**
- Define OddsSnapshot as regular table in SQLAlchemy model
- Create as partitioned table in migration using raw SQL
- pg_partman handles partition creation/cleanup
</context>

<tasks>

<task type="auto">
  <name>Task 1: Initialize Alembic with async configuration</name>
  <files>alembic.ini, alembic/env.py, alembic/script.py.mako</files>
  <action>
1. Install Alembic if not present: `pip install alembic>=1.13.0 psycopg[binary]>=3.1.0`
   (psycopg for sync driver in migrations - simpler than async)

2. Run `alembic init alembic` from project root to create structure

3. Edit `alembic.ini`:
   - Set `sqlalchemy.url = postgresql+psycopg://postgres:postgres@localhost:5432/betpawa`
   - Keep file_template as default

4. Replace `alembic/env.py` with async-compatible version:
```python
from logging.config import fileConfig
import asyncio
from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config
from alembic import context

# Import your models' Base
from src.db.base import Base
from src.db import models  # noqa: F401 - registers all models

config = context.config
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

target_metadata = Base.metadata

def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode."""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )
    with context.begin_transaction():
        context.run_migrations()

def do_run_migrations(connection: Connection) -> None:
    context.configure(connection=connection, target_metadata=target_metadata)
    with context.begin_transaction():
        context.run_migrations()

async def run_async_migrations() -> None:
    """Run migrations in 'online' mode with async engine."""
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )
    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)
    await connectable.dispose()

def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""
    asyncio.run(run_async_migrations())

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

5. Update alembic.ini sqlalchemy.url to use asyncpg:
   `sqlalchemy.url = postgresql+asyncpg://postgres:postgres@localhost:5432/betpawa`

Note: We use asyncpg in alembic.ini because env.py uses async_engine_from_config.
  </action>
  <verify>alembic --help (should show alembic commands)</verify>
  <done>Alembic initialized with async env.py, imports Base and models correctly</done>
</task>

<task type="auto">
  <name>Task 2: Generate initial migration with partitioning</name>
  <files>alembic/versions/001_initial_schema.py</files>
  <action>
1. Generate autogenerated migration:
   `alembic revision --autogenerate -m "initial schema"`

2. Edit the generated migration to add partitioning for odds_snapshots:

The autogenerated migration will create odds_snapshots as a regular table.
Modify the `upgrade()` function to:

a) Create all non-partitioned tables first (sports, tournaments, bookmakers, events, event_bookmakers, scrape_runs, scrape_errors, market_odds)

b) For odds_snapshots, replace the autogenerated CREATE TABLE with:
```python
# Create odds_snapshots as partitioned table
op.execute("""
    CREATE TABLE odds_snapshots (
        id BIGSERIAL,
        event_id INTEGER NOT NULL,
        bookmaker_id INTEGER NOT NULL,
        captured_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
        scrape_run_id INTEGER,
        raw_response JSONB,
        PRIMARY KEY (id, captured_at),
        CONSTRAINT fk_snapshots_event FOREIGN KEY (event_id) REFERENCES events(id),
        CONSTRAINT fk_snapshots_bookmaker FOREIGN KEY (bookmaker_id) REFERENCES bookmakers(id),
        CONSTRAINT fk_snapshots_scrape_run FOREIGN KEY (scrape_run_id) REFERENCES scrape_runs(id)
    ) PARTITION BY RANGE (captured_at)
""")

# Create indexes on partitioned table (will propagate to partitions)
op.execute("CREATE INDEX idx_snapshots_event ON odds_snapshots (event_id)")
op.execute("CREATE INDEX idx_snapshots_bookmaker ON odds_snapshots (bookmaker_id)")
op.execute("CREATE INDEX idx_snapshots_event_time ON odds_snapshots (event_id, captured_at DESC)")
op.execute("CREATE INDEX idx_snapshots_captured_brin ON odds_snapshots USING BRIN (captured_at) WITH (pages_per_range = 32)")
```

c) Add pg_partman setup (in a separate migration or same one with try/except):
```python
# Set up pg_partman for automatic partition management
# Note: pg_partman extension must be installed on the PostgreSQL server
op.execute("""
    -- Create initial partition for today (pg_partman will manage future ones)
    CREATE TABLE odds_snapshots_default PARTITION OF odds_snapshots DEFAULT;
""")

# Optional: If pg_partman is available, configure it
# This is a nice-to-have - partitions can also be created manually
op.execute("""
    DO $$
    BEGIN
        IF EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'pg_partman') THEN
            PERFORM partman.create_parent(
                p_parent_table := 'public.odds_snapshots',
                p_control := 'captured_at',
                p_type := 'native',
                p_interval := 'daily',
                p_premake := 7
            );
            UPDATE partman.part_config
            SET retention = '30 days', retention_keep_table = false
            WHERE parent_table = 'public.odds_snapshots';
        END IF;
    END $$;
""")
```

d) Update market_odds FK to reference the partitioned table:
```python
# market_odds.snapshot_id references odds_snapshots(id, captured_at)
# Since odds_snapshots is partitioned, we need snapshot_captured_at for the FK
```

Actually, simplify: For now, market_odds can reference odds_snapshots.id without the partition key.
PostgreSQL allows this - the FK just won't be able to use partition pruning.

3. Add `downgrade()` function that drops tables in reverse order.
  </action>
  <verify>alembic upgrade head (with a running PostgreSQL database)</verify>
  <done>Initial migration created with partitioned odds_snapshots table and all indexes</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Alembic setup with async env.py and initial migration including partitioned odds_snapshots</what-built>
  <how-to-verify>
    1. Ensure PostgreSQL is running locally (docker run -d -p 5432:5432 -e POSTGRES_PASSWORD=postgres -e POSTGRES_DB=betpawa postgres:16)
    2. Run: alembic upgrade head
    3. Connect to database and verify:
       - psql -h localhost -U postgres -d betpawa -c "\dt" (should show all tables)
       - psql -h localhost -U postgres -d betpawa -c "\d+ odds_snapshots" (should show "Partition key: RANGE (captured_at)")
    4. Verify indexes: psql -h localhost -U postgres -d betpawa -c "\di odds_snapshots*"
  </how-to-verify>
  <resume-signal>Type "approved" if tables created correctly with partitioning, or describe issues</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `alembic --help` works
- [ ] `alembic upgrade head` succeeds (with database running)
- [ ] All 9 tables created (sports, tournaments, bookmakers, events, event_bookmakers, odds_snapshots, market_odds, scrape_runs, scrape_errors)
- [ ] odds_snapshots is partitioned by captured_at
- [ ] Indexes exist on odds_snapshots
</verification>

<success_criteria>

- Alembic initialized with async-compatible env.py
- Initial migration creates all tables from models
- odds_snapshots is a partitioned table with daily partitions
- All foreign keys and indexes in place
- Phase 2 complete - database schema ready for Phase 3 (Scraper Integration)
</success_criteria>

<output>
After completion, create `.planning/phases/02-database-schema/02-03-SUMMARY.md`
</output>
