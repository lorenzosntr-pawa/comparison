---
phase: 34-investigation-matching-audit
plan: 01
type: execute
---

<objective>
Produce a comprehensive investigation report documenting how event matching SHOULD work vs how it ACTUALLY works, with specific bugs identified and evidence from both code and data analysis.

Purpose: Provide clear findings that enable unambiguous planning for Phases 35-37 (fix phases).
Output: `.planning/phases/34-investigation-matching-audit/AUDIT-FINDINGS.md` with code audit, data analysis, and recommendations.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/34-investigation-matching-audit/34-CONTEXT.md

**Key source files to audit:**
@src/matching/service.py
@src/scraping/orchestrator.py
@src/scraping/competitor_events.py
@src/db/models/competitor.py
@src/db/models/event.py
@src/api/routes/palimpsest.py

**Tech stack available:** Python/FastAPI, SQLAlchemy async, PostgreSQL
**Established patterns:** Pydantic v2, structlog, async/await

**From 34-CONTEXT.md:**
- Investigation combines code audit + data analysis
- Known symptoms: missing matches, wrong pairings, inconsistent ID extraction, missing scrape_run refs
- Deliverable: How SHOULD work → How ACTUALLY works → Specific bugs → Recommendation (patch vs refactor)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Code audit - trace SportRadar ID extraction per platform</name>
  <files>.planning/phases/34-investigation-matching-audit/AUDIT-FINDINGS.md (sections 1-3)</files>
  <action>
Read and document how each platform extracts SportRadar IDs:

1. **BetPawa** (orchestrator.py:_parse_betpawa_event):
   - Extracts from `widgets` array where `type == "SPORTRADAR"` → gets `id`
   - Document the exact extraction logic and any edge cases

2. **SportyBet** (competitor_events.py:_parse_sportybet_event):
   - Uses `eventId` field directly (format: `sr:match:{id}`)
   - Strips prefix to get numeric ID
   - Document any URL encoding issues (CONTEXT mentions "URL-encoded sr:match:id")

3. **Bet9ja** (competitor_events.py:_parse_bet9ja_event):
   - Uses `EXTID` field directly
   - Document what format this field contains

Then trace the matching pipeline:
- `events` table: stores Betpawa events with `sportradar_id`
- `competitor_events` table: has `sportradar_id` AND `betpawa_event_id` FK
- `_get_betpawa_event_by_sr_id()`: links competitor events to Betpawa events
- Query-time matching in palimpsest.py routes

Document inconsistencies:
- Are all three platforms using the SAME sportradar_id format?
- Is the matching join correct?
- What happens if IDs don't match exactly (leading zeros, prefixes, etc.)?

Write findings to sections 1-3 of AUDIT-FINDINGS.md:
1. Ideal Architecture (how it SHOULD work)
2. Actual Implementation (how it works)
3. Code-Level Issues (specific problems found)
  </action>
  <verify>AUDIT-FINDINGS.md exists with sections 1-3 populated</verify>
  <done>Code audit complete with documented ID extraction per platform, matching pipeline traced, inconsistencies identified</done>
</task>

<task type="auto">
  <name>Task 2: Data analysis - measure actual matching issues via SQL</name>
  <files>.planning/phases/34-investigation-matching-audit/AUDIT-FINDINGS.md (section 4)</files>
  <action>
Connect to the database and run diagnostic SQL queries to measure the actual state:

1. **Overall match rates:**
   - Count of `competitor_events` with `betpawa_event_id IS NOT NULL` vs total
   - Break down by source (sportybet, bet9ja)

2. **Missing matches analysis:**
   - Find events that exist on ALL 3 platforms (same sportradar_id in events + both competitor sources)
   - But where `betpawa_event_id IS NULL` in competitor_events
   - Show sample rows with sportradar_id, event names, kickoffs

3. **ID format comparison:**
   - Sample sportradar_id values from each table (events, competitor_events by source)
   - Check for format differences (prefixes, numeric vs string, etc.)

4. **scrape_run_id linkage:**
   - Count of competitor_odds_snapshots with `scrape_run_id IS NULL` vs total
   - This validates the CONTEXT.md concern about missing scrape_run refs

5. **Wrong pairings sample:**
   - If possible, find cases where `betpawa_event_id` links to an event with different teams/kickoff
   - Compare home_team/away_team between events and competitor_events

Run queries using `psql` or Python with SQLAlchemy. Add results to section 4 of AUDIT-FINDINGS.md with:
- Actual query used
- Results summary
- Interpretation of what this means

Note: Use the database connection from the project's config (check src/db/engine.py for connection details).
  </action>
  <verify>Section 4 of AUDIT-FINDINGS.md contains SQL queries and their results</verify>
  <done>Data analysis complete with match rates, sample mismatches, ID format comparison, and scrape_run linkage stats</done>
</task>

<task type="auto">
  <name>Task 3: Synthesize findings and recommendation</name>
  <files>.planning/phases/34-investigation-matching-audit/AUDIT-FINDINGS.md (sections 5-7)</files>
  <action>
Based on Tasks 1-2 findings, complete the audit report:

**Section 5: Specific Bugs List**
- Number each bug clearly: BUG-01, BUG-02, etc.
- For each: description, evidence (code line or SQL result), severity (critical/high/medium)
- Group by: ID extraction bugs, matching logic bugs, data linkage bugs

**Section 6: Root Cause Analysis**
- Is this a v1.1 transition issue (hybrid system)?
- What's the fundamental architectural problem?
- Is it small fixes or major rework?

**Section 7: Recommendations**
Based on findings, recommend ONE of:

Option A: **Patch the hybrid system**
- List specific fixes needed
- Estimate complexity (file count, lines of code)
- Risk assessment

Option B: **Refactor toward unified approach**
- Describe target architecture
- Migration path
- Risk assessment

Include a clear recommendation with reasoning.

**Section 8: Phase 35-37 Roadmap**
Based on recommendation, suggest what each fix phase should tackle:
- Phase 35: [specific scope]
- Phase 36: [specific scope]
- Phase 37: [specific scope]
  </action>
  <verify>AUDIT-FINDINGS.md complete with all 8 sections, clear bug list, and recommendation</verify>
  <done>Complete audit report with numbered bugs, root cause analysis, recommendation (patch vs refactor), and roadmap for fix phases</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] AUDIT-FINDINGS.md exists and has all 8 sections
- [ ] Code audit covers all 3 platforms' ID extraction
- [ ] Data analysis has actual SQL queries and results
- [ ] Bug list is numbered and prioritized
- [ ] Recommendation is clear (patch vs refactor)
- [ ] Phase 35-37 scope is unambiguous
</verification>

<success_criteria>
- All tasks completed
- AUDIT-FINDINGS.md is comprehensive and actionable
- Fix phases (35-37) can be planned without additional investigation
- No code changes made (investigation only)
</success_criteria>

<output>
After completion, create `.planning/phases/34-investigation-matching-audit/34-01-SUMMARY.md`:

# Phase 34 Plan 1: Investigation & Matching Audit Summary

**[One-liner summary of key finding]**

## Accomplishments

- [Key findings]
- [Bug count and severity breakdown]
- [Recommendation made]

## Files Created/Modified

- `AUDIT-FINDINGS.md` - Complete investigation report

## Decisions Made

[Recommendation: patch vs refactor, with rationale]

## Issues Encountered

[Any limitations in the investigation]

## Next Phase Readiness

- Phase 35 scope: [what it will fix]
- Phase 36 scope: [what it will fix]
- Phase 37 scope: [what it will fix]
</output>
