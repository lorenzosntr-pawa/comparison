---
phase: 55.1-fix-phase-55-bugs
type: execute
---

<objective>
Fix 3 bugs discovered during Phase 55 re-verification: BUG-005 write_ms duplicate keyword (blocker), BUG-006 stale detection timezone mismatch (major), BUG-007 on-demand scrapes bypass cache/write queue (minor).

Purpose: Unblock the async write pipeline — BUG-005 causes triple-duplicate inserts on every scheduled scrape, BUG-006 breaks stale run detection, BUG-007 leaves cache stale after on-demand scrapes.
Output: All 3 bugs fixed, scheduled scrape writes succeed without retries, stale detection runs without errors, on-demand scrapes update cache.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/ISSUES.md

# Phase 55 context (dependency):
@.planning/phases/55-async-write-pipeline/55-03-SUMMARY.md
@.planning/phases/55-async-write-pipeline/55-03-FIX-SUMMARY.md

# Key source files:
@src/storage/write_queue.py
@src/storage/write_handler.py
@src/scheduling/stale_detection.py
@src/api/routes/scrape.py
@src/scraping/event_coordinator.py

**Tech stack available:** asyncio queues, structlog, SQLAlchemy 2.0 async, frozen dataclass DTOs
**Established patterns:**
- `.replace(tzinfo=None)` for timezone-naive columns (15+ usages across codebase)
- `getattr(app_state, "odds_cache", None)` pattern for safe cache access (used in jobs.py:127)
- `EventCoordinator.from_settings()` accepts optional `odds_cache` and `write_queue` params

**Constraining decisions:**
- [Phase 55]: Dual-path persistence — async when write_queue present, sync when None
- [Phase 55]: Cache updated for ALL data before enqueue — API always serves freshest odds
- [Phase 54]: Cache-first API pattern — check OddsCache first, fall back to DB

**Issues being addressed:** BUG-005, BUG-006, BUG-007 from ISSUES.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix BUG-005 — write_ms duplicate keyword in write queue logging</name>
  <files>src/storage/write_queue.py</files>
  <action>
Fix the TypeError that crashes every async write batch after logging.

**Root cause:** In `_process_with_retry()`, line 184 passes `write_ms=round(elapsed_ms, 1)` explicitly, AND line 185 spreads `**stats` which ALSO contains `write_ms` (set in write_handler.py:180). Python raises `TypeError: got multiple values for keyword argument 'write_ms'`.

**Impact:** DB writes SUCCEED but the logging call after them fails, triggering unnecessary retries. Each batch gets written 3x (triple duplicates), then "dropped" despite data being persisted.

**Fix:** Remove the explicit `write_ms=round(elapsed_ms, 1)` from the success log call at line 184. The `**stats` dict already provides `write_ms` with the correct timing from the write handler.

Change lines 180-186 from:
```python
self._log.info(
    "write_batch_processed",
    batch_index=batch.batch_index,
    attempt=attempt,
    write_ms=round(elapsed_ms, 1),
    **stats,
)
```
To:
```python
self._log.info(
    "write_batch_processed",
    batch_index=batch.batch_index,
    attempt=attempt,
    **stats,
)
```

**Do NOT change:** The retry/error log calls (lines 193-199) — those don't spread `**stats` so `write_ms` is correctly explicit there.
  </action>
  <verify>
Start the application, let a scheduled scrape cycle run. Check terminal logs:
1. `write_batch_processed` logs appear (not `write_batch_failed`)
2. No `got multiple values for keyword argument 'write_ms'` errors
3. Each batch processed once (attempt=1), no retries
  </verify>
  <done>Async write batches log successfully on first attempt. No duplicate keyword errors. No triple-duplicate inserts.</done>
</task>

<task type="auto">
  <name>Task 2: Fix BUG-006 — stale detection timezone mismatch</name>
  <files>src/scheduling/stale_detection.py</files>
  <action>
Fix the TypeError that crashes the `detect_stale_runs` watchdog job every 2 minutes.

**Error:** `TypeError: can't subtract offset-naive and offset-aware datetimes` at `mark_run_stale()`.

**Root cause:** The `mark_run_stale` function subtracts `now - last_activity` (line 100) and `now - run.started_at` (line 108). If either `last_activity` or `run.started_at` has timezone info while `now` (from `datetime.utcnow()`, which is naive) does not, the subtraction raises TypeError.

**Fix:** Ensure `last_activity` is timezone-naive before subtraction. In `mark_run_stale()`, strip timezone info from `last_activity` if present:

After line 96 (`now = datetime.utcnow()`), add:
```python
if last_activity is not None and last_activity.tzinfo is not None:
    last_activity = last_activity.replace(tzinfo=None)
```

Also guard `run.started_at` on line 108 with the same pattern:
```python
started_at = run.started_at
if started_at.tzinfo is not None:
    started_at = started_at.replace(tzinfo=None)
stale_duration = now - started_at
```

This follows the established `.replace(tzinfo=None)` pattern used in 15+ places across the codebase (event_coordinator.py, warmup.py, etc.).

**Do NOT change:** The `find_stale_runs` function — it uses `datetime.utcnow()` for the threshold which is compared in SQL (no Python subtraction).
  </action>
  <verify>
Start the application, wait 2+ minutes for the detect_stale_runs job to fire. Check logs:
1. No `can't subtract offset-naive and offset-aware datetimes` errors
2. Either `No stale scrape runs detected` debug message, or stale runs properly marked as FAILED
  </verify>
  <done>Stale detection watchdog runs without timezone errors. Stuck runs are auto-failed correctly.</done>
</task>

<task type="auto">
  <name>Task 3: Fix BUG-007 — on-demand scrapes bypass cache and write queue</name>
  <files>src/api/routes/scrape.py</files>
  <action>
Fix all 3 on-demand scrape paths to pass `odds_cache` and `write_queue` from `request.app.state` to EventCoordinator, so on-demand scrapes update the cache and use the async write pipeline.

**Root cause:** All 3 `EventCoordinator.from_settings()` calls in scrape.py omit the `odds_cache` and `write_queue` parameters, so on-demand scrapes always use the sync fallback path and never update the in-memory cache.

**Fix:** Add `odds_cache` and `write_queue` to all 3 calls using `getattr()` for safe access (same pattern as `src/scheduling/jobs.py:127-128`):

**Call site 1 (~line 101):** Quick scrape endpoint
```python
coordinator = EventCoordinator.from_settings(
    betpawa_client=betpawa,
    sportybet_client=sportybet,
    bet9ja_client=bet9ja,
    settings=settings,
    odds_cache=getattr(request.app.state, "odds_cache", None),
    write_queue=getattr(request.app.state, "write_queue", None),
)
```

**Call site 2 (~line 200):** Background scrape endpoint
```python
coordinator = EventCoordinator.from_settings(
    betpawa_client=betpawa,
    sportybet_client=sportybet,
    bet9ja_client=bet9ja,
    settings=settings,
    odds_cache=getattr(request.app.state, "odds_cache", None),
    write_queue=getattr(request.app.state, "write_queue", None),
)
```

Note: Call site 2 runs in a background task — it captures `request.app.state` references before the request ends. The `odds_cache` and `write_queue` are app-lifetime objects (not request-scoped), so capturing them is safe. However, this call site may not have `request` in the background task scope. Check whether `request.app.state` is available or if `app_state` needs to be captured before the background task. If `request` isn't available, capture the values in the outer scope:
```python
odds_cache = getattr(request.app.state, "odds_cache", None)
write_queue = getattr(request.app.state, "write_queue", None)
```
Then pass them in the background function.

**Call site 3 (~line 629):** Retry scrape endpoint
```python
coordinator = EventCoordinator.from_settings(
    betpawa_client=betpawa,
    sportybet_client=sportybet,
    bet9ja_client=bet9ja,
    settings=settings,
    odds_cache=getattr(request.app.state, "odds_cache", None),
    write_queue=getattr(request.app.state, "write_queue", None),
)
```

**Do NOT change:** The EventCoordinator itself, the write queue, or the cache module. Only modify the 3 call sites in scrape.py.
  </action>
  <verify>
1. Trigger an on-demand scrape via the UI button
2. Check terminal logs for `cache_update_ms` values > 0 (proving cache is being updated)
3. Check that `write_batch_processed` log events appear (proving write queue is used)
4. Verify the event list API returns fresh data immediately after on-demand scrape
  </verify>
  <done>All 3 on-demand scrape paths pass odds_cache and write_queue. On-demand scrapes update cache immediately and persist via async write queue. cache_update_ms > 0 in logs.</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] No `got multiple values for keyword argument 'write_ms'` errors in logs
- [ ] No `can't subtract offset-naive and offset-aware datetimes` errors in logs
- [ ] Scheduled scrape writes succeed on first attempt (no retries)
- [ ] Stale detection job runs without errors
- [ ] On-demand scrape updates cache (cache_update_ms > 0)
- [ ] Application starts and runs without errors
</verification>

<success_criteria>

- BUG-005 resolved: write batches log without duplicate keyword errors
- BUG-006 resolved: stale detection runs without timezone errors
- BUG-007 resolved: on-demand scrapes update cache and use write queue
- No regressions introduced
- All 3 bugs closed in ISSUES.md
  </success_criteria>

<output>
After completion, create `.planning/phases/55.1-fix-phase-55-bugs/55.1-01-SUMMARY.md`
</output>
