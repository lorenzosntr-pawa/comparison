---
phase: 105-investigation-schema-design
plan: 01
type: execute
---

<objective>
Investigate codebase data flow and design market-level storage architecture.

Purpose: Understand all code paths that read/write market data and design new schema to reduce storage from ~8GB/day to <200MB/day through market-level change detection.
Output: DISCOVERY.md with code path analysis, schema design, and migration strategy.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/100-investigation-analysis/100-01-SUMMARY.md

**Prior context from v2.8 Storage Optimization:**
- Database reduced from 63 GB to 12 GB via raw_response removal
- market_odds is now the primary storage driver (~35% of database)
- Current architecture stores ALL markets on every snapshot, even when only 1 market changed

**Current schema structure:**
@src/db/models/odds.py - OddsSnapshot + MarketOdds (BetPawa)
@src/db/models/competitor.py - CompetitorOddsSnapshot + CompetitorMarketOdds

**Write path:**
@src/storage/write_queue.py - WriteBatch, MarketWriteData dataclasses
@src/storage/write_handler.py - INSERT snapshots + markets, UPDATE timestamps

**Change detection:**
@src/caching/change_detection.py - markets_changed() at snapshot level (problem: any change = full snapshot write)

**Read paths:**
@src/api/routes/events.py - Events list with inline odds
@src/api/routes/history.py - Historical odds/margin timelines
@src/caching/odds_cache.py - CachedSnapshot, CachedMarket structures
</context>

<tasks>

<task type="auto">
  <name>Task 1: Document current write path code flow</name>
  <files>.planning/phases/105-investigation-schema-design/DISCOVERY.md</files>
  <action>
Trace the complete write path from scraping to database:

1. **Scraping layer**: Where markets are collected
   - Find EventCoordinator in src/scraping/
   - Trace how markets are assembled into SnapshotWriteData

2. **Change detection layer**: Where snapshot-vs-market decision happens
   - src/caching/change_detection.py classify_batch_changes()
   - Document the current algorithm: snapshot-level comparison

3. **Write queue layer**: How data flows to DB
   - WriteBatch structure with changed/unchanged separation
   - handle_write_batch() INSERT and UPDATE patterns

4. **Database layer**: Final INSERT patterns
   - OddsSnapshot + MarketOdds relationship
   - CompetitorOddsSnapshot + CompetitorMarketOdds relationship

Document in DISCOVERY.md with code references (file:line) and a diagram of the flow.
  </action>
  <verify>DISCOVERY.md contains "## Current Write Path" section with code references</verify>
  <done>Write path documented with file:line references for all key functions</done>
</task>

<task type="auto">
  <name>Task 2: Document current read path code flow</name>
  <files>.planning/phases/105-investigation-schema-design/DISCOVERY.md</files>
  <action>
Trace all read paths that consume market_odds data:

1. **Events list API** (src/api/routes/events.py):
   - _build_inline_odds() for Odds Comparison page
   - _load_snapshots_cached() cache-first pattern
   - How markets are filtered (INLINE_MARKET_IDS)

2. **Event detail API** (src/api/routes/events.py):
   - Full market list for Event Details page
   - Market grouping (market_groups field usage)

3. **History API** (src/api/routes/history.py):
   - get_odds_history() - JOIN OddsSnapshot + MarketOdds
   - get_margin_history() - same pattern
   - Time-range queries with captured_at

4. **Cache layer** (src/caching/odds_cache.py):
   - CachedSnapshot structure
   - How cache is populated from DB
   - Cache warmup on startup

Document query patterns and which fields are actually used in each path.
  </action>
  <verify>DISCOVERY.md contains "## Current Read Paths" section with query patterns</verify>
  <done>Read paths documented with SQL patterns and field usage</done>
</task>

<task type="auto">
  <name>Task 3: Design new market-level schema</name>
  <files>.planning/phases/105-investigation-schema-design/DISCOVERY.md</files>
  <action>
Design the new schema for market-level change detection:

**New Tables:**

1. **market_odds_current** (upsert pattern - latest odds):
   ```sql
   CREATE TABLE market_odds_current (
     id BIGSERIAL PRIMARY KEY,
     event_id INTEGER NOT NULL REFERENCES events(id),
     bookmaker_id INTEGER NOT NULL REFERENCES bookmakers(id),
     betpawa_market_id VARCHAR(50) NOT NULL,
     betpawa_market_name VARCHAR(255) NOT NULL,
     line FLOAT,
     handicap_type VARCHAR(50),
     handicap_home FLOAT,
     handicap_away FLOAT,
     outcomes JSONB NOT NULL,
     market_groups JSONB,
     unavailable_at TIMESTAMPTZ,
     last_updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
     last_confirmed_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
     UNIQUE (event_id, bookmaker_id, betpawa_market_id, line)
   );
   ```

2. **market_odds_history** (append-only - when odds change):
   ```sql
   CREATE TABLE market_odds_history (
     id BIGSERIAL PRIMARY KEY,
     event_id INTEGER NOT NULL REFERENCES events(id),
     bookmaker_id INTEGER NOT NULL REFERENCES bookmakers(id),
     betpawa_market_id VARCHAR(50) NOT NULL,
     line FLOAT,
     outcomes JSONB NOT NULL,
     captured_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
     -- No unique constraint - append only
   );
   ```

**Design decisions to document:**
- Do we need separate tables for BetPawa vs competitor? (analyze current usage)
- Index strategy for history queries (event_id, market_id, captured_at)
- Partitioning strategy for history table (by captured_at range)
- Migration path from odds_snapshots + market_odds to new schema

**Storage estimate:**
- Current: ~50 markets × 500 events × 288 scrapes/day = 7.2M rows/day
- New: Only changed markets = ~5% change rate = 360K rows/day
- Reduction: 95% (7.2M → 360K rows/day)
  </action>
  <verify>DISCOVERY.md contains "## New Schema Design" section with CREATE TABLE statements</verify>
  <done>Schema design complete with tables, indexes, partitioning, and storage estimates</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] DISCOVERY.md exists with all 3 sections
- [ ] Write path code references are accurate (verify file:line numbers)
- [ ] Read path query patterns are documented
- [ ] Schema design includes indexes and partitioning strategy
- [ ] Storage reduction estimate is calculated
</verification>

<success_criteria>

- DISCOVERY.md created with comprehensive analysis
- All code paths documented with file:line references
- New schema design is complete with SQL DDL
- Migration strategy is outlined
- No code changes in this phase (investigation only)
</success_criteria>

<output>
After completion, create `.planning/phases/105-investigation-schema-design/105-01-SUMMARY.md`:

# Phase 105 Plan 01: Investigation & Schema Design Summary

**[Substantive one-liner about key findings]**

## Accomplishments

- [Key finding about write path]
- [Key finding about read paths]
- [Schema design decision]

## Files Created/Modified

- `.planning/phases/105-investigation-schema-design/DISCOVERY.md` - Full analysis

## Decisions Made

[Key schema design decisions and rationale]

## Issues Encountered

[Any complexity discovered that affects later phases]

## Next Phase Readiness

- Phase 106 ready: Schema migration with Alembic
- Tables designed: market_odds_current, market_odds_history
- Migration complexity: [Low/Medium/High]
</output>
