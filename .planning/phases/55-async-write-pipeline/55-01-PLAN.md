---
phase: 55-async-write-pipeline
plan: 01
type: execute
---

<objective>
Add `last_confirmed_at` column to snapshot tables and create change detection module for comparing cached vs newly scraped market data.

Purpose: Provide the data model and comparison logic that enables incremental upserts — only writing changed snapshots while recording freshness for unchanged ones.
Output: Alembic migration adding `last_confirmed_at`, updated ORM models, and `src/caching/change_detection.py` module with per-bookmaker market equality comparison.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/55-async-write-pipeline/55-CONTEXT.md

# Auto-selected based on dependency graph:
@.planning/phases/54-in-memory-cache/54-01-SUMMARY.md
@.planning/phases/54-in-memory-cache/54-02-SUMMARY.md
@.planning/phases/53-investigation-benchmarking/53-01-SUMMARY.md
@.planning/phases/53-investigation-benchmarking/BENCHMARK-BASELINE.md

# Key files:
@src/db/models/odds.py
@src/db/models/competitor.py
@src/caching/odds_cache.py
@src/caching/warmup.py

**Tech stack available:** SQLAlchemy 2.0, Alembic, frozen dataclasses (CachedSnapshot/CachedMarket)
**Established patterns:** Frozen dataclass cache entries, Alembic migrations with PostgreSQL dialect
**Constraining decisions:**
- Phase 54: CachedSnapshot stores snapshot_id, event_id, bookmaker_id, captured_at, and tuple of CachedMarket
- Phase 54: CachedMarket has betpawa_market_id, betpawa_market_name, line, outcomes (dict), market_groups
- Phase 54: Cache keyed by event_id → bookmaker_id (betpawa) or event_id → source (competitor)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add last_confirmed_at column to snapshot tables</name>
  <files>src/db/models/odds.py, src/db/models/competitor.py, alembic/versions/i5j1k7l2m6n8_add_last_confirmed_at.py</files>
  <action>
Add a nullable `last_confirmed_at` DateTime column to both `OddsSnapshot` and `CompetitorOddsSnapshot` models. This column records when the snapshot was last confirmed as current (odds unchanged) during a scrape cycle.

In `src/db/models/odds.py`, add to OddsSnapshot:
```python
last_confirmed_at: Mapped[datetime | None] = mapped_column(DateTime, nullable=True)
```

In `src/db/models/competitor.py`, add to CompetitorOddsSnapshot:
```python
last_confirmed_at: Mapped[datetime | None] = mapped_column(DateTime, nullable=True)
```

Create Alembic migration `i5j1k7l2m6n8_add_last_confirmed_at.py`:
- Revision depends on `h4i0j6k1l4m5` (latest migration)
- Add nullable DateTime column `last_confirmed_at` to `odds_snapshots` table
- Add nullable DateTime column `last_confirmed_at` to `competitor_odds_snapshots` table
- Downgrade drops both columns

Do NOT set a default value or backfill existing rows — nullable is sufficient. New snapshots written by the async pipeline will set this field; existing historical snapshots don't need it.
  </action>
  <verify>
Run `alembic check` to confirm migration is detected. Run `alembic upgrade head` to apply. Verify columns exist with: `python -c "from src.db.models.odds import OddsSnapshot; print(hasattr(OddsSnapshot, 'last_confirmed_at'))"` — should print True.
  </verify>
  <done>Both models have `last_confirmed_at` field, migration applied successfully, no errors.</done>
</task>

<task type="auto">
  <name>Task 2: Create change detection module</name>
  <files>src/caching/change_detection.py, src/caching/__init__.py</files>
  <action>
Create `src/caching/change_detection.py` with a `markets_changed()` function that compares a cached snapshot's markets against newly parsed markets to determine if odds have changed.

**Function signature:**
```python
def markets_changed(
    cached_markets: tuple[CachedMarket, ...] | None,
    new_markets: list[dict],  # List of dicts with betpawa_market_id, line, outcomes keys
) -> bool:
```

**Comparison logic:**
1. If `cached_markets` is None (first scrape for this event+bookmaker), return True (changed — must write)
2. If market count differs, return True
3. Build a lookup by `(betpawa_market_id, line)` for both cached and new
4. For each market key: compare `outcomes` dicts. Outcomes is a list of `{name, odds, is_active}` — compare the full list for equality. Use sorted comparison by outcome name to handle ordering differences
5. If any market is missing or any outcomes differ → return True
6. If all markets match → return False

**Also create a helper:**
```python
def classify_batch_changes(
    cache: OddsCache,
    betpawa_snapshots: list[tuple],  # (event_id, bookmaker_id, markets_data)
    competitor_snapshots: list[tuple],  # (event_id, source, markets_data)
) -> tuple[list, list, list[int], list[int]]:
    """
    Returns (changed_betpawa, changed_competitor, unchanged_betpawa_ids, unchanged_competitor_ids)

    changed_*: snapshots that need INSERT (odds changed)
    unchanged_*_ids: existing snapshot IDs that need last_confirmed_at UPDATE
    """
```

This function iterates each snapshot, calls `markets_changed()` against the cache's current entry for that event+bookmaker, and classifies into changed vs unchanged. For unchanged, it pulls the `snapshot_id` from the cached entry (CachedSnapshot.snapshot_id).

**Important:** The `new_markets` format must match what EventCoordinator produces during market parsing. The coordinator creates lists of MarketOdds-like dicts with `betpawa_market_id`, `line`, and `outcomes` fields. Normalize both sides to the same comparison format.

Export `markets_changed` and `classify_batch_changes` from `src/caching/__init__.py`.
  </action>
  <verify>
Python import test: `python -c "from src.caching.change_detection import markets_changed, classify_batch_changes; print('OK')"` — should print OK. Manually verify logic by reading the function and confirming edge cases are handled (empty markets, None cached, different market counts, same markets different odds).
  </verify>
  <done>Change detection module exists with `markets_changed()` and `classify_batch_changes()`, both importable from `src.caching`, handles all edge cases (None, empty, different count, same odds, changed odds).</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Alembic migration applies cleanly (`alembic upgrade head`)
- [ ] Both ORM models have `last_confirmed_at` field
- [ ] `markets_changed()` returns True for None/different markets, False for identical markets
- [ ] `classify_batch_changes()` separates changed vs unchanged correctly
- [ ] No import errors in the caching package
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Migration is reversible (`alembic downgrade -1`)
- Change detection module ready for integration in Plan 55-03
  </success_criteria>

<output>
After completion, create `.planning/phases/55-async-write-pipeline/55-01-SUMMARY.md`
</output>
