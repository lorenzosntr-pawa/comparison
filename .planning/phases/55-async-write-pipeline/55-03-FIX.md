---
phase: 55-async-write-pipeline
plan: 55-03-FIX
type: fix
---

<objective>
Fix 2 UAT issues from plan 55-03.

Source: 55-03-ISSUES.md
Priority: 1 blocker, 0 major, 1 minor
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md

**Issues being fixed:**
@.planning/phases/55-async-write-pipeline/55-03-ISSUES.md

**Original plan for reference:**
@.planning/phases/55-async-write-pipeline/55-03-PLAN.md

**Key files:**
@src/storage/write_handler.py
@src/db/models/odds.py
@src/db/models/competitor.py
@src/scraping/event_coordinator.py

**Established patterns:**
- The codebase already uses `.replace(tzinfo=None)` in event_coordinator.py (lines 1391, 1607, 1640) and warmup.py (line 200) when storing timezone-aware datetimes into timezone-naive columns
- All existing snapshot columns (captured_at) are TIMESTAMP WITHOUT TIME ZONE
- The `last_confirmed_at` column follows the same pattern
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix UAT-001 — timezone mismatch in write handler</name>
  <files>src/storage/write_handler.py</files>
  <action>
Fix the timezone mismatch that causes every write batch to fail with `asyncpg.exceptions.DataError: can't subtract offset-naive and offset-aware datetimes`.

**Root cause:** `write_handler.py` line 81 creates `now = datetime.now(timezone.utc)` (timezone-aware), but `last_confirmed_at` column is `TIMESTAMP WITHOUT TIME ZONE` (timezone-naive). asyncpg rejects the type mismatch.

**Fix:** Strip timezone info from the `now` variable in `handle_write_batch()`:

Change:
```python
now = datetime.now(timezone.utc)
```
To:
```python
now = datetime.now(timezone.utc).replace(tzinfo=None)
```

This is consistent with the existing codebase pattern used in:
- `event_coordinator.py` lines 1391, 1607, 1640
- `caching/warmup.py` line 200

The `now` variable is used in 4 places within the function:
1. INSERT into OddsSnapshot (last_confirmed_at=now)
2. INSERT into CompetitorOddsSnapshot (last_confirmed_at=now)
3. UPDATE OddsSnapshot SET last_confirmed_at=now
4. UPDATE CompetitorOddsSnapshot SET last_confirmed_at=now

All 4 will be fixed by the single line change since they all reference the same `now` variable.

**Do NOT change:** Column type (DateTime stays without timezone=True), migration, or ORM model definitions. The timezone-naive column approach is consistent with existing captured_at columns.
  </action>
  <verify>
Start the application, let a scheduled scrape cycle run, and check terminal logs. Should see:
1. `change_detection.results` logs (already working)
2. `write_batch_complete` logs instead of `write_batch_failed` errors
3. No `can't subtract offset-naive and offset-aware datetimes` errors
4. Verify DB has data: query `SELECT COUNT(*) FROM odds_snapshots WHERE last_confirmed_at IS NOT NULL` — should be non-zero
  </verify>
  <done>Write handler persists changed snapshots and updates last_confirmed_at on unchanged snapshots without timezone errors. All write batches succeed.</done>
</task>

<task type="auto">
  <name>Task 2: Investigate UAT-002 — on-demand scrape performance</name>
  <files>src/scraping/event_coordinator.py</files>
  <action>
Investigate whether Phase 55 introduced overhead to the on-demand (sync fallback) scrape path.

**Investigation steps:**

1. Read `store_batch_results()` and identify all code that runs in the sync path (when `_write_queue` is None)
2. Compare against git history to identify what changed vs pre-Phase 55 code
3. Check if reconciliation, EventBookmaker creation, or fallback tournament logic was added to the common path (runs before the async/sync split)
4. Add `perf_counter` timing to the sync path's flush+commit to log how long DB writes take:
   - Add `storage_flush_ms` and `storage_commit_ms` timing to the sync path's progress event
   - This helps determine if DB write time or something else is the bottleneck

**If overhead found in common path:**
- Document what was added and whether it can be optimized
- Log the finding but do NOT refactor — performance tuning is Phase 56's scope

**If no overhead found:**
- The perceived slowness may be due to network conditions during the test run
- Document findings and close UAT-002 as "not reproducible / needs baseline comparison"

**Do NOT:** Refactor the sync path, change scraping logic, or optimize performance. This task is investigation + diagnostic logging only.
  </action>
  <verify>
Run an on-demand scrape via the button. Check logs for new timing fields (storage_flush_ms, storage_commit_ms) in the sync path. Document findings.
  </verify>
  <done>Investigation complete. Either: (a) overhead identified and documented for Phase 56, or (b) no overhead found, UAT-002 closed with findings. Diagnostic timing added to sync path for future comparison.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All critical issues fixed (UAT-001 timezone mismatch)
- [ ] All major issues fixed (none)
- [ ] Minor issues investigated (UAT-002 performance)
- [ ] Original acceptance criteria from issues met
- [ ] Write queue successfully persists data to DB during scheduled scrape
</verification>

<success_criteria>
- UAT-001 resolved: write handler persists data without timezone errors
- UAT-002 investigated: root cause identified or ruled out
- Tests pass
- Ready for re-verification
</success_criteria>

<output>
After completion, create `.planning/phases/55-async-write-pipeline/55-03-FIX-SUMMARY.md`
</output>
