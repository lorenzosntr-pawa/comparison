---
phase: 55-async-write-pipeline
plan: 02
type: execute
---

<objective>
Create the async write queue infrastructure that decouples scraping from database persistence.

Purpose: Scraping should never wait for DB commits. The write queue processes snapshot inserts and timestamp updates in the background with automatic retry on failure.
Output: `src/storage/write_queue.py` with AsyncWriteQueue class (bounded queue, worker loop, retry+backoff, graceful shutdown) and `src/storage/write_handler.py` with the DB write logic.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/55-async-write-pipeline/55-CONTEXT.md

# Auto-selected based on dependency graph:
@.planning/phases/55-async-write-pipeline/55-01-SUMMARY.md
@.planning/phases/54-in-memory-cache/54-01-SUMMARY.md
@.planning/phases/54-in-memory-cache/54-02-SUMMARY.md

# Key files:
@src/db/session.py
@src/db/models/odds.py
@src/db/models/competitor.py
@src/scraping/event_coordinator.py
@src/caching/change_detection.py

**Tech stack available:** asyncio, SQLAlchemy 2.0 async, structlog
**Established patterns:** Frozen dataclass for data transfer, perf_counter timing, structlog structured logging
**Constraining decisions:**
- Phase 54: ORM objects cannot be shared across async sessions — use plain data objects for queue items
- Phase 55-01: `last_confirmed_at` column exists on OddsSnapshot and CompetitorOddsSnapshot
- Phase 55 CONTEXT: Queue should handle backpressure gracefully if writes fall behind
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AsyncWriteQueue class</name>
  <files>src/storage/__init__.py, src/storage/write_queue.py</files>
  <action>
Create `src/storage/` package and `src/storage/write_queue.py` with an AsyncWriteQueue class.

**Data structures for queue items** (use frozen dataclasses):
```python
@dataclass(frozen=True)
class SnapshotWriteData:
    """Plain data for creating an OddsSnapshot — no ORM dependency."""
    event_id: int
    bookmaker_id: int
    scrape_run_id: int | None
    raw_response: dict | None
    markets: tuple[MarketWriteData, ...]

@dataclass(frozen=True)
class MarketWriteData:
    """Plain data for creating a MarketOdds row."""
    betpawa_market_id: str
    betpawa_market_name: str
    line: float | None
    handicap_type: str | None
    handicap_home: float | None
    handicap_away: float | None
    outcomes: dict  # [{name, odds, is_active}, ...]
    market_groups: list[str] | None

@dataclass(frozen=True)
class CompetitorSnapshotWriteData:
    """Plain data for creating a CompetitorOddsSnapshot."""
    competitor_event_id: int
    scrape_run_id: int | None
    raw_response: dict | None
    markets: tuple[MarketWriteData, ...]

@dataclass(frozen=True)
class WriteBatch:
    """A complete batch of writes to process."""
    changed_betpawa: tuple[SnapshotWriteData, ...]
    changed_competitor: tuple[CompetitorSnapshotWriteData, ...]
    unchanged_betpawa_ids: tuple[int, ...]  # snapshot IDs for last_confirmed_at update
    unchanged_competitor_ids: tuple[int, ...]
    scrape_run_id: int | None
    batch_index: int
```

**AsyncWriteQueue class:**
```python
class AsyncWriteQueue:
    def __init__(self, session_factory, maxsize: int = 50):
        self._queue: asyncio.Queue[WriteBatch] = asyncio.Queue(maxsize=maxsize)
        self._session_factory = session_factory  # async_sessionmaker
        self._worker_task: asyncio.Task | None = None
        self._running = False
        self._log = structlog.get_logger("write_queue")

    async def start(self):
        """Start the background worker task."""
        self._running = True
        self._worker_task = asyncio.create_task(self._worker_loop())
        self._log.info("write_queue_started", maxsize=self._queue.maxsize)

    async def stop(self):
        """Signal stop, drain remaining items, then cancel worker."""
        self._running = False
        if self._worker_task:
            # Process remaining items before stopping
            await self._drain()
            self._worker_task.cancel()
            try:
                await self._worker_task
            except asyncio.CancelledError:
                pass
        self._log.info("write_queue_stopped")

    async def enqueue(self, batch: WriteBatch):
        """Add a write batch to the queue. Blocks if queue is full (backpressure)."""
        await self._queue.put(batch)
        self._log.debug("write_batch_enqueued", batch_index=batch.batch_index,
                        changed_bp=len(batch.changed_betpawa),
                        changed_comp=len(batch.changed_competitor),
                        unchanged_bp=len(batch.unchanged_betpawa_ids),
                        unchanged_comp=len(batch.unchanged_competitor_ids))

    async def _worker_loop(self):
        """Main worker: dequeue and process batches."""
        while self._running or not self._queue.empty():
            try:
                batch = await asyncio.wait_for(self._queue.get(), timeout=1.0)
            except asyncio.TimeoutError:
                continue
            await self._process_with_retry(batch)
            self._queue.task_done()

    async def _drain(self):
        """Process all remaining items in the queue."""
        while not self._queue.empty():
            batch = self._queue.get_nowait()
            await self._process_with_retry(batch)
            self._queue.task_done()

    def stats(self) -> dict:
        """Return queue statistics."""
        return {
            "queue_size": self._queue.qsize(),
            "queue_maxsize": self._queue.maxsize,
            "running": self._running,
        }
```

**Retry logic in `_process_with_retry`:**
- Max 3 attempts per batch
- Exponential backoff: 1s, 2s, 4s between retries
- On final failure: log error with full batch details (event IDs, counts), do NOT re-enqueue (avoid infinite loops)
- On success: log with perf_counter timing (write_ms)
- Call `self._handle_write(batch)` (the actual DB write — implemented in Task 2 as a method or imported handler)

Use `time.perf_counter()` for timing. Use `structlog` for all logging (consistent with project patterns).

Create `src/storage/__init__.py` exporting `AsyncWriteQueue`, `WriteBatch`, `SnapshotWriteData`, `CompetitorSnapshotWriteData`, `MarketWriteData`.
  </action>
  <verify>
Import test: `python -c "from src.storage import AsyncWriteQueue, WriteBatch, SnapshotWriteData; print('OK')"` — should print OK. Verify class has start(), stop(), enqueue(), stats() methods.
  </verify>
  <done>AsyncWriteQueue class with bounded queue, worker loop, retry with exponential backoff, graceful shutdown, and structured logging. All data classes importable from src.storage.</done>
</task>

<task type="auto">
  <name>Task 2: Create write handler with DB logic</name>
  <files>src/storage/write_handler.py, src/storage/write_queue.py</files>
  <action>
Create `src/storage/write_handler.py` with the `handle_write_batch()` function that performs the actual DB operations. Then wire it into AsyncWriteQueue._process_with_retry().

**Write handler function:**
```python
async def handle_write_batch(session_factory, batch: WriteBatch) -> dict:
    """
    Process a WriteBatch: INSERT changed snapshots, UPDATE unchanged timestamps.
    Opens its own DB session (isolated from scraping session).
    Returns stats dict with counts and timing.
    """
```

**Implementation steps:**
1. Open a new async session from `session_factory` (do NOT reuse the coordinator's session)
2. **INSERT changed BetPawa snapshots:**
   - For each `SnapshotWriteData` in `batch.changed_betpawa`:
     - Create `OddsSnapshot(event_id=..., bookmaker_id=..., scrape_run_id=..., raw_response=..., last_confirmed_at=now)`
     - `db.add(snapshot)`
   - `await db.flush()` to get snapshot IDs
   - For each snapshot + its markets:
     - Create `MarketOdds(snapshot_id=snapshot.id, ...)` for each `MarketWriteData`
     - `db.add(market)`
3. **INSERT changed competitor snapshots:** Same pattern with `CompetitorOddsSnapshot` and `CompetitorMarketOdds`
4. **UPDATE unchanged BetPawa timestamps:**
   - If `batch.unchanged_betpawa_ids` is non-empty:
   - `await db.execute(update(OddsSnapshot).where(OddsSnapshot.id.in_(list(batch.unchanged_betpawa_ids))).values(last_confirmed_at=datetime.utcnow()))`
5. **UPDATE unchanged competitor timestamps:** Same pattern with `CompetitorOddsSnapshot`
6. `await db.commit()`
7. Return stats dict: `{inserted_bp, inserted_comp, updated_bp, updated_comp, write_ms}`

**Wire into AsyncWriteQueue:** In `write_queue.py`, the `_process_with_retry` method should call `handle_write_batch(self._session_factory, batch)`. Import the handler at the top of write_queue.py.

**Error handling:**
- Wrap the entire handler in try/except
- On IntegrityError (e.g., duplicate key from concurrent writes): log warning, skip the conflicting record, continue with rest
- On OperationalError (DB connection issue): raise to trigger retry in `_process_with_retry`
- Always use `async with session_factory() as db:` context manager to ensure session cleanup
- Use `perf_counter` to time the entire write operation

**Important:** Do NOT store `raw_response` for unchanged snapshots — that data is already in the existing DB row. Only changed snapshots get raw_response stored.
  </action>
  <verify>
Import test: `python -c "from src.storage.write_handler import handle_write_batch; print('OK')"` — should print OK. Read the code and verify: opens own session, creates ORM objects from data classes, handles both INSERT and UPDATE paths, proper error handling, timing logged.
  </verify>
  <done>Write handler processes INSERT for changed snapshots and bulk UPDATE for unchanged timestamps. Own session, proper error handling, timing metrics. Wired into AsyncWriteQueue._process_with_retry().</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] AsyncWriteQueue starts and stops cleanly
- [ ] WriteBatch and data classes are all importable
- [ ] Write handler opens its own session (no session sharing)
- [ ] INSERT path creates OddsSnapshot + MarketOdds correctly
- [ ] UPDATE path sets last_confirmed_at on unchanged snapshot IDs
- [ ] Retry with backoff on OperationalError
- [ ] No import errors in src.storage package
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Write queue infrastructure ready for integration in Plan 55-03
- Handles both INSERT (changed) and UPDATE (unchanged) paths
  </success_criteria>

<output>
After completion, create `.planning/phases/55-async-write-pipeline/55-02-SUMMARY.md`
</output>
