---
phase: 37-event-coordination-layer
plan: 01
type: execute
---

<objective>
Create the EventCoordinator class with parallel event discovery and priority queue building.

Purpose: Enable simultaneous odds capture across all bookmakers by establishing the coordination layer that collects all events from all platforms and prioritizes them by kickoff urgency and comparison value.

Output: Working EventCoordinator with discovery integration and priority queue ready for Phase 38 parallel scraping implementation.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Architecture design from Phase 36:
@.planning/phases/36-investigation-architecture-design/ARCHITECTURE-DESIGN.md

# Phase context from discussion:
@.planning/phases/37-event-coordination-layer/37-CONTEXT.md

# Prior phase summary:
@.planning/phases/36-investigation-architecture-design/36-01-SUMMARY.md

# Existing clients to integrate with:
@src/scraping/clients/betpawa.py
@src/scraping/clients/sportybet.py
@src/scraping/clients/bet9ja.py

# Existing tournament discovery (reference pattern):
@src/scraping/tournament_discovery.py

# Existing competitor event scraping (reference fetch-then-store pattern):
@src/scraping/competitor_events.py

**Tech stack available:** Python 3.11+, asyncio, httpx, Pydantic v2, heapq
**Established patterns:**
- Fetch-then-store (parallel API → sequential DB) from Phase 15
- Async clients with semaphore concurrency control
- Frozen Pydantic models with ConfigDict

**Constraining decisions from Phase 36:**
- Preserve fetch-then-store pattern
- Semaphore limits: BetPawa 50, SportyBet 50, Bet9ja 15+25ms delay
- Priority: kickoff ASC, coverage DESC, has_betpawa DESC
- Batch size of 50 events

**Phase 37/38 split:**
- Phase 37: EventCoordinator core (discovery + queue) ← THIS PLAN
- Phase 38: Parallel scraping (scrape_batch, _scrape_event_all_platforms)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create coordinator data structures</name>
  <files>src/scraping/schemas/coordinator.py</files>
  <action>
Create new file with coordinator-specific data structures from ARCHITECTURE-DESIGN.md:

1. **ScrapeStatus** enum (StrEnum for DB compatibility):
   - PENDING, IN_PROGRESS, COMPLETED, FAILED

2. **EventTarget** dataclass with:
   - sr_id: str (SportRadar ID)
   - kickoff: datetime (event start time)
   - platforms: set[str] (available bookmakers: {"betpawa", "sportybet", "bet9ja"})
   - status: ScrapeStatus = PENDING
   - results: dict[str, dict | None] = field(default_factory=dict)
   - errors: dict[str, str] = field(default_factory=dict)
   - Properties: coverage_count, has_betpawa
   - Method: priority_key() returning tuple for heapq ordering

3. **ScrapeBatch** TypedDict:
   - batch_id: str
   - events: list[EventTarget]
   - created_at: datetime

Use dataclass for EventTarget (mutable during scraping), TypedDict for ScrapeBatch (simple dict structure).

**Priority key implementation** (from ARCHITECTURE-DESIGN.md):
```python
def priority_key(self) -> tuple:
    """Lower values = higher priority for min-heap."""
    now = datetime.now(timezone.utc)
    minutes_until = (self.kickoff - now).total_seconds() / 60

    # Urgency tier (0 = imminent, 1 = soon, 2 = future)
    if minutes_until < 30:
        urgency = 0
    elif minutes_until < 120:
        urgency = 1
    else:
        urgency = 2

    return (
        urgency,
        self.kickoff,
        -self.coverage_count,  # More coverage = higher priority
        not self.has_betpawa,  # BetPawa presence = higher priority
    )
```

AVOID: Using Pydantic models for EventTarget since it needs mutability during scrape cycle. Dataclass is appropriate here.
  </action>
  <verify>python -c "from src.scraping.schemas.coordinator import ScrapeStatus, EventTarget, ScrapeBatch; print('Import OK')"</verify>
  <done>All three data structures importable, priority_key returns comparable tuple</done>
</task>

<task type="auto">
  <name>Task 2: Create EventCoordinator with discovery methods</name>
  <files>src/scraping/event_coordinator.py</files>
  <action>
Create new EventCoordinator class implementing parallel discovery from all 3 platforms.

**Class structure:**
```python
class EventCoordinator:
    def __init__(
        self,
        betpawa_client: BetPawaClient,
        sportybet_client: SportyBetClient,
        bet9ja_client: Bet9jaClient,
        batch_size: int = 50,
    ) -> None:
        self._clients = {...}
        self._batch_size = batch_size
        self._event_map: dict[str, EventTarget] = {}
        self._priority_queue: list[EventTarget] = []
```

**Discovery methods:**

1. **discover_events()** - Main entry point:
   - Run _discover_betpawa(), _discover_sportybet(), _discover_bet9ja() in parallel via asyncio.gather(return_exceptions=True)
   - Merge results into unified _event_map keyed by SR ID
   - If SR ID already in map, add platform to existing EventTarget.platforms set
   - Log discovery counts per platform
   - Return the event map

2. **_discover_betpawa()** - BetPawa events:
   - Call fetch_categories() to get competition list
   - For each competition, call fetch_events() with sem=5 (from current code)
   - Extract SR ID from widgets array (type=SPORTRADAR)
   - Filter out started events (kickoff < now)
   - Return list of dicts with {sr_id, kickoff}
   - Handle exceptions gracefully, return empty list on failure

3. **_discover_sportybet()** - SportyBet events:
   - Call fetch_tournaments() to get tournament structure
   - For each tournament, call fetch_events() or similar
   - Extract SR ID from eventId (format: "sr:match:12345678" → "12345678")
   - Filter out started events
   - Return list of dicts with {sr_id, kickoff}

4. **_discover_bet9ja()** - Bet9ja events:
   - Call fetch_sports() to get tournament structure
   - Extract events from tournament data
   - Extract SR ID from EXTID field
   - Filter out started events
   - Return list of dicts with {sr_id, kickoff}

**Key implementation details:**
- Use existing client methods where available
- For SportyBet/Bet9ja, may need to adapt existing CompetitorEventScrapingService patterns
- Each _discover method should be resilient - log errors, return empty list, don't block other platforms
- Pre-match focus: exclude events where kickoff <= datetime.now(timezone.utc)

AVOID: Hitting DB during discovery - this is pure API work. DB storage is Phase 39.
  </action>
  <verify>python -c "from src.scraping.event_coordinator import EventCoordinator; print('Import OK')"</verify>
  <done>EventCoordinator class importable with discover_events() method that runs parallel discovery</done>
</task>

<task type="auto">
  <name>Task 3: Implement priority queue logic</name>
  <files>src/scraping/event_coordinator.py</files>
  <action>
Add priority queue building and batch extraction to EventCoordinator.

**Methods to implement:**

1. **build_priority_queue()** - Build heap from discovered events:
   - Convert _event_map values to list
   - Use heapq.heapify with key function (sort by priority_key())
   - Note: Python heapq doesn't support key parameter directly, so use a wrapper or sort-then-heapify
   - Store in _priority_queue
   - Return the queue (for chaining/inspection)

   Implementation approach (heapq workaround):
   ```python
   def build_priority_queue(self) -> list[EventTarget]:
       # heapq doesn't support key, so we sort by priority_key
       self._priority_queue = sorted(
           self._event_map.values(),
           key=lambda e: e.priority_key()
       )
       return self._priority_queue
   ```

2. **get_next_batch()** - Extract next batch from queue:
   - If queue empty, return None
   - Pop up to batch_size events from front of queue
   - Return ScrapeBatch dict with batch_id, events, created_at
   - batch_id format: f"batch_{uuid.uuid4().hex[:8]}" or timestamp-based

3. **has_pending_events()** - Check if queue has events:
   - Return bool(self._priority_queue)

4. **get_queue_stats()** - For observability:
   - Return dict with total_events, events_by_urgency, events_by_platform_count
   - Useful for debugging and SSE progress

**Queue behavior:**
- Events processed in priority order (imminent kickoffs first, then by coverage)
- Once an event is popped via get_next_batch(), it's removed from queue
- Batch size default 50 (configurable in __init__)

AVOID: Using heapq.heappop in a loop for get_next_batch - list slicing is cleaner since we pre-sorted. Reserve heapq for if we need dynamic insertion later.
  </action>
  <verify>python -c "
from src.scraping.event_coordinator import EventCoordinator
from src.scraping.clients.betpawa import BetPawaClient
from src.scraping.clients.sportybet import SportyBetClient
from src.scraping.clients.bet9ja import Bet9jaClient
import httpx

# Create mock coordinator (won't actually call APIs)
# Just verify the class structure is correct
print('EventCoordinator structure verified')
"</verify>
  <done>build_priority_queue() and get_next_batch() implemented, queue stats available</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `python -c "from src.scraping.schemas.coordinator import *"` imports successfully
- [ ] `python -c "from src.scraping.event_coordinator import EventCoordinator"` imports successfully
- [ ] EventTarget.priority_key() returns comparable tuples
- [ ] No type errors: `cd src && python -m py_compile scraping/event_coordinator.py scraping/schemas/coordinator.py`
- [ ] Unit test basic priority ordering (imminent > soon > future, 3 platforms > 2 > 1)
</verification>

<success_criteria>
- All tasks completed
- EventTarget, ScrapeBatch, ScrapeStatus data structures created
- EventCoordinator class with discover_events() method
- Priority queue building and batch extraction working
- Ready for Phase 38 parallel scraping implementation
</success_criteria>

<output>
After completion, create `.planning/phases/37-event-coordination-layer/37-01-SUMMARY.md` following the summary template with:

---
phase: 37-event-coordination-layer
plan: 01
subsystem: scraping
tags: [event-coordinator, discovery, priority-queue, async]

# Dependency graph
requires:
  - phase: 36-investigation-architecture-design
    provides: EventCoordinator architecture design
provides:
  - EventTarget, ScrapeBatch, ScrapeStatus data structures
  - EventCoordinator with parallel discovery
  - Priority queue building and batch extraction
affects: [38, 39, 40, 41, 42]

# Tech tracking
tech-stack:
  added: []
  patterns:
    - EventCoordinator for event-centric scraping coordination
    - Priority queue with kickoff urgency + coverage value

key-files:
  created:
    - src/scraping/schemas/coordinator.py
    - src/scraping/event_coordinator.py

key-decisions: []
patterns-established: []
issues-created: []

# Metrics
duration: Xmin
completed: YYYY-MM-DD
---
</output>
