---
phase: 03-scraper-integration
plan: 02
type: execute
domain: fastapi
---

<objective>
Create async scraper clients for SportyBet and BetPawa with tenacity retry logic.

Purpose: Convert existing sync scrapers to async for use in FastAPI, enabling concurrent execution.
Output: Async SportyBet and BetPawa clients with identical functionality to sync versions.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-scraper-integration/03-RESEARCH.md
@.planning/phases/03-scraper-integration/03-01-SUMMARY.md
@scraper/src/sportybet_scraper/client.py
@scraper/src/sportybet_scraper/config.py
@scraper/src/betpawa_scraper/client.py
@scraper/src/betpawa_scraper/config.py
@scraper/src/betpawa_scraper/models.py

**From RESEARCH.md:**
- tenacity @retry decorator works with async functions unchanged
- Convert `client.get()` to `await client.get()`
- Keep validation helpers sync (they don't do I/O)
- Reuse exception classes from scraper package

**Async conversion pattern:**
```python
# Before
def fetch_event(client: httpx.Client, event_id: str) -> dict:
    response = client.get(...)

# After
async def fetch_event(client: httpx.AsyncClient, event_id: str) -> dict:
    response = await client.get(...)
```
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create base async client protocol and shared exceptions</name>
  <files>src/scraping/__init__.py, src/scraping/clients/__init__.py, src/scraping/clients/base.py, src/scraping/exceptions.py</files>
  <action>
1. Create `src/scraping/__init__.py` (empty)
2. Create `src/scraping/clients/__init__.py` (empty)

3. Create `src/scraping/exceptions.py` with exception hierarchy:
   - `ScraperError(Exception)` - base class with message attribute
   - `InvalidEventIdError(ScraperError)` - event not found (include event_id)
   - `NetworkError(ScraperError)` - connection/timeout failures
   - `ApiError(ScraperError)` - unexpected response structure (include details)
   - `RateLimitError(ScraperError)` - rate limited by API

4. Create `src/scraping/clients/base.py` with:
   - Protocol class `ScraperClient` defining the interface:
     - `async def fetch_event(self, event_id: str) -> dict` - fetch single event
     - `async def check_health(self) -> bool` - connectivity check
   - Shared retry configuration constants:
     - MAX_RETRIES = 3
     - RETRY_MIN_WAIT = 1.0
     - RETRY_MAX_WAIT = 10.0
     - RETRY_MULTIPLIER = 2.0
   - Helper function `create_retry_decorator()` returning configured tenacity @retry:
     - retry_if_exception_type((httpx.HTTPStatusError, httpx.TimeoutException, httpx.ConnectError))
     - stop_after_attempt(MAX_RETRIES)
     - wait_exponential(multiplier=RETRY_MULTIPLIER, min=RETRY_MIN_WAIT, max=RETRY_MAX_WAIT)
  </action>
  <verify>python -c "from src.scraping.exceptions import ScraperError, InvalidEventIdError; from src.scraping.clients.base import create_retry_decorator; print('OK')"</verify>
  <done>Exception classes and base protocol import successfully</done>
</task>

<task type="auto">
  <name>Task 2: Create async SportyBet client</name>
  <files>src/scraping/clients/sportybet.py</files>
  <action>
Create async SportyBet client adapted from scraper/src/sportybet_scraper/client.py:

1. Import httpx, tenacity, exceptions from src.scraping.exceptions, retry decorator from base

2. Create `SportyBetClient` class:
   - `__init__(self, client: httpx.AsyncClient)` - stores client reference
   - `async def fetch_event(self, event_id: str) -> dict` with @retry decorator:
     - GET /api/ng/factsCenter/event with params: eventId={event_id}, productType=1
     - Call response.raise_for_status()
     - Parse JSON, validate with _validate_response_structure()
     - Return data["data"] (the inner payload)
     - On validation failure, raise ApiError
   - `async def check_health(self) -> bool`:
     - Try GET /api/ng/factsCenter/sportsMenu/list with timeout=5
     - Return True if 200, False otherwise
     - Catch exceptions, return False

3. Create `_validate_response_structure(data: dict, event_id: str) -> None` helper (sync):
   - Check data.get("bizCode") == 10000
   - Check "data" key exists
   - Raise ApiError with details if validation fails
   - Raise InvalidEventIdError if bizCode indicates not found

Keep same validation logic as original sync version.
  </action>
  <verify>python -c "from src.scraping.clients.sportybet import SportyBetClient; print('SportyBetClient imported')"</verify>
  <done>SportyBetClient class imports, has fetch_event and check_health methods</done>
</task>

<task type="auto">
  <name>Task 3: Create async BetPawa client</name>
  <files>src/scraping/clients/betpawa.py</files>
  <action>
Create async BetPawa client adapted from scraper/src/betpawa_scraper/client.py:

1. Import httpx, tenacity, json, urllib.parse.quote, exceptions, retry decorator

2. Create `BetPawaClient` class:
   - `__init__(self, client: httpx.AsyncClient)`

   - `async def fetch_event(self, event_id: str) -> dict` with @retry:
     - GET /api/sportsbook/v3/events/{event_id}
     - Parse JSON, validate structure
     - Return full response dict (contains markets, participants, widgets)

   - `async def fetch_events(self, competition_id: str, state: str = "upcoming", page: int = 1, size: int = 100) -> dict` with @retry:
     - Build query: {"competitionIds": [competition_id], "state": state, "page": page, "size": size}
     - URL encode as q parameter: /api/sportsbook/v3/events/lists/by-queries?q={encoded}
     - Return full response with events list and pagination

   - `async def fetch_categories(self, category_id: str = "2") -> dict` with @retry:
     - GET /api/sportsbook/v3/categories/list/{category_id}
     - Return categories/regions structure

   - `async def check_health(self) -> bool`:
     - Try GET /api/sportsbook/v3/categories/list/2 with timeout=5
     - Return True if 200, False otherwise

3. Create `_validate_response_structure(data: dict) -> None` helper:
   - Basic structure checks (dict with expected keys)
   - Raise ApiError if unexpected format

Note: Don't parse into dataclasses here - return raw dicts. Parsing happens in orchestrator layer.
  </action>
  <verify>python -c "from src.scraping.clients.betpawa import BetPawaClient; print('BetPawaClient imported')"</verify>
  <done>BetPawaClient class imports, has fetch_event, fetch_events, fetch_categories, check_health methods</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All imports work: `from src.scraping.clients.sportybet import SportyBetClient`
- [ ] All imports work: `from src.scraping.clients.betpawa import BetPawaClient`
- [ ] Exception classes defined in src/scraping/exceptions.py
- [ ] Retry decorator configured in base.py
</verification>

<success_criteria>
- SportyBetClient with async fetch_event, check_health
- BetPawaClient with async fetch_event, fetch_events, fetch_categories, check_health
- Shared exception hierarchy (ScraperError, InvalidEventIdError, NetworkError, ApiError)
- Retry decorator with exponential backoff configured
</success_criteria>

<output>
After completion, create `.planning/phases/03-scraper-integration/03-02-SUMMARY.md`
</output>
