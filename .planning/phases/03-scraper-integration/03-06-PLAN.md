---
phase: 03-scraper-integration
plan: 06
type: execute
domain: fastapi
---

<objective>
Integrate scraping with database to persist scrape runs, errors, and odds snapshots.

Purpose: Store scrape results for tracking, debugging, and future comparison features.
Output: Full database integration - ScrapeRun, ScrapeError, OddsSnapshot records created during scraping.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-scraper-integration/03-CONTEXT.md
@.planning/phases/03-scraper-integration/03-05-SUMMARY.md
@.planning/phases/02-database-schema/02-02-SUMMARY.md
@src/db/models/scrape.py
@src/db/models/odds.py
@src/db/engine.py

**From Phase 2:**
- ScrapeRun model with status, started_at, completed_at, events_scraped, events_failed, trigger
- ScrapeError model with scrape_run_id, bookmaker_id, event_id, error_type, error_message
- OddsSnapshot model with event_id, bookmaker_id, captured_at, scrape_run_id, raw_response
- MarketOdds model with snapshot_id, betpawa_market_id, outcomes JSONB
- ScrapeStatus enum: PENDING, RUNNING, COMPLETED, PARTIAL, FAILED

**From 03-CONTEXT.md:**
- Store both raw platform-specific data AND mapped unified format
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ScrapeRun on scrape start, update on completion</name>
  <files>src/api/routes/scrape.py, src/scraping/orchestrator.py</files>
  <action>
Update POST /scrape endpoint to create and update ScrapeRun records:

1. At start of trigger_scrape():
```python
from src.db.models.scrape import ScrapeRun, ScrapeStatus
from datetime import datetime

# Create ScrapeRun record
scrape_run = ScrapeRun(
    status=ScrapeStatus.RUNNING,
    trigger="manual",  # or "api" - can be parameterized later
)
db.add(scrape_run)
await db.commit()
await db.refresh(scrape_run)
scrape_run_id = scrape_run.id
```

2. Pass scrape_run_id to orchestrator:
```python
result = await orchestrator.scrape_all(
    ...,
    scrape_run_id=scrape_run_id,
    db=db,  # Pass session for error logging
)
```

3. After scrape completes, update ScrapeRun:
```python
# Map orchestrator status to ScrapeStatus
status_map = {
    "completed": ScrapeStatus.COMPLETED,
    "partial": ScrapeStatus.PARTIAL,
    "failed": ScrapeStatus.FAILED,
}

scrape_run.status = status_map[result.status]
scrape_run.completed_at = datetime.utcnow()
scrape_run.events_scraped = result.total_events
scrape_run.events_failed = sum(
    1 for p in result.platforms if not p.success
)
await db.commit()
```

4. Update orchestrator signature to accept scrape_run_id and db session.
  </action>
  <verify>python -c "from src.api.routes.scrape import trigger_scrape; print('Endpoint updated')"</verify>
  <done>ScrapeRun created at start, updated with final status/counts at end</done>
</task>

<task type="auto">
  <name>Task 2: Store ScrapeError records for failures</name>
  <files>src/scraping/orchestrator.py</files>
  <action>
Update orchestrator to log errors to database:

1. Add helper method to ScrapingOrchestrator:
```python
async def _log_error(
    self,
    db: AsyncSession,
    scrape_run_id: int,
    platform: Platform,
    error: Exception,
    event_id: int | None = None,
) -> None:
    """Log scrape error to database."""
    from src.db.models.scrape import ScrapeError
    from src.db.models.bookmaker import Bookmaker

    # Get bookmaker ID for platform (or create mapping)
    # For now, use platform name as error context
    bookmaker_id = await self._get_bookmaker_id(db, platform)

    error_type = type(error).__name__  # e.g., "TimeoutException", "ApiError"

    scrape_error = ScrapeError(
        scrape_run_id=scrape_run_id,
        bookmaker_id=bookmaker_id,
        event_id=event_id,
        error_type=error_type,
        error_message=str(error)[:1000],  # Truncate long messages
    )
    db.add(scrape_error)
    # Don't commit here - batch commit at end
```

2. In scrape_all(), after gathering results, log any exceptions:
```python
for platform, result in zip(platforms_to_scrape, results):
    if isinstance(result, Exception):
        await self._log_error(db, scrape_run_id, platform, result)
```

3. Add bookmaker lookup helper:
```python
async def _get_bookmaker_id(self, db: AsyncSession, platform: Platform) -> int | None:
    """Get or create bookmaker record for platform."""
    from sqlalchemy import select
    from src.db.models.bookmaker import Bookmaker

    result = await db.execute(
        select(Bookmaker).where(Bookmaker.code == platform.value)
    )
    bookmaker = result.scalar_one_or_none()
    if bookmaker:
        return bookmaker.id

    # Create if not exists (first run scenario)
    bookmaker = Bookmaker(
        name=platform.value.title(),
        code=platform.value,
        base_url=...  # Platform-specific
    )
    db.add(bookmaker)
    await db.flush()
    return bookmaker.id
```
  </action>
  <verify>python -c "from src.scraping.orchestrator import ScrapingOrchestrator; print('Orchestrator has error logging')"</verify>
  <done>Scrape errors logged to database with platform, error type, and message</done>
</task>

<task type="auto">
  <name>Task 3: Store OddsSnapshot and MarketOdds from results</name>
  <files>src/scraping/orchestrator.py, src/api/routes/scrape.py</files>
  <action>
Add odds storage to orchestrator:

1. Create helper to store snapshot for an event:
```python
async def _store_odds_snapshot(
    self,
    db: AsyncSession,
    scrape_run_id: int,
    event_id: int,  # Our internal event ID
    bookmaker_id: int,
    raw_response: dict,
    markets: list[dict],  # Normalized market data
) -> int:
    """Store odds snapshot and market odds, return snapshot ID."""
    from src.db.models.odds import OddsSnapshot, MarketOdds

    snapshot = OddsSnapshot(
        event_id=event_id,
        bookmaker_id=bookmaker_id,
        scrape_run_id=scrape_run_id,
        raw_response=raw_response,
    )
    db.add(snapshot)
    await db.flush()  # Get snapshot.id

    for market in markets:
        market_odds = MarketOdds(
            snapshot_id=snapshot.id,
            betpawa_market_id=market["market_id"],
            betpawa_market_name=market["market_name"],
            line=market.get("line"),
            handicap_type=market.get("handicap_type"),
            handicap_home=market.get("handicap_home"),
            handicap_away=market.get("handicap_away"),
            outcomes=market["outcomes"],  # JSONB
        )
        db.add(market_odds)

    return snapshot.id
```

2. Note: Full market mapping integration requires the Phase 1 market_mapping module.
   For now, store raw_response and basic market structure.
   Market normalization will be enhanced when event matching (Phase 4) is implemented.

3. Update GET /scrape/{id} endpoint to query actual data:
```python
@router.get("/{scrape_run_id}", response_model=ScrapeStatusResponse)
async def get_scrape_status(
    scrape_run_id: int,
    db: AsyncSession = Depends(get_db),
) -> ScrapeStatusResponse:
    from sqlalchemy import select
    from src.db.models.scrape import ScrapeRun

    result = await db.execute(
        select(ScrapeRun).where(ScrapeRun.id == scrape_run_id)
    )
    scrape_run = result.scalar_one_or_none()

    if not scrape_run:
        raise HTTPException(status_code=404, detail="Scrape run not found")

    return ScrapeStatusResponse(
        scrape_run_id=scrape_run.id,
        status=scrape_run.status.value,
        started_at=scrape_run.started_at,
        completed_at=scrape_run.completed_at,
        events_scraped=scrape_run.events_scraped,
        events_failed=scrape_run.events_failed,
    )
```
  </action>
  <verify>python -c "from src.api.routes.scrape import get_scrape_status; print('GET endpoint implemented')"</verify>
  <done>OddsSnapshot and MarketOdds stored, GET /scrape/{id} queries real data</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] POST /scrape creates ScrapeRun record
- [ ] ScrapeRun updated with final status and counts
- [ ] ScrapeError records created for platform failures
- [ ] GET /scrape/{id} returns actual data from database
- [ ] All imports work without circular dependencies
</verification>

<success_criteria>
- ScrapeRun lifecycle: created -> updated with results
- ScrapeError logged for each platform failure
- OddsSnapshot storage structure in place (full mapping in Phase 4)
- GET /scrape/{id} returns real scrape run data
- Phase 3 complete - scraper integration with database persistence
</success_criteria>

<output>
After completion, create `.planning/phases/03-scraper-integration/03-06-SUMMARY.md`:

# Phase 3 Plan 6: Database Integration Summary

**[One-liner summarizing database integration]**

## Accomplishments
- ScrapeRun record lifecycle
- ScrapeError logging
- OddsSnapshot storage
- GET /scrape/{id} implementation

## Files Created/Modified
- src/api/routes/scrape.py - DB integration
- src/scraping/orchestrator.py - Error logging, snapshot storage

## Decisions Made
[Key decisions]

## Issues Encountered
[Problems and resolutions]

## Next Phase Readiness
Phase 3 complete. Ready for Phase 4: Event Matching Service.
</output>
