---
phase: 03-scraper-integration
plan: 04
type: execute
domain: fastapi
---

<objective>
Create scrape API endpoints for triggering and monitoring scrape operations.

Purpose: Expose the scraping orchestrator via REST API for on-demand scraping.
Output: POST /scrape to trigger scraping, GET /scrape/{id} to check status.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-scraper-integration/03-CONTEXT.md
@.planning/phases/03-scraper-integration/03-03-SUMMARY.md
@src/scraping/orchestrator.py
@src/scraping/schemas.py
@src/api/app.py
@src/api/dependencies.py

**From 03-CONTEXT.md:**
- Response should be configurable â€” summary vs full data via query param
- Filter by platform (e.g., just SportyBet)
- Configurable timeouts per request

**From 03-RESEARCH.md code examples:**
```python
@router.post("/scrape")
async def scrape_events(
    request: Request,
    timeout: int = Query(default=30, ge=5, le=300),
):
```
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create POST /scrape endpoint</name>
  <files>src/api/routes/scrape.py, src/api/schemas.py</files>
  <action>
1. Create `src/api/schemas.py` with request/response models:

```python
from pydantic import BaseModel, Field
from datetime import datetime
from src.scraping.schemas import Platform, PlatformResult

class ScrapeRequest(BaseModel):
    platforms: list[Platform] | None = Field(
        default=None,
        description="Platforms to scrape. Default: all three"
    )
    sport_id: str | None = Field(
        default=None,
        description="Filter by sport ID (e.g., '2' for football)"
    )
    competition_id: str | None = Field(
        default=None,
        description="Filter by specific competition"
    )

class ScrapeResponse(BaseModel):
    scrape_run_id: int
    status: str  # "completed", "partial", "failed"
    started_at: datetime
    completed_at: datetime
    platforms: list[PlatformResult]
    total_events: int
    # Full data only if requested via query param
    events: list[dict] | None = Field(
        default=None,
        description="Full event data, only if detail=full"
    )
```

2. Create `src/api/routes/scrape.py`:

```python
from fastapi import APIRouter, Request, Query, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession

from src.api.schemas import ScrapeRequest, ScrapeResponse
from src.api.dependencies import get_db
from src.scraping.orchestrator import ScrapingOrchestrator
from src.scraping.clients import SportyBetClient, BetPawaClient, Bet9jaClient

router = APIRouter(prefix="/scrape", tags=["scrape"])

@router.post("", response_model=ScrapeResponse)
async def trigger_scrape(
    request: Request,
    body: ScrapeRequest = None,
    detail: str = Query(default="summary", enum=["summary", "full"]),
    timeout: int = Query(default=30, ge=5, le=300, description="Timeout per platform in seconds"),
    db: AsyncSession = Depends(get_db),
) -> ScrapeResponse:
    """
    Trigger a scrape operation across selected platforms.

    - **platforms**: Which platforms to scrape (default: all)
    - **detail**: "summary" for counts only, "full" to include all event data
    - **timeout**: Max seconds per platform (default 30, max 300)
    """
    # 1. Create ScrapeRun record in DB (status=RUNNING)
    # 2. Build orchestrator from request.state clients
    # 3. Call orchestrator.scrape_all(platforms, include_data=(detail=="full"), timeout)
    # 4. Update ScrapeRun with final status and counts
    # 5. Return ScrapeResponse

    # For now, implement without DB (add in Plan 06)
    sportybet = SportyBetClient(request.state.sportybet_client)
    betpawa = BetPawaClient(request.state.betpawa_client)
    bet9ja = Bet9jaClient(request.state.bet9ja_client)

    orchestrator = ScrapingOrchestrator(sportybet, betpawa, bet9ja)

    result = await orchestrator.scrape_all(
        platforms=body.platforms if body else None,
        include_data=(detail == "full"),
        timeout=float(timeout),
    )

    return ScrapeResponse(
        scrape_run_id=0,  # Placeholder until DB integration
        status=result.status,
        started_at=result.started_at,
        completed_at=result.completed_at,
        platforms=result.platforms,
        total_events=result.total_events,
        events=... if detail == "full" else None,  # Extract from platform results
    )
```

3. Update `src/api/app.py` to include the scrape router.
  </action>
  <verify>python -c "from src.api.routes.scrape import router; print([r.path for r in router.routes])"</verify>
  <done>POST /scrape endpoint defined, router imports successfully</done>
</task>

<task type="auto">
  <name>Task 2: Create GET /scrape/{id} status endpoint</name>
  <files>src/api/routes/scrape.py</files>
  <action>
Add status endpoint to src/api/routes/scrape.py:

```python
class ScrapeStatusResponse(BaseModel):
    scrape_run_id: int
    status: str
    started_at: datetime
    completed_at: datetime | None
    events_scraped: int
    events_failed: int
    platforms: list[PlatformResult] | None = None

@router.get("/{scrape_run_id}", response_model=ScrapeStatusResponse)
async def get_scrape_status(
    scrape_run_id: int,
    db: AsyncSession = Depends(get_db),
) -> ScrapeStatusResponse:
    """
    Get status of a scrape run by ID.

    Returns current status, counts, and optionally platform-level details.
    """
    # Query ScrapeRun by ID
    # If not found, raise HTTPException(404)
    # Return status response

    # Placeholder implementation until DB integration:
    raise HTTPException(status_code=501, detail="DB integration pending - see Plan 06")
```

Also add to src/api/schemas.py:
- ScrapeStatusResponse model (as shown above)

This endpoint will be fully implemented in Plan 06 when we add database integration.
  </action>
  <verify>python -c "from src.api.routes.scrape import router; paths = [r.path for r in router.routes]; print(paths); assert '/scrape/{scrape_run_id}' in paths or '/{scrape_run_id}' in paths"</verify>
  <done>GET /scrape/{id} endpoint defined, returns 501 until DB integration</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] POST /scrape endpoint registered
- [ ] GET /scrape/{id} endpoint registered
- [ ] Scrape router included in main app
- [ ] `python -c "from src.api.app import create_app; app = create_app()"` works
</verification>

<success_criteria>
- POST /scrape triggers orchestrator with configurable platforms, detail, timeout
- GET /scrape/{id} endpoint defined (placeholder until Plan 06)
- Request/response Pydantic models defined
- Router integrated into main app
</success_criteria>

<output>
After completion, create `.planning/phases/03-scraper-integration/03-04-SUMMARY.md`
</output>
