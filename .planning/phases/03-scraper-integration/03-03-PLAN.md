---
phase: 03-scraper-integration
plan: 03
type: execute
domain: fastapi
---

<objective>
Create async Bet9ja client and scraping orchestrator for concurrent execution with partial failure handling.

Purpose: Complete the async client set and provide orchestration layer that runs all scrapers concurrently.
Output: Bet9ja client and orchestrator that handles concurrent scraping with partial failure tolerance.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-scraper-integration/03-RESEARCH.md
@.planning/phases/03-scraper-integration/03-02-SUMMARY.md
@scraper/src/bet9ja_scraper/client.py
@scraper/src/bet9ja_scraper/config.py
@scraper/src/bet9ja_scraper/models.py

**From RESEARCH.md - Concurrent execution pattern:**
```python
results = await asyncio.gather(*tasks, return_exceptions=True)
# Check isinstance(result, Exception) for each result
```

**From 03-CONTEXT.md:**
- Scrapers should run independently - if SportyBet fails, BetPawa and Bet9ja still complete
- Partial success is fine; report what worked and what didn't
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create async Bet9ja client</name>
  <files>src/scraping/clients/bet9ja.py, src/scraping/clients/__init__.py</files>
  <action>
Create async Bet9ja client adapted from scraper/src/bet9ja_scraper/client.py:

1. Create `Bet9jaClient` class in src/scraping/clients/bet9ja.py:
   - `__init__(self, client: httpx.AsyncClient)`

   - `async def fetch_event(self, event_id: str) -> dict` with @retry:
     - GET /desktop/feapi/PalimpsestAjax/GetEvent
     - Params: eventID={event_id}, lng=en
     - Validate response has R="D" (success) and D key
     - Return D (the data payload) which contains event details + odds dict
     - Raise InvalidEventIdError if event not found
     - Raise ApiError if unexpected structure

   - `async def fetch_events(self, tournament_id: str) -> list[dict]` with @retry:
     - GET /desktop/feapi/PalimpsestAjax/GetEventsInGroupV2
     - Params: groupId={tournament_id}, lng=en, skinName=bet9ja
     - Return list of event dicts

   - `async def fetch_sports(self) -> dict` with @retry:
     - GET /desktop/feapi/PalimpsestAjax/GetSports
     - Params: lng=en, skinName=bet9ja, timeFilter=all
     - Return navigation structure

   - `async def check_health(self) -> bool`:
     - Try fetch_sports() with timeout=5
     - Return True if succeeds, False otherwise

2. Update `src/scraping/clients/__init__.py` to export all clients:
   - from .sportybet import SportyBetClient
   - from .betpawa import BetPawaClient
   - from .bet9ja import Bet9jaClient
   - __all__ = ["SportyBetClient", "BetPawaClient", "Bet9jaClient"]
  </action>
  <verify>python -c "from src.scraping.clients import SportyBetClient, BetPawaClient, Bet9jaClient; print('All clients imported')"</verify>
  <done>All three client classes import from src.scraping.clients</done>
</task>

<task type="auto">
  <name>Task 2: Create scraping orchestrator with partial failure handling</name>
  <files>src/scraping/orchestrator.py, src/scraping/schemas.py</files>
  <action>
1. Create `src/scraping/schemas.py` with Pydantic models for orchestration:

```python
from enum import StrEnum
from pydantic import BaseModel, Field
from datetime import datetime

class Platform(StrEnum):
    SPORTYBET = "sportybet"
    BETPAWA = "betpawa"
    BET9JA = "bet9ja"

class PlatformResult(BaseModel):
    platform: Platform
    success: bool
    events_count: int = 0
    error_message: str | None = None
    duration_ms: int
    events: list[dict] | None = Field(default=None, description="Raw events if include_data=True")

class ScrapeResult(BaseModel):
    status: str  # "completed", "partial", "failed"
    started_at: datetime
    completed_at: datetime
    platforms: list[PlatformResult]
    total_events: int = Field(description="Sum across successful platforms")
```

2. Create `src/scraping/orchestrator.py`:

```python
class ScrapingOrchestrator:
    def __init__(
        self,
        sportybet_client: SportyBetClient,
        betpawa_client: BetPawaClient,
        bet9ja_client: Bet9jaClient,
    ):
        self.clients = {
            Platform.SPORTYBET: sportybet_client,
            Platform.BETPAWA: betpawa_client,
            Platform.BET9JA: bet9ja_client,
        }

    async def scrape_all(
        self,
        platforms: list[Platform] | None = None,
        include_data: bool = False,
        timeout: float = 30.0,
    ) -> ScrapeResult:
        """
        Scrape all (or specified) platforms concurrently.

        Uses asyncio.gather(return_exceptions=True) for partial failure tolerance.
        If one platform fails, others continue and complete.
        """
        # Implementation:
        # 1. Filter to requested platforms (default all)
        # 2. Create tasks for each platform's scrape operation
        # 3. Use asyncio.gather(return_exceptions=True)
        # 4. Process results - check isinstance(result, Exception)
        # 5. Build PlatformResult for each (success or failure)
        # 6. Determine overall status: completed (all success), partial (some), failed (none)
        # 7. Return ScrapeResult

    async def _scrape_platform(
        self,
        platform: Platform,
        include_data: bool,
        timeout: float,
    ) -> tuple[list[dict], int]:
        """Scrape a single platform, return (events, duration_ms)."""
        # For now, use fetch_events from BetPawa to discover events
        # Then fetch individual events from each platform
        # This is a simplified version - full implementation will be enhanced
```

Key implementation details:
- Use `asyncio.wait_for(task, timeout=timeout)` to enforce per-platform timeout
- Catch all exceptions in gather results, convert to PlatformResult with error
- Track timing with `time.perf_counter()` or `datetime.now()`
- Status logic: all success = "completed", any success = "partial", none = "failed"
  </action>
  <verify>python -c "from src.scraping.orchestrator import ScrapingOrchestrator; from src.scraping.schemas import ScrapeResult, Platform; print('Orchestrator imported')"</verify>
  <done>ScrapingOrchestrator and schemas import, orchestrator has scrape_all method</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `from src.scraping.clients import Bet9jaClient` works
- [ ] `from src.scraping.orchestrator import ScrapingOrchestrator` works
- [ ] `from src.scraping.schemas import ScrapeResult, Platform, PlatformResult` works
- [ ] Orchestrator uses asyncio.gather with return_exceptions=True
</verification>

<success_criteria>
- Bet9jaClient with async fetch_event, fetch_events, fetch_sports, check_health
- ScrapingOrchestrator with scrape_all method
- Partial failure handling (one platform failing doesn't stop others)
- Pydantic schemas for scrape results
</success_criteria>

<output>
After completion, create `.planning/phases/03-scraper-integration/03-03-SUMMARY.md`
</output>
