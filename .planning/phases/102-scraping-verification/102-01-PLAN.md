---
phase: 102-scraping-verification
plan: 01
type: execute
---

<objective>
Verify scraping pipeline works correctly after raw_response column removal from Phase 101.

Purpose: Ensure the schema changes didn't break data collection or storage.
Output: Verification report confirming scraping and data integrity are intact.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/101-schema-implementation/101-01-SUMMARY.md

**Phase 101 Changes:**
- Removed raw_response from OddsSnapshot and CompetitorOddsSnapshot models
- Removed raw_response from SnapshotWriteData and CompetitorSnapshotWriteData DTOs
- Removed raw_response from scraping code (competitor_events.py, event_coordinator.py)
- Migration m9n5o1p7q3r9 dropped columns from database

**Verification Approach:**
1. Run a full scrape cycle
2. Query database to verify data stored correctly
3. Test API endpoints return expected data
4. User confirms UI displays correctly
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run end-to-end scrape validation</name>
  <files>scripts/verify_scraping.py (create)</files>
  <action>
Create a verification script that:
1. Triggers a scrape via the EventCoordinator (like benchmark_pipeline.py)
2. Waits for completion
3. Verifies ScrapeRun status is COMPLETED (not FAILED)
4. Counts events scraped and confirms > 0
5. Checks for any errors logged

Script should:
- Use async_session_factory for DB access
- Create EventCoordinator with settings
- Run scrape_all_events()
- Query ScrapeRun table for the run status
- Output clear PASS/FAIL results

Run the script and verify scraping completes successfully.
  </action>
  <verify>python scripts/verify_scraping.py returns success, scrape run status is COMPLETED</verify>
  <done>Scrape completes with COMPLETED status, events_scraped > 0, no critical errors</done>
</task>

<task type="auto">
  <name>Task 2: Validate database data integrity</name>
  <files>scripts/verify_scraping.py (extend)</files>
  <action>
Extend the verification script to check data integrity:

1. **Verify raw_response column is gone:**
   - Query information_schema.columns for odds_snapshots
   - Confirm raw_response column does not exist
   - Same check for competitor_odds_snapshots

2. **Verify data is stored correctly:**
   - Query odds_snapshots: count recent records (last hour)
   - Query market_odds: verify outcomes JSON is populated
   - Query competitor_odds_snapshots: count recent records
   - Query competitor_market_odds: verify outcomes JSON is populated

3. **Sample data verification:**
   - Fetch one recent odds_snapshot with its market_odds
   - Verify market_odds.outcomes contains outcome data (not empty)
   - Verify market_odds.betpawa_market_id is set

Output verification results with counts and sample data check.
  </action>
  <verify>Script outputs data integrity checks all PASS, raw_response confirmed removed, outcomes data present</verify>
  <done>Database schema verified (no raw_response), recent data exists with valid outcomes JSON</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Verification script that runs scrape and validates database integrity</what-built>
  <how-to-verify>
    1. Ensure Docker is running: `docker ps` should show postgres container
    2. Start the backend: `cd src && python -m uvicorn main:app --reload`
    3. Start the frontend: `cd frontend && npm run dev`
    4. Open http://localhost:5173/odds-comparison
    5. Verify:
       - Page loads without errors
       - Events are displayed in the table
       - Odds values are shown (not all dashes)
       - No console errors (F12 -> Console)
    6. Click on an event to open Event Details
    7. Verify:
       - Market data is displayed
       - Outcomes show odds values
  </how-to-verify>
  <resume-signal>Type "approved" if UI works correctly, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] verify_scraping.py created and runs successfully
- [ ] Scrape completes with COMPLETED status
- [ ] raw_response column confirmed removed from both tables
- [ ] odds_snapshots and market_odds have recent data
- [ ] competitor_odds_snapshots and competitor_market_odds have recent data
- [ ] market_odds.outcomes JSON contains valid outcome data
- [ ] UI loads and displays odds data correctly
</verification>

<success_criteria>

- Scrape pipeline works end-to-end without raw_response
- Data integrity verified via SQL queries
- UI confirms odds display correctly
- No errors or regressions from Phase 101 changes
</success_criteria>

<output>
After completion, create `.planning/phases/102-scraping-verification/102-01-SUMMARY.md`:

# Phase 102 Plan 01: Scraping Verification Summary

**[Substantive one-liner about verification results]**

## Accomplishments

- [Verification results]

## Files Created/Modified

- `scripts/verify_scraping.py` - Verification script

## Decisions Made

[Any decisions during verification, or "None"]

## Issues Encountered

[Problems found and how they were resolved, or "None"]

## Next Phase Readiness

[Ready for Phase 103: Data Migration & Validation]
</output>
