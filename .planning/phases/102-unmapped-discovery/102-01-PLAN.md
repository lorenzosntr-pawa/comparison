---
phase: 102-unmapped-discovery
plan: 01
type: execute
---

<objective>
Create UnmappedLogger service and integrate into scraping code to log unmapped markets during scrape runs.

Purpose: Enable automatic discovery of unmapped markets by capturing them when MappingError occurs.
Output: UnmappedLogger service with in-memory batching and DB persistence, integrated into scraping.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/100-investigation-design/DESIGN.md
@.planning/phases/101-backend-foundation/101-01-SUMMARY.md
@.planning/phases/101-backend-foundation/101-02-SUMMARY.md
@.planning/phases/101-backend-foundation/101-03-SUMMARY.md

# Key files
@src/db/models/mapping.py  # UnmappedMarketLog model
@src/scraping/event_coordinator.py  # Lines 2893-2895: MappingError catch point
@src/scraping/competitor_events.py  # Lines 324-326: MappingError catch point
@src/market_mapping/cache.py  # MappingCache pattern to follow

**Tech stack available:**
- SQLAlchemy async with Mapped[] columns
- Frozen dataclasses for cache entries
- structlog for logging
- asyncio.Lock for thread safety

**Established patterns:**
- Frozen dataclass cache entries (v2.0 OddsCache pattern)
- Module-level singleton instances (mapping_cache)
- Batch write to DB with session isolation
- Deduplication via unique constraints

**Constraining decisions:**
- Phase 101: UnmappedMarketLog table has UNIQUE(source, external_market_id)
- Phase 101: Status values: NEW, ACKNOWLEDGED, MAPPED, IGNORED
- v2.0: Frozen dataclass pattern for in-memory storage

**Integration points:**
- event_coordinator.py:2893 catches MappingError for SportyBet markets
- competitor_events.py:324 catches MappingError for SportyBet markets
- Both currently just `pass` - need to log unmapped market data
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create UnmappedLogger service with batch write</name>
  <files>src/market_mapping/unmapped_logger.py</files>
  <action>
Create UnmappedLogger class with:

1. **UnmappedEntry frozen dataclass:**
   - source: str ("sportybet" or "bet9ja")
   - external_market_id: str
   - market_name: Optional[str]
   - sample_outcomes: Optional[list[dict]]
   - seen_at: datetime

2. **UnmappedLogger class:**
   - `_pending: dict[tuple[str, str], UnmappedEntry]` - keyed by (source, market_id) for dedup
   - `_lock: asyncio.Lock` - for thread-safe access
   - `_new_markets: list[UnmappedEntry]` - NEW markets discovered this cycle (for WebSocket alerts later)

   Methods:
   - `log(entry: UnmappedEntry) -> bool` - adds to pending, returns True if NEW (not seen before in this session)
   - `async flush(session: AsyncSession) -> int` - upserts all pending to DB, updates last_seen_at and occurrence_count for existing, inserts new, clears pending, returns count of NEW insertions
   - `get_new_markets() -> list[UnmappedEntry]` - returns _new_markets list (for WebSocket alerts)
   - `clear_new_markets()` - clears _new_markets after alerts sent

   flush() implementation:
   - For each pending entry:
     - Check if exists in DB (SELECT by source, external_market_id)
     - If exists: UPDATE last_seen_at = NOW(), occurrence_count += 1
     - If not exists: INSERT new row, add to _new_markets list
   - Use INSERT ... ON CONFLICT for efficiency if PostgreSQL

3. **Module-level singleton:**
   - `unmapped_logger = UnmappedLogger()` at bottom of file

Follow existing patterns from mapping_cache.py (frozen dataclass, asyncio.Lock, module singleton).

Do NOT use complex batch operations - keep it simple with individual upserts for clarity.
  </action>
  <verify>python -c "from src.market_mapping.unmapped_logger import unmapped_logger, UnmappedEntry; print('OK')"</verify>
  <done>UnmappedLogger class with log(), flush(), get_new_markets() methods. Module singleton accessible.</done>
</task>

<task type="auto">
  <name>Task 2: Integrate logger into scraping code</name>
  <files>src/scraping/event_coordinator.py, src/scraping/competitor_events.py</files>
  <action>
Modify both files to log unmapped markets when MappingError occurs:

1. **event_coordinator.py** (around line 2893):
   - Add import: `from src.market_mapping.unmapped_logger import unmapped_logger, UnmappedEntry`
   - In the `except MappingError` block for SportyBet:
     ```python
     except MappingError:
         # Log unmapped market for discovery
         unmapped_logger.log(UnmappedEntry(
             source="sportybet",
             external_market_id=str(market.id),
             market_name=market.name,
             sample_outcomes=[{"desc": o.desc, "odds": o.odds} for o in market.outcomes[:3]],
             seen_at=datetime.now(timezone.utc),
         ))
     ```
   - Do same for Bet9ja markets (find the except MappingError block for bet9ja parsing)

2. **competitor_events.py** (around line 324):
   - Same pattern: import and log UnmappedEntry on MappingError
   - For SportyBet: capture market.id, market.name, sample outcomes
   - For Bet9ja: capture the key and outcome data

3. **Flush at end of scrape cycle:**
   - In event_coordinator.py, find the end of `run_full_cycle()` or `scrape_batch()`
   - Add after all events processed:
     ```python
     # Flush unmapped markets to DB
     new_count = await unmapped_logger.flush(db)
     if new_count > 0:
         logger.info("unmapped_markets.discovered", count=new_count)
     ```

Make sure to extract market info before the MappingError - the error happens inside the mapper, but we have access to the raw market data in the calling code.

Do NOT change the flow - just add logging inside the existing except blocks.
  </action>
  <verify>grep -n "unmapped_logger" src/scraping/event_coordinator.py src/scraping/competitor_events.py</verify>
  <done>Both scraping files log unmapped markets when MappingError occurs. Flush happens after scrape batch.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] UnmappedLogger class follows frozen dataclass pattern
- [ ] Module singleton accessible
- [ ] Both scraping files import and use unmapped_logger
- [ ] MappingError blocks capture market info before logging
- [ ] Flush called at end of scrape cycle
- [ ] python -c import succeeds
</verification>

<success_criteria>

- UnmappedLogger service created with correct patterns
- Scraping code logs unmapped markets when MappingError occurs
- Market name and sample outcomes captured for reference
- No changes to existing scraping flow (just added logging)
</success_criteria>

<output>
After completion, create `.planning/phases/102-unmapped-discovery/102-01-SUMMARY.md`
</output>
