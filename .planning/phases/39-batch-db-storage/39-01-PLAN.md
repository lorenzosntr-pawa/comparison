---
phase: 39-batch-db-storage
plan: 01
type: execute
---

<objective>
Implement batch database storage for the EventCoordinator with per-event status tracking.

Purpose: Complete the scrape-to-storage pipeline for the new event-centric architecture by adding bulk insert patterns and per-event status tracking, enabling full observability of scrape results.

Output: EventScrapeStatus model with migration, store_batch_results() method with bulk inserts, and a complete run_full_cycle() orchestration method.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase summaries (dependency chain)
@.planning/phases/36-investigation-architecture-design/36-01-SUMMARY.md
@.planning/phases/37-event-coordination-layer/37-01-SUMMARY.md
@.planning/phases/38-sr-id-parallel-scraping/38-01-SUMMARY.md

# Architecture design reference
@.planning/phases/36-investigation-architecture-design/ARCHITECTURE-DESIGN.md

# Key files (current implementation)
@src/scraping/event_coordinator.py
@src/scraping/schemas/coordinator.py
@src/db/models/odds.py
@src/db/models/event.py
@src/db/models/competitor.py
@src/db/models/scrape.py

# Existing storage patterns (fetch-then-store reference)
@src/scraping/competitor_events.py

**Tech stack available:** SQLAlchemy 2.0 async, Pydantic v2, structlog
**Established patterns:** Fetch-then-store (Phase 1: API parallel, Phase 2: DB sequential)
**Key insight:** AsyncSession cannot be shared across concurrent asyncio tasks - storage must be sequential after parallel API phase

**Constraining decisions:**
- Phase 36: Preserve fetch-then-store pattern from v1.1
- Phase 36: Batch size of 50 events for optimal throughput
- Phase 38: Results populated in EventTarget.results and EventTarget.errors after scrape_batch()
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create EventScrapeStatus model and migration</name>
  <files>src/db/models/event_scrape_status.py, src/db/models/__init__.py, alembic/versions/xxx_add_event_scrape_status.py</files>
  <action>
Create new SQLAlchemy model for per-event scrape status tracking:

1. Create `src/db/models/event_scrape_status.py`:
   - Class: `EventScrapeStatus(Base)`
   - Fields:
     - `id: Mapped[int]` - primary key
     - `scrape_run_id: Mapped[int]` - FK to scrape_runs.id
     - `sportradar_id: Mapped[str]` - SR ID of the event (String(50))
     - `status: Mapped[str]` - "completed" or "failed" (String(20))
     - `platforms_requested: Mapped[list]` - JSON list of platforms that were attempted
     - `platforms_scraped: Mapped[list]` - JSON list of platforms that succeeded
     - `platforms_failed: Mapped[list]` - JSON list of platforms that failed
     - `timing_ms: Mapped[int]` - scrape duration for this event
     - `error_details: Mapped[dict | None]` - JSON of platform -> error message (nullable)
     - `created_at: Mapped[datetime]` - server_default=func.now()
   - Indexes:
     - `idx_event_scrape_status_run` on scrape_run_id
     - `idx_event_scrape_status_sr_id` on sportradar_id

2. Export from `src/db/models/__init__.py`

3. Create Alembic migration:
   - `alembic revision --autogenerate -m "add_event_scrape_status"`
   - Verify migration looks correct
   - Run `alembic upgrade head`

Use Mapped[] type hints with SQLAlchemy 2.0 pattern. Use JSON type for list fields (not ARRAY - for PostgreSQL compatibility with the existing patterns).
  </action>
  <verify>
   - `python -c "from src.db.models.event_scrape_status import EventScrapeStatus; print('OK')"`
   - `alembic current` shows latest migration applied
   - `psql -c "SELECT * FROM event_scrape_status LIMIT 1"` runs without error (empty table OK)
  </verify>
  <done>EventScrapeStatus model exists, exports correctly, migration applied, table exists in database</done>
</task>

<task type="auto">
  <name>Task 2: Implement store_batch_results() with bulk insert patterns</name>
  <files>src/scraping/event_coordinator.py</files>
  <action>
Add `store_batch_results()` method to EventCoordinator that persists scraped data using bulk insert patterns.

1. Add method signature:
   ```python
   async def store_batch_results(
       self,
       db: AsyncSession,
       batch: ScrapeBatch,
       scrape_run_id: int,
   ) -> dict:
   ```

2. Implementation pattern (fetch-then-store Phase 2):
   - Collect all EventScrapeStatus records in a list
   - For each event in batch["events"]:
     - Create EventScrapeStatus record with status, platforms info, timing
     - For each platform in event.results:
       - Parse the raw API response using existing parsers (reference competitor_events.py patterns)
       - Build snapshot and market objects for bulk insert
   - Use `db.execute(insert(EventScrapeStatus), status_records)` for bulk insert
   - Sequential commit after all records prepared

3. Storage routing:
   - BetPawa results -> OddsSnapshot + MarketOdds (existing tables)
   - SportyBet results -> CompetitorOddsSnapshot + CompetitorMarketOdds
   - Bet9ja results -> CompetitorOddsSnapshot + CompetitorMarketOdds

4. Handle edge cases:
   - Skip events with status=FAILED (no results to store)
   - Log events with partial success (some platforms failed)
   - Return summary dict: {events_stored, snapshots_created, errors}

5. Import EventScrapeStatus and necessary DB models at top of file.

Key pattern: Prepare all records first, then bulk insert. Do NOT insert during loop iteration - this avoids session conflicts.
  </action>
  <verify>
   - File compiles: `python -c "from src.scraping.event_coordinator import EventCoordinator; print('OK')"`
   - No type errors: `pyright src/scraping/event_coordinator.py`
  </verify>
  <done>store_batch_results() method exists, accepts batch and db session, uses bulk insert pattern, creates EventScrapeStatus records</done>
</task>

<task type="auto">
  <name>Task 3: Add run_full_cycle() orchestration method</name>
  <files>src/scraping/event_coordinator.py</files>
  <action>
Add `run_full_cycle()` async generator that orchestrates the complete scrape cycle:

1. Add method signature:
   ```python
   async def run_full_cycle(
       self,
       db: AsyncSession,
       scrape_run_id: int,
   ) -> AsyncGenerator[dict, None]:
   ```

2. Implementation flow:
   ```python
   # Phase 1: Discovery
   yield {"event_type": "CYCLE_START", "scrape_run_id": scrape_run_id}

   await self.discover_events()
   yield {
       "event_type": "DISCOVERY_COMPLETE",
       "discovery_counts": {...},
       "total_events": len(self._event_map),
   }

   # Phase 2: Build queue
   self.build_priority_queue()
   batch_count = (len(self._priority_queue) + self._batch_size - 1) // self._batch_size

   # Phase 3: Process batches
   batch_index = 0
   events_scraped = 0
   events_failed = 0
   total_start = time.perf_counter()

   while batch := self.get_next_batch():
       yield {
           "event_type": "BATCH_START",
           "batch_id": batch["batch_id"],
           "batch_index": batch_index,
           "event_count": len(batch["events"]),
       }

       # Scrape and yield per-event progress
       async for progress in self.scrape_batch(batch):
           yield progress

       # Store results
       store_result = await self.store_batch_results(db, batch, scrape_run_id)

       yield {
           "event_type": "BATCH_COMPLETE",
           "batch_id": batch["batch_id"],
           "events_stored": store_result["events_stored"],
           "snapshots_created": store_result["snapshots_created"],
       }

       batch_index += 1
       events_scraped += store_result["events_stored"]

   # Phase 4: Cycle complete
   total_ms = int((time.perf_counter() - total_start) * 1000)
   yield {
       "event_type": "CYCLE_COMPLETE",
       "total_events": len(self._event_map),
       "events_scraped": events_scraped,
       "total_timing_ms": total_ms,
   }

   # Clear for next cycle
   self.clear()
   ```

3. Add import for `time` module if not present.

This method provides the complete pipeline for integration with the orchestrator in Phase 40.
  </action>
  <verify>
   - File compiles: `python -c "from src.scraping.event_coordinator import EventCoordinator; print('OK')"`
   - Method exists: `python -c "from src.scraping.event_coordinator import EventCoordinator; assert hasattr(EventCoordinator, 'run_full_cycle'); print('OK')"`
  </verify>
  <done>run_full_cycle() async generator exists, orchestrates discovery->queue->scrape->store cycle, yields SSE-compatible progress events</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] EventScrapeStatus model created with correct fields
- [ ] Alembic migration applied, table exists
- [ ] store_batch_results() method implemented with bulk insert
- [ ] run_full_cycle() method orchestrates full pipeline
- [ ] All Python files compile without errors
- [ ] No pyright/mypy type errors
</verification>

<success_criteria>

- All 3 tasks completed
- All verification checks pass
- EventCoordinator has complete scrape-to-storage pipeline
- Per-event status tracking is available for observability
- Bulk insert pattern used (not individual inserts)
</success_criteria>

<output>
After completion, create `.planning/phases/39-batch-db-storage/39-01-SUMMARY.md` following the summary template with frontmatter including:
- phase, plan, subsystem: scraping
- tags: [bulk-insert, status-tracking, event-coordinator]
- requires: 38-sr-id-parallel-scraping
- provides: store_batch_results, run_full_cycle, EventScrapeStatus model
- affects: [40-concurrency-tuning, 41-on-demand-api]
- key-files: created (event_scrape_status.py, migration), modified (event_coordinator.py)
- key-decisions: List any implementation decisions made
</output>
