---
phase: 06.1-cross-platform-scraping
plan: 01
type: execute
---

<objective>
Complete SportyBet and Bet9ja scraping in the orchestrator using SportRadar ID matching.

Purpose: Enable cross-platform odds comparison by fetching events from all three bookmakers and storing raw API responses for later market mapping.
Output: Working orchestrator that scrapes BetPawa, SportyBet, and Bet9ja in sequence, storing raw_response in OddsSnapshot and creating EventBookmaker links.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase research (completed):
@.planning/phases/06.1-cross-platform-scraping/06.1-RESEARCH.md
@.planning/phases/06.1-cross-platform-scraping/06.1-CONTEXT.md

# Related prior summaries:
@.planning/phases/03-scraper-integration/03-06-SUMMARY.md
@.planning/phases/04-event-matching/04-01-SUMMARY.md

# Key source files:
@src/scraping/orchestrator.py
@src/scraping/clients/sportybet.py
@src/scraping/clients/bet9ja.py
@src/matching/service.py
@src/db/models/event.py
@src/db/models/odds.py

**Tech stack available:** httpx, tenacity, SQLAlchemy 2.0 async, APScheduler
**Established patterns:**
- EventMatchingService for upsert_event, upsert_event_bookmaker
- `_get_bookmaker_id()` for auto-creating bookmaker records
- `create_retry_decorator()` for tenacity retry logic
- BetPawa-first metadata priority (competitors only update kickoff)
- `asyncio.gather(return_exceptions=True)` for partial failure tolerance

**Constraining decisions:**
- Store raw API responses in OddsSnapshot.raw_response for maximum flexibility
- Apply market mapping on read, not on capture
- BetPawa scraped first (existing), then competitors query DB for SportRadar IDs

**Issues being addressed:** ISS-001 (SportyBet/Bet9ja scraping not implemented)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement SportyBet scraping via SportRadar ID lookup</name>
  <files>src/scraping/orchestrator.py</files>
  <action>
Replace the stub `_scrape_platform` case for `Platform.SPORTYBET` with real implementation:

1. Add helper method `_scrape_sportybet(self, client: SportyBetClient, db: AsyncSession) -> list[dict]`:
   - Query DB for events with `sportradar_id IS NOT NULL` and `kickoff > datetime.utcnow()`
   - For each event, convert SportRadar ID to SportyBet format: `f"sr:match:{event.sportradar_id}"`
   - Call `client.fetch_event(sportybet_id)` with try/except for InvalidEventIdError (event doesn't exist on SportyBet - skip)
   - Add 0.1s delay between requests to avoid rate limiting: `await asyncio.sleep(0.1)`
   - Return list of dicts with: sportradar_id, external_event_id, event_url, raw_data, event (db reference)

2. Update `_scrape_platform` for SPORTYBET case to call `_scrape_sportybet(client, db)` - requires db parameter pass-through

3. Update `_store_events` to handle SportyBet events:
   - Use existing EventMatchingService.upsert_event_bookmaker for the link
   - Create OddsSnapshot with raw_response containing the API data
   - Import OddsSnapshot from src.db.models.odds

Note: SportyBet events don't need tournament handling - they're already in the database from BetPawa scraping. We just create the bookmaker link and store the raw odds.

Avoid: Don't fetch event metadata from SportyBet - we already have it from BetPawa. Competitors only update kickoff time via the existing upsert_event logic.
  </action>
  <verify>
Run the app: `cd c:\Users\loren\Desktop\betpawa\comparison\mvp && python -c "from src.scraping.orchestrator import ScrapingOrchestrator; print('Import OK')"`
Check for syntax errors in the modified file.
  </verify>
  <done>
- `_scrape_sportybet` method exists and queries DB for events with SportRadar IDs
- SportyBet ID format conversion works (sr:match:NNNNN)
- OddsSnapshot records created with raw_response from SportyBet API
- EventBookmaker links created for matched events
- Rate limiting delay (0.1s) between requests
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement Bet9ja scraping via tournament discovery</name>
  <files>src/scraping/orchestrator.py</files>
  <action>
Replace the stub `_scrape_platform` case for `Platform.BET9JA` with real implementation:

1. Add helper `_extract_football_tournaments(sports_data: dict) -> list[str]`:
   - Extract football tournament IDs from GetSports response
   - Navigate: `sports_data["D"]["PAL"]["1"]["SG"]` (Sport ID 1 = Soccer)
   - Return list of tournament IDs from the SG.*.G.* structure

2. Add helper method `_scrape_bet9ja(self, client: Bet9jaClient, db: AsyncSession) -> list[dict]`:
   - Call `client.fetch_sports()` to get all sports/tournaments
   - Extract football tournament IDs with `_extract_football_tournaments`
   - For each tournament, call `client.fetch_events(tournament_id)` with try/except
   - Add 0.2s delay between tournament requests to avoid rate limiting
   - For each event in response, extract EXTID (SportRadar ID) - skip if None
   - Parse: home_team/away_team from "DS" field split by " - "
   - Parse: kickoff from "STARTDATE" field (format: "%Y-%m-%d %H:%M:%S")
   - Build tournament data from response for EventMatchingService
   - Return list of dicts with: sportradar_id (from EXTID), external_event_id (C field), event_url, raw_data, name, home_team, away_team, kickoff, tournament

3. Update `_scrape_platform` for BET9JA case to call `_scrape_bet9ja(client, db)`

4. Update `_store_events` to handle Bet9ja events:
   - Bet9ja events may be NEW (not from BetPawa) - use full EventMatchingService.process_scraped_events flow
   - BUT with Platform.BET9JA so metadata is insert-only (Betpawa-first priority preserved)
   - After upsert, create OddsSnapshot with raw_response

Note: Bet9ja events that don't exist in BetPawa will be created as new events. This is intentional - we want to flag "competitor-only" events for review.

Avoid: Don't assume all Bet9ja events have EXTID - some older/minor events may not. Skip gracefully with logging.
  </action>
  <verify>
Run the app: `cd c:\Users\loren\Desktop\betpawa\comparison\mvp && python -c "from src.scraping.orchestrator import ScrapingOrchestrator; print('Import OK')"`
Check for syntax errors and verify all imports resolve.
  </verify>
  <done>
- `_scrape_bet9ja` method exists and discovers football tournaments
- `_extract_football_tournaments` helper parses PAL structure correctly
- EXTID field extracted as SportRadar ID (skip events without it)
- EventMatchingService used for full event upsert with Betpawa-first priority
- OddsSnapshot records created with raw_response from Bet9ja API
- Rate limiting delay (0.2s) between tournament requests
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `python -c "from src.scraping.orchestrator import ScrapingOrchestrator"` succeeds
- [ ] No Python syntax errors in orchestrator.py
- [ ] Both `_scrape_sportybet` and `_scrape_bet9ja` methods exist
- [ ] `_scrape_platform` dispatches to both new methods (not returning empty arrays)
- [ ] OddsSnapshot import added and used for storing raw responses
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No import errors or syntax errors
- ScrapingOrchestrator.scrape_all() will now scrape all three platforms
- Raw API responses stored in OddsSnapshot.raw_response for SportyBet and Bet9ja
- EventBookmaker links created for matched events
- ISS-001 resolved (SportyBet/Bet9ja scraping implemented)
</success_criteria>

<output>
After completion, create `.planning/phases/06.1-cross-platform-scraping/06.1-01-SUMMARY.md`:

# Phase 6.1 Plan 1: Cross-Platform Scraping Summary

**[Substantive one-liner about what shipped]**

## Performance

- **Duration:** X min
- **Tasks:** 2
- **Files modified:** 1 (src/scraping/orchestrator.py)

## Accomplishments

- SportyBet scraping via SportRadar ID lookup
- Bet9ja scraping via tournament discovery
- Raw API responses stored in OddsSnapshot
- EventBookmaker links created for cross-platform matching

## Task Commits

1. **Task 1: SportyBet scraping** - commit hash (feat)
2. **Task 2: Bet9ja scraping** - commit hash (feat)

## Files Created/Modified

- `src/scraping/orchestrator.py` - Added _scrape_sportybet, _scrape_bet9ja, _extract_football_tournaments

## Decisions Made

[Any decisions made during implementation]

## Issues Encountered

[Any problems and resolutions]

## Next Phase Readiness

- Phase 6.1 complete
- Cross-platform scraping working
- Ready for Phase 7: Match Views
</output>
