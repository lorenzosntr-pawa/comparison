---
phase: 06.1-cross-platform-scraping
plan: 01-FIX3
type: fix
---

<objective>
Fix 1 UAT issue from plan 06.1-01.

Source: 06.1-01-ISSUES.md
Priority: 1 blocker, 0 major, 0 minor

**Issue:** UAT-006 - BetPawa scrape times out (60s insufficient for 60+ competitions)
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md

**Issues being fixed:**
@.planning/phases/06.1-cross-platform-scraping/06.1-01-ISSUES.md

**Original plan for reference:**
@.planning/phases/06.1-cross-platform-scraping/06.1-01-PLAN.md

**Key source files:**
@src/scraping/orchestrator.py
@src/api/routes/scrape.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix timeout configuration for full scrape</name>
  <files>src/api/routes/scrape.py</files>
  <action>
Increase the default and maximum timeout for the scrape endpoint to accommodate full competition scraping:

1. In `trigger_scrape`, change the timeout Query parameter:
   - Default: 30 → 300 (5 minutes)
   - Maximum: 300 → 600 (10 minutes)

2. Update the description to reflect the new defaults.

This is a simple fix because BetPawa has 60+ competitions, each requiring ~1s for HTTP request + 0.1s rate limit delay. 60+ seconds minimum needed, plus buffer for network latency.

Rationale for 300s default:
- 60 competitions × ~1.5s per competition = ~90s minimum
- Network variability can double this
- 5 minutes (300s) provides adequate buffer
- 10 minute max (600s) allows for exceptional cases
  </action>
  <verify>
Check file: `grep -n "timeout" src/api/routes/scrape.py`
Verify default is 300 and max is 600.
  </verify>
  <done>
- Timeout default increased to 300 seconds
- Timeout maximum increased to 600 seconds
- BetPawa scrape has sufficient time to complete
  </done>
</task>

<task type="auto">
  <name>Task 2: Add meaningful error message for TimeoutError</name>
  <files>src/scraping/orchestrator.py</files>
  <action>
Improve error handling in `scrape_all` to provide meaningful messages for TimeoutError:

1. In the exception handling block (around line 93), add special handling for TimeoutError:

```python
if isinstance(result, Exception):
    # Handle TimeoutError specially (str(TimeoutError()) returns empty string in Python 3.11+)
    if isinstance(result, TimeoutError):
        error_message = f"Platform scrape timed out after {timeout}s - too many competitions or slow network"
    else:
        error_message = str(result) or f"Unknown error: {type(result).__name__}"
```

2. Use the new `error_message` variable instead of `str(result)` when creating PlatformResult.

This ensures:
- TimeoutError gets a descriptive message
- Other exceptions with empty str() also get a fallback message
- Users can diagnose why scrapes fail
  </action>
  <verify>
Check file: `grep -n "TimeoutError" src/scraping/orchestrator.py`
Verify the special handling exists.
  </verify>
  <done>
- TimeoutError now produces meaningful error message
- Empty error strings have fallback to exception type name
- Users can diagnose timeout issues from API response
  </done>
</task>

<task type="auto">
  <name>Task 3: Test the fix end-to-end</name>
  <files></files>
  <action>
Verify the fix by running a quick test:

1. Start the server: `python -m uvicorn src.api.app:create_app --factory --port 8000`

2. In another terminal, trigger a scrape with the new default timeout:
   `curl -s -X POST http://localhost:8000/scrape | python -m json.tool`

   OR using PowerShell:
   `(Invoke-WebRequest -Method POST -Uri 'http://localhost:8000/scrape').Content | ConvertFrom-Json | ConvertTo-Json -Depth 10`

3. Check the response:
   - BetPawa should show `success: true` with `events_count > 0`
   - If any platform times out, error_message should be non-empty

Note: This test may take 2-5 minutes due to BetPawa's 60+ competitions.
  </action>
  <verify>
Server starts without errors.
Scrape endpoint responds within the new timeout.
BetPawa shows success=true OR meaningful error message if still timing out.
  </verify>
  <done>
- Server runs without import errors
- Scrape endpoint accepts requests
- BetPawa scrape either succeeds OR shows meaningful timeout error
- No more empty error_message for timeout failures
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Timeout default is 300s, max is 600s in scrape.py
- [ ] TimeoutError handling added in orchestrator.py
- [ ] Server starts without errors
- [ ] Scrape response has meaningful error messages (not empty strings)
</verification>

<success_criteria>
- UAT-006 addressed: BetPawa has sufficient timeout and meaningful error messages
- Tests pass
- Ready for re-verification
</success_criteria>

<output>
After completion, create `.planning/phases/06.1-cross-platform-scraping/06.1-01-FIX3-SUMMARY.md`
</output>
