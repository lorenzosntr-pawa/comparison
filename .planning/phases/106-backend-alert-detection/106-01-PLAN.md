---
phase: 106-backend-alert-detection
plan: 01
type: execute
domain: backend
---

<objective>
Implement the risk detection engine that identifies significant odds movements, direction disagreements between bookmakers, and availability changes.

Purpose: Enable real-time risk monitoring by detecting concerning odds changes as they happen during scrape cycles.
Output: Working detection module integrated into the scraping pipeline, generating RiskAlertData DTOs for downstream persistence and broadcast.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/105-investigation-schema/DISCOVERY.md
@.planning/phases/105-investigation-schema/105-01-SUMMARY.md
@src/caching/change_detection.py
@src/caching/odds_cache.py
@src/storage/write_queue.py
@src/scraping/event_coordinator.py

**Tech stack available:**
- SQLAlchemy 2.0 with Mapped[] columns
- Pydantic v2 with frozen models
- structlog for logging
- Frozen dataclasses for DTOs (established pattern)

**Established patterns:**
- Frozen dataclass DTOs decouple detection from persistence (Phase 55, v2.0)
- Change detection module pattern: helper functions + main classify function (change_detection.py)
- Availability detection integration pattern (Phase 87, v2.5)

**Key decisions from Phase 105:**
- Integration point: store_batch_results() after classify_batch_changes() and _detect_and_log_availability_changes()
- Alert per outcome, not per market
- Direction disagreement always ELEVATED severity
- Availability alerts always WARNING severity
- Separate alert_retention_days from odds_retention_days
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create risk detection module and DTOs</name>
  <files>src/caching/risk_detection.py, src/storage/write_queue.py</files>
  <action>
Create the risk detection module following the algorithms in DISCOVERY.md:

**1. Create src/caching/risk_detection.py with:**

Enums:
- `AlertType(str, Enum)`: PRICE_CHANGE, DIRECTION_DISAGREEMENT, AVAILABILITY
- `AlertSeverity(str, Enum)`: WARNING, ELEVATED, CRITICAL

Helper functions (from DISCOVERY.md pseudocode):
- `calculate_outcome_change_percent(old_odds: float, new_odds: float) -> float`
- `classify_severity(change_percent, thresholds) -> AlertSeverity | None`
- `get_movement_direction(old_odds: float, new_odds: float) -> str | None` (returns "up", "down", or None)

Detection functions:
- `detect_price_change_alerts(...)` - compare cached vs new, create alerts for % changes exceeding thresholds
- `detect_direction_disagreement_alerts(...)` - detect when Betpawa moves opposite to competitors
- `convert_availability_to_alerts(...)` - transform availability changes to alerts

Main orchestrator:
- `detect_risk_alerts(cache, changed_bp, changed_comp, unavailable_bp, unavailable_comp, event_kickoffs, thresholds, timestamp)` - calls all detection functions, returns list[RiskAlertData]

Use NamedTuple `AlertThresholds` for threshold configuration (warning, elevated, critical percentages).

**2. Add RiskAlertData to src/storage/write_queue.py:**

```python
@dataclass(frozen=True)
class RiskAlertData:
    """DTO for risk alert creation."""
    event_id: int
    bookmaker_slug: str
    market_id: str
    market_name: str
    line: float | None
    outcome_name: str | None
    alert_type: str  # AlertType enum value
    severity: str    # AlertSeverity enum value
    change_percent: float
    old_value: float | None
    new_value: float | None
    competitor_direction: str | None
    detected_at: datetime
    event_kickoff: datetime
```

**What to avoid:**
- Do NOT import ORM models in risk_detection.py (keep it decoupled)
- Do NOT persist alerts in this phase (that's Phase 107)
- Do NOT broadcast via WebSocket in this phase (that's Phase 109)
- Use `from __future__ import annotations` for forward references
- Use existing CachedMarket type from odds_cache.py for type hints
  </action>
  <verify>
```bash
python -c "from src.caching.risk_detection import detect_risk_alerts, AlertType, AlertSeverity; print('Import OK')"
python -c "from src.storage.write_queue import RiskAlertData; print('DTO OK')"
```
  </verify>
  <done>
- risk_detection.py exists with all 3 detection algorithms
- RiskAlertData DTO added to write_queue.py
- All imports succeed without errors
- Module follows existing patterns (frozen dataclass, Enum, structlog)
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate detection into event coordinator</name>
  <files>src/scraping/event_coordinator.py</files>
  <action>
Integrate the detection module into the scraping pipeline at the identified point:

**1. Add imports at top of event_coordinator.py:**
```python
from src.caching.risk_detection import detect_risk_alerts, AlertThresholds
```

**2. After _detect_and_log_availability_changes() call (~line 1612), add detection call:**

```python
# 4. Detect risk alerts (Phase 106)
risk_alerts = detect_risk_alerts(
    cache=self._odds_cache,
    changed_bp=changed_bp,
    changed_comp=changed_comp,
    bp_write_data=bp_write_data,
    comp_write_data=comp_write_data,
    comp_meta=comp_meta,
    event_id_map=event_id_map,
    unavailable_bp=unavailable_bp,
    unavailable_comp=unavailable_comp,
    kickoff_by_sr=kickoff_by_sr,
    thresholds=AlertThresholds(
        warning=7.0,
        elevated=10.0,
        critical=15.0,
    ),  # TODO: Phase 111 will read from settings
    timestamp=now_naive,
)

if risk_alerts:
    logger.info(
        "risk_detection.alerts_detected",
        total=len(risk_alerts),
        price_change=sum(1 for a in risk_alerts if a.alert_type == "price_change"),
        direction=sum(1 for a in risk_alerts if a.alert_type == "direction_disagreement"),
        availability=sum(1 for a in risk_alerts if a.alert_type == "availability"),
    )
```

**What to avoid:**
- Do NOT add alerts to WriteBatch yet (Phase 107 handles persistence)
- Do NOT broadcast alerts yet (Phase 109 handles WebSocket)
- Do NOT read thresholds from settings yet (Phase 111 handles configuration)
- Keep the detection call simple and focused on logging
  </action>
  <verify>
Start the dev server and trigger a manual scrape. Check logs for `risk_detection.alerts_detected` entries:
```bash
# Start backend
cd src && uvicorn main:app --reload &
sleep 5
# Trigger scrape
curl -X POST http://localhost:8000/api/scrape
# Check logs for detection output
```
  </verify>
  <done>
- Detection integrated into store_batch_results()
- Logs show alert counts by type when changes detected
- No errors during scrape cycle
- Detection runs after change classification and availability detection
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `python -c "from src.caching import risk_detection; print('OK')"` passes
- [ ] risk_detection.py has all 3 detection algorithms
- [ ] RiskAlertData DTO exists in write_queue.py
- [ ] event_coordinator.py imports and calls detect_risk_alerts()
- [ ] Manual scrape shows detection logs (even if 0 alerts)
- [ ] No TypeErrors or import errors during scrape
</verification>

<success_criteria>

- All tasks completed
- Detection module created with full algorithm implementation
- Integration point working in event coordinator
- Logs show detection activity during scrape cycles
- Ready for Phase 107 (schema + persistence)
</success_criteria>

<output>
After completion, create `.planning/phases/106-backend-alert-detection/106-01-SUMMARY.md`:

# Phase 106 Plan 01: Backend Alert Detection Summary

**[Substantive one-liner]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `src/caching/risk_detection.py` - Description
- `src/storage/write_queue.py` - Description
- `src/scraping/event_coordinator.py` - Description

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for Phase 107: Alert Storage & API
</output>
