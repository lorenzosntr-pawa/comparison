---
phase: 100-investigation-analysis
plan: 01
type: execute
---

<objective>
Profile all database tables to measure sizes and growth patterns, identify storage drivers, and design an optimized schema strategy for reducing database from 20+ GB to under 10 GB.

Purpose: Provide data-driven foundation for storage optimization decisions in subsequent phases.
Output: DISCOVERY.md with complete profiling results, identified storage drivers, and recommended optimization strategy.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/60-investigation-schema-design/60-01-SUMMARY.md

**Prior Phase 60 findings (Feb 2026):**
- ~18 GB in 4 days at that time
- 15.1M market_odds rows, 90K snapshots
- 73% change rate (frequent new snapshots)
- ~3.1 MB/event/day growth

**Current milestone goal:** Reduce 20+ GB database to under 10 GB

**Key tables to profile:**
- `odds_snapshots` - has `raw_response` JSON (full API response)
- `market_odds` - has `outcomes` JSON, high volume
- `competitor_odds_snapshots` - has `raw_response` JSON
- `competitor_market_odds` - has `outcomes` JSON, high volume
- `scrape_runs`, `scrape_phase_logs`, `scrape_errors` - operational logs
- Event/tournament tables - reference data

**Database models:**
@src/db/models/odds.py
@src/db/models/competitor.py
@src/db/models/scrape.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run comprehensive database profiling</name>
  <files>.planning/phases/100-investigation-analysis/DISCOVERY.md</files>
  <action>
Execute SQL profiling queries against the database to gather storage metrics. Run queries via psql or Python script connecting to the database.

**Required metrics:**
1. **Table sizes** - pg_relation_size for all tables, including TOAST
2. **Row counts** - COUNT(*) for each table
3. **Index sizes** - pg_indexes_size for each table
4. **Column-level storage** - Estimate size of JSON columns (raw_response, outcomes, market_groups)
5. **Growth rate** - Compare snapshots by date range (daily row counts for last 7 days)
6. **Retention impact** - Rows older than 7 days, 14 days, 30 days

**Key SQL queries to run:**
```sql
-- Table sizes with TOAST
SELECT
  schemaname, tablename,
  pg_size_pretty(pg_table_size(schemaname||'.'||tablename)) as table_size,
  pg_size_pretty(pg_indexes_size(schemaname||'.'||tablename)) as index_size,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as total_size
FROM pg_tables WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Row counts
SELECT relname, reltuples::bigint FROM pg_class WHERE relnamespace = 'public'::regnamespace;

-- JSON column size estimates (sample-based)
SELECT
  'odds_snapshots.raw_response' as column,
  pg_size_pretty(SUM(pg_column_size(raw_response))) as estimated_size,
  COUNT(*) as rows_sampled
FROM odds_snapshots
WHERE id IN (SELECT id FROM odds_snapshots ORDER BY RANDOM() LIMIT 1000);

-- Growth by date (last 7 days)
SELECT DATE(captured_at) as date, COUNT(*) as snapshot_count
FROM odds_snapshots
WHERE captured_at > NOW() - INTERVAL '7 days'
GROUP BY DATE(captured_at) ORDER BY date;

-- Retention analysis
SELECT
  'Last 7 days' as period,
  COUNT(*) as snapshot_count,
  (SELECT COUNT(*) FROM market_odds mo
   JOIN odds_snapshots os ON mo.snapshot_id = os.id
   WHERE os.captured_at > NOW() - INTERVAL '7 days') as market_count
FROM odds_snapshots WHERE captured_at > NOW() - INTERVAL '7 days'
UNION ALL
SELECT 'Last 14 days', COUNT(*),
  (SELECT COUNT(*) FROM market_odds mo
   JOIN odds_snapshots os ON mo.snapshot_id = os.id
   WHERE os.captured_at > NOW() - INTERVAL '14 days')
FROM odds_snapshots WHERE captured_at > NOW() - INTERVAL '14 days'
UNION ALL
SELECT 'Last 30 days', COUNT(*),
  (SELECT COUNT(*) FROM market_odds mo
   JOIN odds_snapshots os ON mo.snapshot_id = os.id
   WHERE os.captured_at > NOW() - INTERVAL '30 days')
FROM odds_snapshots WHERE captured_at > NOW() - INTERVAL '30 days';
```

Document results in DISCOVERY.md with tables and analysis.
  </action>
  <verify>DISCOVERY.md contains table sizes, row counts, JSON column estimates, growth metrics</verify>
  <done>Complete profiling data documented with all required metrics</done>
</task>

<task type="auto">
  <name>Task 2: Identify storage drivers and optimization opportunities</name>
  <files>.planning/phases/100-investigation-analysis/DISCOVERY.md</files>
  <action>
Analyze profiling results to identify:

**Storage drivers analysis:**
1. Which tables consume the most space (likely odds_snapshots and market_odds)
2. Which columns within tables are largest (likely raw_response JSON)
3. What percentage of storage is indexes vs data
4. What is the data-to-index ratio

**Specific questions to answer:**
1. Is `raw_response` actually needed? What uses it? (Check if any code reads it after initial store)
2. Can `outcomes` JSON be normalized to reduce redundancy?
3. Are there duplicate snapshots (same odds stored multiple times)?
4. What is the actual retention policy in use vs what's needed?
5. Is TOAST compression effective on JSON columns?

**Code search for raw_response usage:**
```bash
grep -r "raw_response" src/ --include="*.py" | grep -v "__pycache__"
```

**Optimization opportunity categories:**
- **Eliminate**: Remove unused data (e.g., raw_response if not needed)
- **Compress**: TimescaleDB compression, PostgreSQL native compression
- **Normalize**: Extract repetitive JSON into lookup tables
- **Partition**: Better partition strategies for time-series data
- **Retain**: Aggressive retention policies (7-day operational, 30-day historical)

Document findings and rank opportunities by impact/effort.
  </action>
  <verify>DISCOVERY.md has "Storage Drivers" and "Optimization Opportunities" sections with ranked recommendations</verify>
  <done>Root causes identified, opportunities ranked by storage impact and implementation effort</done>
</task>

<task type="auto">
  <name>Task 3: Design optimization strategy</name>
  <files>.planning/phases/100-investigation-analysis/DISCOVERY.md</files>
  <action>
Based on analysis, design concrete optimization strategy for Phases 101-104.

**Evaluate strategies:**

1. **Remove raw_response** - If unused, drop column and migrate
   - Potential savings: Estimate based on column size analysis
   - Risk: Low if confirmed unused
   - Implementation: Phase 101

2. **TimescaleDB hypertables** - Convert odds_snapshots to hypertable
   - Benefits: Native compression (90%+ on time-series), automatic partitioning
   - Requires: TimescaleDB extension installation
   - Implementation: Phase 101

3. **Aggressive retention** - 7-day default, 30-day max historical
   - Benefits: Immediate space reclamation
   - Risk: Losing historical data (user has accepted this)
   - Implementation: Can run immediately

4. **Outcome normalization** - Extract outcomes to separate rows
   - Benefits: Better compression, deduplication of repeated odds values
   - Risk: Query complexity increases
   - Implementation: Significant migration effort

5. **Native compression** - TOAST compression settings, pg_compress
   - Benefits: Works without schema changes
   - Risk: CPU overhead
   - Implementation: Simple ALTER TABLE

**Recommend specific strategy with phases:**
- Phase 101: Schema changes (which tables, which optimizations)
- Phase 102: Application code updates
- Phase 103: Data migration approach
- Phase 104: Monitoring to prevent recurrence

Document recommended strategy with expected storage savings per approach.
  </action>
  <verify>DISCOVERY.md has "Recommended Strategy" section with Phase 101-104 breakdown and expected savings</verify>
  <done>Concrete optimization plan with estimated savings, implementation phases, and risk assessment</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Database connection works, SQL queries execute
- [ ] Table sizes documented for all public tables
- [ ] Top 3 storage drivers identified with percentages
- [ ] raw_response usage determined (needed or not)
- [ ] Optimization strategy documented with expected savings
- [ ] Phase 101-104 scope is clear from recommendations
</verification>

<success_criteria>

- DISCOVERY.md created with complete profiling analysis
- Storage drivers identified and quantified
- Optimization strategy chosen with rationale
- Phase 101 scope is concrete and actionable
- User can make informed decision on approach
  </success_criteria>

<output>
After completion, create `.planning/phases/100-investigation-analysis/100-01-SUMMARY.md`:

# Phase 100 Plan 01: Investigation & Analysis Summary

**[One-liner summary of key findings and recommended strategy]**

## Accomplishments

- Database profiled: X GB total, Y tables analyzed
- Top storage driver: [table/column] consuming Z% of space
- Optimization strategy selected: [approach]

## Key Findings

| Table | Size | Rows | % of Total |
|-------|------|------|------------|
| ... | ... | ... | ... |

## Recommended Strategy

[Summary of chosen approach with expected savings]

## Files Created/Modified

- `.planning/phases/100-investigation-analysis/DISCOVERY.md` - Full analysis

## Decisions Made

- [Key decisions with rationale]

## Issues Encountered

[Any issues or None]

## Next Phase Readiness

- Phase 101 scope defined: [specific changes]
- Expected storage reduction: X GB
- Risk level: [Low/Medium/High]
</output>
