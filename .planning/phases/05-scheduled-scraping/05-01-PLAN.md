---
phase: 05-scheduled-scraping
plan: 01
type: execute
---

<objective>
Set up APScheduler with AsyncIOScheduler and create the scheduled scraping job.

Purpose: Enable automatic periodic scraping of all platforms with proper run history tracking.
Output: Working scheduler infrastructure with scrape job that triggers on interval.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-scheduled-scraping/05-RESEARCH.md
@.planning/phases/05-scheduled-scraping/05-CONTEXT.md
@.planning/phases/03-scraper-integration/03-06-SUMMARY.md

# Key files:
@src/api/app.py
@src/scraping/orchestrator.py
@src/db/models/scrape.py

**Tech stack available:** FastAPI lifespan, SQLAlchemy async, httpx clients, ScrapeRun model
**Established patterns:**
- Lifespan context manager for resource lifecycle
- Async clients injected via app.state
- ScrapeRun with trigger field for scheduled/manual distinction
- DB session via get_db dependency

**From research (don't hand-roll):**
- Use APScheduler IntervalTrigger, not custom asyncio.sleep loop
- Use APScheduler coalesce=True for missed run handling
- Use APScheduler shutdown(wait=True) for graceful shutdown
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add APScheduler and create scheduling module</name>
  <files>pyproject.toml, src/scheduling/__init__.py, src/scheduling/scheduler.py</files>
  <action>
1. Add apscheduler>=3.10.0 to pyproject.toml dependencies.
2. Create src/scheduling/ directory with __init__.py.
3. Create src/scheduling/scheduler.py with:
   - Module-level AsyncIOScheduler instance with timezone="UTC"
   - configure_scheduler() function that adds scrape job with:
     - IntervalTrigger(minutes=int(os.getenv("SCRAPE_INTERVAL_MINUTES", "5")))
     - id="scrape_all_platforms"
     - replace_existing=True
     - misfire_grace_time=60
     - coalesce=True
   - start_scheduler() and shutdown_scheduler(wait=True) functions

Import the job function from jobs.py (created in Task 2). Use deferred import inside configure_scheduler to avoid circular import.
  </action>
  <verify>python -c "from src.scheduling.scheduler import scheduler, configure_scheduler, start_scheduler, shutdown_scheduler; print('OK')"</verify>
  <done>APScheduler installed, scheduling module exists with scheduler instance and lifecycle functions</done>
</task>

<task type="auto">
  <name>Task 2: Create scrape job with run history</name>
  <files>src/scheduling/jobs.py</files>
  <action>
Create src/scheduling/jobs.py with async scrape_all_platforms job:

1. Create async def scrape_all_platforms() that:
   - Gets async db session from src.db.session (create async session maker if needed)
   - Creates ScrapeRun record with status=RUNNING, trigger="scheduled"
   - Creates ScrapingOrchestrator with client instances (need access to app.state or create new clients)
   - Calls orchestrator.scrape_all() passing db session and scrape_run_id
   - Updates ScrapeRun with final status, events_scraped count, completed_at
   - Uses try/except to catch and log any exceptions, update ScrapeRun to FAILED

For client access: Since job runs outside request context, create a module-level reference to app.state that gets set during lifespan startup. Pattern:
```python
_app_state = None

def set_app_state(state):
    global _app_state
    _app_state = state
```

Use logging (import logging, logger = logging.getLogger(__name__)) to log job start/complete/error.
  </action>
  <verify>python -c "from src.scheduling.jobs import scrape_all_platforms; print('OK')"</verify>
  <done>Scrape job function exists, uses ScrapeRun lifecycle, handles errors gracefully</done>
</task>

<task type="auto">
  <name>Task 3: Integrate scheduler with FastAPI lifespan</name>
  <files>src/api/app.py</files>
  <action>
Modify the lifespan context manager in src/api/app.py:

1. Import from src.scheduling.scheduler: configure_scheduler, start_scheduler, shutdown_scheduler
2. Import from src.scheduling.jobs: set_app_state
3. After HTTP clients are created and stored in app.state, call:
   - set_app_state(app.state)  # Give jobs access to clients
   - configure_scheduler()
   - start_scheduler()
4. After yield (in shutdown), call:
   - shutdown_scheduler()

The updated lifespan should look like:
```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    async with httpx.AsyncClient(...) as sportybet_client:
        async with httpx.AsyncClient(...) as betpawa_client:
            async with httpx.AsyncClient(...) as bet9ja_client:
                app.state.sportybet_client = sportybet_client
                app.state.betpawa_client = betpawa_client
                app.state.bet9ja_client = bet9ja_client

                # Initialize scheduler
                set_app_state(app.state)
                configure_scheduler()
                start_scheduler()

                yield

                # Shutdown scheduler
                shutdown_scheduler()
```
  </action>
  <verify>python -c "from src.api.app import create_app; app = create_app(); print('OK')"</verify>
  <done>FastAPI app starts scheduler on startup and shuts down gracefully, SCRAPE_INTERVAL_MINUTES env var controls interval</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pip install -e .` installs apscheduler successfully
- [ ] `python -c "from src.scheduling import scheduler, jobs"` imports without error
- [ ] `python -c "from src.api.app import create_app"` imports without error
- [ ] Scheduler configured with 5-minute default interval
- [ ] SCRAPE_INTERVAL_MINUTES env var is respected
</verification>

<success_criteria>
- APScheduler added to dependencies
- Scheduling module with scheduler lifecycle management
- Scrape job with ScrapeRun tracking (trigger="scheduled")
- Scheduler integrated into FastAPI lifespan
- All imports work correctly
</success_criteria>

<output>
After completion, create `.planning/phases/05-scheduled-scraping/05-01-SUMMARY.md`
</output>
