---
phase: 31-backend-heartbeat
type: execute
---

<objective>
Add a background watchdog that auto-fails scrape runs stuck in RUNNING status, and recover stale runs on server startup.

Purpose: Ensure no scrape run ever stays in RUNNING status forever after crashes, disconnects, or hangs.
Output: Watchdog APScheduler job, startup recovery hook, and broadcaster cleanup for stale runs.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/31-backend-heartbeat/31-CONTEXT.md
@.planning/phases/31-backend-heartbeat/31-RESEARCH.md

# Key source files:
@src/db/models/scrape.py
@src/scheduling/scheduler.py
@src/scheduling/jobs.py
@src/scraping/broadcaster.py
@src/api/app.py

**Tech stack available:** APScheduler (IntervalTrigger), SQLAlchemy 2.0 async, structlog, existing ScrapeRun/ScrapePhaseLog/ScrapeError models
**Established patterns:** APScheduler job registration in configure_scheduler(), async_session_factory for independent DB sessions, ProgressRegistry singleton for broadcaster management
**Constraining decisions:**
- AsyncSession cannot be shared across concurrent asyncio tasks — watchdog must create its own session
- ScrapePhaseLog.started_at already provides "last activity" timestamps — no new heartbeat column needed
- ProgressRegistry.remove_broadcaster() handles subscriber cleanup
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create stale run detection module and register watchdog job</name>
  <files>src/scheduling/stale_detection.py, src/scheduling/scheduler.py</files>
  <action>
Create `src/scheduling/stale_detection.py` with:

1. `find_stale_runs(db: AsyncSession, stale_threshold_minutes: int = 10) -> list[ScrapeRun]`
   - Query RUNNING scrape runs where last activity exceeds threshold
   - Use subquery: MAX(ScrapePhaseLog.started_at) grouped by scrape_run_id as last_activity
   - Outerjoin ScrapeRun to subquery
   - Stale condition: `(last_activity < threshold) OR (last_activity IS NULL AND ScrapeRun.started_at < threshold)`
   - Return list of stale ScrapeRun objects

2. `mark_run_stale(db: AsyncSession, run: ScrapeRun, last_activity: datetime | None) -> None`
   - Set run.status = ScrapeStatus.FAILED, run.completed_at = now
   - Create ScrapeError with error_type="stale", descriptive message including stale duration, last phase (run.current_phase), last platform (run.current_platform)
   - Flush (don't commit — caller commits)

3. `detect_stale_runs() -> None` (the async job function)
   - Create session via async_session_factory()
   - Call find_stale_runs()
   - For each stale run:
     - Get last_activity from a separate query (MAX ScrapePhaseLog.started_at for that run)
     - Call mark_run_stale()
     - Clean up broadcaster: get from progress_registry, close() it, remove_broadcaster()
   - Commit once at end
   - Log with structlog: count of stale runs detected, or debug log if none

In `src/scheduling/scheduler.py`:
- Import detect_stale_runs from stale_detection
- In configure_scheduler(), add third job:
  - Function: detect_stale_runs
  - ID: "detect_stale_runs"
  - Trigger: IntervalTrigger(minutes=2)
  - misfire_grace_time=60
  - coalesce=True

Use optimistic check: only update if status is still RUNNING (re-check after query, before commit). This avoids race conditions with the orchestrator completing a run between query and update.
  </action>
  <verify>
Import and syntax check: `python -c "from src.scheduling.stale_detection import detect_stale_runs, find_stale_runs, mark_run_stale"`
Scheduler check: `python -c "from src.scheduling.scheduler import configure_scheduler; print('OK')"`
  </verify>
  <done>
- stale_detection.py exists with find_stale_runs, mark_run_stale, detect_stale_runs functions
- Watchdog job registered in configure_scheduler() with 2-minute interval
- Broadcaster cleanup included in stale detection flow
- Optimistic concurrency check prevents race with active orchestrator
  </done>
</task>

<task type="auto">
  <name>Task 2: Add startup recovery for stale runs from previous process</name>
  <files>src/scheduling/stale_detection.py, src/api/app.py</files>
  <action>
In `src/scheduling/stale_detection.py`, add:

4. `recover_stale_runs_on_startup() -> int`
   - Create session via async_session_factory()
   - Query ALL ScrapeRun where status == RUNNING (any RUNNING run at startup is stale by definition — no orchestrator is running yet)
   - For each: call mark_run_stale(db, run, last_activity=None) with message "Run recovered on server startup: process restarted while scrape was in progress"
   - Commit
   - Log count with structlog
   - Return count of recovered runs

In `src/api/app.py` lifespan:
- Import recover_stale_runs_on_startup
- Call it BEFORE configure_scheduler() and start_scheduler() — this ensures stale runs from previous process are cleaned up before new scheduled scrapes begin
- Log the result

No broadcaster cleanup needed at startup — ProgressRegistry is in-memory and empty after restart.
  </action>
  <verify>
Import check: `python -c "from src.scheduling.stale_detection import recover_stale_runs_on_startup"`
App check: `python -c "from src.api.app import lifespan; print('OK')"`
  </verify>
  <done>
- recover_stale_runs_on_startup() exists and is called during app lifespan startup
- Called before scheduler starts, ensuring clean state
- Logs count of recovered runs
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `python -c "from src.scheduling.stale_detection import detect_stale_runs, find_stale_runs, mark_run_stale, recover_stale_runs_on_startup"` succeeds
- [ ] `python -c "from src.scheduling.scheduler import configure_scheduler"` succeeds
- [ ] `python -c "from src.api.app import lifespan"` succeeds
- [ ] No import errors or circular dependencies
- [ ] Watchdog job registered with 2-minute interval in configure_scheduler()
- [ ] Startup recovery called before scheduler starts in lifespan
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No import errors introduced
- Stale detection query uses ScrapePhaseLog.started_at with fallback to ScrapeRun.started_at
- ScrapeError records created with error_type="stale" and descriptive messages
- Broadcaster cleanup included in watchdog flow
- Startup recovery handles pre-existing RUNNING runs
- Optimistic concurrency prevents race conditions
</success_criteria>

<output>
After completion, create `.planning/phases/31-backend-heartbeat/31-01-SUMMARY.md`
</output>
