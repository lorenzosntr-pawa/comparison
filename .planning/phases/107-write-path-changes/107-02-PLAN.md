---
phase: 107-write-path-changes
plan: 02
type: execute
---

<objective>
Implement database write operations for new market-level schema and integrate into scraping pipeline.

Purpose: Replace snapshot-based writes (INSERT all 50+ markets when any changes) with market-level writes (UPSERT current + INSERT history only for changed). This achieves the 95% storage reduction designed in Phase 105.

Output: New handle_market_write_batch() function that writes to market_odds_current/history tables, and updated event_coordinator.py store_batch_results() that uses the new write path.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase context:
@.planning/phases/105-investigation-schema-design/105-01-SUMMARY.md
@.planning/phases/106-schema-migration/106-01-SUMMARY.md
@.planning/phases/107-write-path-changes/107-01-PLAN.md

# Key source files:
@src/storage/write_handler.py
@src/storage/write_queue.py
@src/scraping/event_coordinator.py
@src/db/models/market_odds.py
@src/caching/change_detection.py

**Tech stack available:** PostgreSQL ON CONFLICT, SQLAlchemy 2.0 async, frozen dataclasses
**Established patterns:** Session factory isolation, perf_counter timing, structlog logging

**Constraining decisions:**
- Phase 105: UPSERT pattern for market_odds_current (ON CONFLICT UPDATE)
- Phase 105: Append-only INSERT for market_odds_history
- Phase 106: COALESCE(line, 0) in unique constraint
- Phase 106: Partitioned history table by captured_at
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement handle_market_write_batch() in write_handler.py</name>
  <files>src/storage/write_handler.py</files>
  <action>
Add new function to handle market-level writes to new tables:

```python
async def handle_market_write_batch(session_factory, batch: MarketWriteBatch) -> dict:
    """Process a MarketWriteBatch: UPSERT current, INSERT history for changed.

    For each market in batch.markets:
    1. UPSERT into market_odds_current (always)
       - ON CONFLICT (event_id, bookmaker_slug, betpawa_market_id, COALESCE(line, 0))
       - UPDATE all columns + last_confirmed_at = now
       - If changed=True, also set last_updated_at = now

    2. INSERT into market_odds_history (only if changed=True)
       - Append-only record of the odds change

    Returns stats dict with counts and timing.
    """
```

Implementation details:
- Import MarketOddsCurrent, MarketOddsHistory from src.db.models.market_odds
- Import MarketWriteBatch from write_queue
- Use PostgreSQL INSERT...ON CONFLICT via SQLAlchemy:
  ```python
  from sqlalchemy.dialects.postgresql import insert

  stmt = insert(MarketOddsCurrent).values(...)
  stmt = stmt.on_conflict_do_update(
      index_elements=['event_id', 'bookmaker_slug', 'betpawa_market_id'],
      index_where=(func.coalesce(MarketOddsCurrent.line, 0) == func.coalesce(stmt.excluded.line, 0)),
      set_={
          'betpawa_market_name': stmt.excluded.betpawa_market_name,
          'outcomes': stmt.excluded.outcomes,
          'market_groups': stmt.excluded.market_groups,
          'handicap_type': stmt.excluded.handicap_type,
          'handicap_home': stmt.excluded.handicap_home,
          'handicap_away': stmt.excluded.handicap_away,
          'unavailable_at': stmt.excluded.unavailable_at,
          'last_confirmed_at': now,
          'last_updated_at': case(
              (market.changed, now),
              else_=MarketOddsCurrent.last_updated_at
          )
      }
  )
  ```
- For history INSERTs, batch all changed markets into single executemany
- Track counts: upserted_current, inserted_history
- Follow existing error handling pattern: IntegrityError skip, OperationalError re-raise
- Add timing with perf_counter, return stats dict

Keep existing handle_write_batch() for backward compatibility during read path migration.
  </action>
  <verify>
python -c "
from src.storage.write_handler import handle_market_write_batch
print('Function imported successfully')
"
  </verify>
  <done>handle_market_write_batch() exists with UPSERT current + INSERT history logic</done>
</task>

<task type="auto">
  <name>Task 2: Add market batch processing to AsyncWriteQueue</name>
  <files>src/storage/write_queue.py</files>
  <action>
Update AsyncWriteQueue to support MarketWriteBatch in addition to WriteBatch:

1. Update _process_with_retry to detect batch type:
   ```python
   async def _process_with_retry(self, batch: WriteBatch | MarketWriteBatch) -> None:
       from src.storage.write_handler import handle_write_batch, handle_market_write_batch

       if isinstance(batch, MarketWriteBatch):
           handler = handle_market_write_batch
       else:
           handler = handle_write_batch
       # ... rest of retry logic unchanged
   ```

2. Update type hints on enqueue() to accept Union[WriteBatch, MarketWriteBatch]

3. Update logging in enqueue() to handle both batch types

This enables gradual migration: old code uses WriteBatch, new code uses MarketWriteBatch.
  </action>
  <verify>
python -c "
from src.storage.write_queue import AsyncWriteQueue, MarketWriteBatch
# Type check passes
print('AsyncWriteQueue can handle MarketWriteBatch')
"
  </verify>
  <done>AsyncWriteQueue._process_with_retry routes to correct handler based on batch type</done>
</task>

<task type="auto">
  <name>Task 3: Update event_coordinator.py store_batch_results() for new write path</name>
  <files>src/scraping/event_coordinator.py</files>
  <action>
Update store_batch_results() to use new market-level write path:

1. Find the async path section (around line 1468) that currently builds SnapshotWriteData

2. After building market data, call classify_market_changes() instead of classify_batch_changes():
   ```python
   from src.caching import classify_market_changes
   from src.storage.write_queue import MarketWriteBatch

   # Build market data with bookmaker_slug
   market_input: list[tuple[int, str, list[dict]]] = []

   # BetPawa markets: (event_id, 'betpawa', markets_data)
   for swd in bp_write_data:
       markets_as_dicts = [...]  # existing conversion
       market_input.append((swd.event_id, 'betpawa', markets_as_dicts))

   # Competitor markets: (event_id, source, markets_data)
   for cswd in comp_write_data:
       source = ...  # 'sportybet' or 'bet9ja'
       markets_as_dicts = [...]
       # Use betpawa event_id for unified storage
       market_input.append((betpawa_eid, source, markets_as_dicts))

   # Get per-market change status
   market_writes = classify_market_changes(self._odds_cache, market_input)

   # Enqueue new batch type
   market_batch = MarketWriteBatch(
       markets=tuple(market_writes),
       scrape_run_id=scrape_run_id,
       batch_index=batch_idx,
   )
   await self._write_queue.enqueue(market_batch)
   ```

3. Keep the existing WriteBatch path temporarily commented or behind a feature flag for rollback. Use new path as default.

4. Update cache after writes - cache still needs to store market data for change detection on next cycle. The existing put_betpawa_snapshot/put_competitor_snapshot calls can remain for now (Phase 108 will update cache structure).

5. Handle availability detection - since unavailable_at is now part of MarketCurrentWrite, integrate availability changes into the market write data instead of separate UnavailableMarketUpdate.
  </action>
  <verify>
cd backend && python -c "
from src.scraping.event_coordinator import EventCoordinator
print('EventCoordinator imports successfully')
"
  </verify>
  <done>store_batch_results() uses classify_market_changes and enqueues MarketWriteBatch</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from src.storage.write_handler import handle_market_write_batch"` succeeds
- [ ] `python -c "from src.scraping.event_coordinator import EventCoordinator"` succeeds
- [ ] No Python syntax errors in modified files
- [ ] Manual test: Start scheduler, trigger scrape, check market_odds_current has data
- [ ] Manual test: Check market_odds_history has entries for changed markets
</verification>

<success_criteria>

- handle_market_write_batch() implemented with UPSERT + INSERT logic
- AsyncWriteQueue routes MarketWriteBatch to new handler
- event_coordinator.py store_batch_results() uses new write path
- New tables (market_odds_current, market_odds_history) receive data on scrape
- Old tables still receive data (dual-write not removed yet for safety)
- Phase 107 complete, ready for Phase 108 read path migration
</success_criteria>

<output>
After completion, create `.planning/phases/107-write-path-changes/107-02-SUMMARY.md`
</output>
