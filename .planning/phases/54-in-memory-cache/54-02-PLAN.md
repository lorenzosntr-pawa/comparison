---
phase: 54-in-memory-cache
plan: 02
type: execute
---

<objective>
Wire the scraping pipeline to populate OddsCache after each batch storage, and implement automatic eviction of expired events.

Purpose: Keep the cache current — every scrape cycle updates the cache so API responses reflect the latest odds without DB queries.
Output: EventCoordinator updates OddsCache after each batch commit; expired events are evicted automatically.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/54-in-memory-cache/54-01-SUMMARY.md

# Key source files:
@src/scraping/event_coordinator.py
@src/caching/odds_cache.py
@src/caching/warmup.py
@src/scheduling/jobs.py
@src/api/app.py

**Tech stack available:** FastAPI, SQLAlchemy 2.0 async, structlog, asyncio
**Established patterns:**
- Single-flush batch insert in store_batch_results() (line 1252-1283)
- perf_counter timing on progress events
- Factory method for EventCoordinator (from_settings)

**Constraining decisions:**
- Phase 53: Memory usage must stay reasonable (<100MB for active events)
- v1.7: AsyncSession cannot be shared across concurrent asyncio tasks
- EventCoordinator receives db session from caller (jobs.py), does NOT own the session
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire EventCoordinator to update OddsCache after batch storage</name>
  <files>src/scraping/event_coordinator.py, src/scheduling/jobs.py</files>
  <action>
**Modify EventCoordinator to accept an optional OddsCache reference:**

1. Add `_odds_cache: OddsCache | None = None` to `__init__` and `from_settings()`.
   - `from_settings()` gains optional `odds_cache: OddsCache | None = None` parameter
   - Store as `self._odds_cache`

2. **In `store_batch_results()`**, after the `await db.commit()` at line 1283, add cache population:

```python
# Update in-memory cache with freshly stored snapshots
if self._odds_cache is not None:
    cache_update_start = time.perf_counter()
    cache_updated = 0

    for snapshot, markets in betpawa_snapshots:
        # snapshot.id and snapshot.event_id are populated after flush
        cached = snapshot_to_cached_from_models(snapshot, markets)
        # Need event kickoff — look up from event data in batch
        event_kickoff = self._get_event_kickoff(batch, snapshot)
        self._odds_cache.put_betpawa_snapshot(
            snapshot.event_id, snapshot.bookmaker_id, cached, event_kickoff
        )
        cache_updated += 1

    for snapshot, markets in competitor_snapshots:
        # Need betpawa_event_id to store in cache correctly
        cached = snapshot_to_cached_from_models(snapshot, markets)
        betpawa_event_id = self._get_betpawa_event_id_for_competitor(
            snapshot, competitor_event_map
        )
        if betpawa_event_id:
            source = self._get_source_for_competitor_event(snapshot, ...)
            self._odds_cache.put_competitor_snapshot(
                betpawa_event_id, source, cached, ...
            )
            cache_updated += 1

    cache_update_ms = int((time.perf_counter() - cache_update_start) * 1000)
    logger.debug("Cache updated", snapshots=cache_updated, duration_ms=cache_update_ms)
```

**Important implementation details:**
- Use the `snapshot_to_cached` conversion helper from 54-01 (or create a variant `snapshot_to_cached_from_models()` that works with in-memory models + market lists before they're fully committed — the flush has already assigned IDs)
- For competitor snapshots, we need to map back to betpawa_event_id. The `competitor_event_map` (dict mapping SR ID → {source: comp_event_id}) is already available in store_batch_results. Cross-reference with `event_id_map` (SR ID → event ID).
- The cache update should be fast (just dict writes) — add perf_counter timing and include in `_last_storage_timings`
- Do NOT use the DB session for cache updates — extract data from the in-memory model objects that were just flushed/committed

3. **Modify `src/scheduling/jobs.py`** `scrape_all_platforms()`:
   - After creating EventCoordinator.from_settings(), pass the cache:
   ```python
   coordinator = EventCoordinator.from_settings(
       betpawa_client=betpawa_client,
       sportybet_client=sportybet_client,
       bet9ja_client=bet9ja_client,
       settings=settings,
       odds_cache=getattr(_app_state, 'odds_cache', None),
   )
   ```
   - The app.state has odds_cache from 54-01. `_app_state` is the FastAPI state object set via `set_app_state()`.

4. Add the cache timing to `_last_storage_timings` dict so it appears in progress events.
  </action>
  <verify>Start server, trigger a scrape (or wait for scheduled scrape), check logs for "Cache updated" message with snapshot count > 0. After scrape, verify cache stats show increased counts vs warmup-only.</verify>
  <done>EventCoordinator populates OddsCache after every batch commit, cache update timing logged, no errors during scrape cycle</done>
</task>

<task type="auto">
  <name>Task 2: Implement automatic cache eviction for expired events</name>
  <files>src/caching/odds_cache.py, src/scheduling/jobs.py</files>
  <action>
**Add eviction to the scrape cycle:**

1. In `OddsCache.evict_expired(cutoff: datetime) -> int`:
   - Already defined in 54-01. Ensure implementation removes entries from `_betpawa_snapshots`, `_competitor_snapshots`, and `_event_kickoffs` where kickoff < cutoff.
   - Return count of events removed.
   - Log eviction stats with structlog.

2. **In `scrape_all_platforms()` in jobs.py**, after the scrape cycle completes successfully:
   ```python
   # Evict expired events from cache (2 hours past kickoff grace period)
   odds_cache = getattr(_app_state, 'odds_cache', None)
   if odds_cache:
       cutoff = datetime.now(timezone.utc).replace(tzinfo=None) - timedelta(hours=2)
       evicted = odds_cache.evict_expired(cutoff)
       if evicted > 0:
           logger.info("Cache eviction", events_removed=evicted, **odds_cache.stats())
   ```

3. **Add memory monitoring to stats():**
   - Estimate memory usage: count total markets across all cached snapshots, multiply by approximate bytes per market (~200 bytes for a CachedMarket with outcomes)
   - Add `estimated_memory_mb` to stats() return dict
   - Log stats after warmup and after eviction

4. **Grace period rationale:** Keep events 2 hours past kickoff because:
   - Users may still be viewing recently started events
   - Matches typically have odds for ~90 min after kickoff
   - This is configurable via the cutoff parameter

**Do NOT create a separate background task for eviction** — piggyback on the existing scrape schedule. Eviction runs once per scrape cycle (every 2-10 minutes depending on settings).
  </action>
  <verify>Start server, wait for 1-2 scrape cycles, check logs for eviction messages (if any expired events exist). Verify cache stats show reasonable memory estimate.</verify>
  <done>Eviction removes expired events from cache, memory estimate logged, no memory growth over multiple scrape cycles for stable event count</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] EventCoordinator updates cache after every batch
- [ ] Cache update timing appears in storage timings
- [ ] Eviction runs after each scrape cycle
- [ ] Cache stats logged with memory estimate
- [ ] Server runs through multiple scrape cycles without errors or memory growth
</verification>

<success_criteria>

- Cache is populated from scraping pipeline (not just warmup)
- Expired events are evicted automatically
- Memory usage stays reasonable (<100MB)
- Cache update adds minimal latency to scrape pipeline (<50ms)
- All tasks completed with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/54-in-memory-cache/54-02-SUMMARY.md`
</output>
