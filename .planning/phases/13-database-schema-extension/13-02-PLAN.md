---
phase: 13-database-schema-extension
plan: 02
type: execute
---

<objective>
Create scrape batch model and Alembic migration for all new tables.

Purpose: Enable grouped scrape runs (batch visibility) and persist the new competitor schema to the database.
Output: ScrapeBatch model, updated ScrapeRun with batch FK, and working Alembic migration.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-database-schema-extension/13-CONTEXT.md

# Prior plan in this phase:
@.planning/phases/13-database-schema-extension/13-01-PLAN.md

# Existing models to reference:
@src/db/models/scrape.py
@src/db/models/competitor.py
@alembic/versions/845263fcf673_initial_schema.py

**Tech stack available:** SQLAlchemy 2.0, Alembic, asyncpg
**Established patterns:**
- Alembic migrations with op.create_table()
- Index creation in migrations
- StrEnum stored as String(N) in DB

**Constraining decisions:**
- Phase context: Fresh start approach - drop existing data and rescrape
- Phase context: Scrape batches group platform runs for unified view
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ScrapeBatch model and update ScrapeRun</name>
  <files>src/db/models/scrape.py, src/db/models/__init__.py</files>
  <action>
Add ScrapeBatch model to scrape.py:

1. ScrapeBatch model:
   - id (primary key)
   - started_at (server_default=func.now())
   - completed_at (datetime, nullable)
   - status (ScrapeStatus enum, default=PENDING) - overall batch status
   - trigger (String(50), nullable) - "scheduled", "manual"
   - notes (Text, nullable) - optional batch notes
   - Relationship: runs → list[ScrapeRun] (back_populates="batch")
   - Index on started_at

2. Update ScrapeRun model:
   - Add batch_id (FK to scrape_batches.id, nullable) - nullable for backwards compatibility
   - Add relationship: batch → ScrapeBatch (back_populates="runs")

3. Update __init__.py to export ScrapeBatch

The batch groups multiple ScrapeRuns (one per platform) so UI can show:
"Batch 123: betpawa ✓, sportybet ✓, bet9ja ✗"
  </action>
  <verify>python -c "from src.db.models import ScrapeBatch, ScrapeRun; print(ScrapeBatch.__tablename__, hasattr(ScrapeRun, 'batch_id'))"</verify>
  <done>ScrapeBatch model created, ScrapeRun has batch_id FK and relationship</done>
</task>

<task type="auto">
  <name>Task 2: Create Alembic migration for all new tables</name>
  <files>alembic/versions/[new_migration].py</files>
  <action>
Create new Alembic migration with all Phase 13 tables:

1. Run: `cd src && alembic revision -m "add_competitor_tables_and_scrape_batches"`

2. In the generated migration file, add to upgrade():

   a. Create scrape_batches table:
      - id SERIAL PRIMARY KEY
      - started_at TIMESTAMP DEFAULT now()
      - completed_at TIMESTAMP NULL
      - status VARCHAR(20) DEFAULT 'pending'
      - trigger VARCHAR(50) NULL
      - notes TEXT NULL
      - INDEX idx_scrape_batches_started on started_at

   b. Add batch_id column to scrape_runs:
      - batch_id INTEGER NULL REFERENCES scrape_batches(id)

   c. Create competitor_tournaments table:
      - id SERIAL PRIMARY KEY
      - source VARCHAR(20) NOT NULL
      - sport_id INTEGER NOT NULL REFERENCES sports(id)
      - name VARCHAR(255) NOT NULL
      - country_raw VARCHAR(100) NULL
      - country_iso VARCHAR(3) NULL
      - external_id VARCHAR(100) NOT NULL
      - sportradar_id VARCHAR(100) NULL
      - created_at TIMESTAMP DEFAULT now()
      - deleted_at TIMESTAMP NULL
      - UNIQUE(source, external_id)
      - Indexes: source, sport_id, sportradar_id

   d. Create competitor_events table:
      - id SERIAL PRIMARY KEY
      - source VARCHAR(20) NOT NULL
      - tournament_id INTEGER NOT NULL REFERENCES competitor_tournaments(id)
      - betpawa_event_id INTEGER NULL REFERENCES events(id)
      - name VARCHAR(500) NOT NULL
      - home_team VARCHAR(255) NOT NULL
      - away_team VARCHAR(255) NOT NULL
      - kickoff TIMESTAMP NOT NULL
      - external_id VARCHAR(100) NOT NULL
      - sportradar_id VARCHAR(100) NOT NULL
      - created_at TIMESTAMP DEFAULT now()
      - updated_at TIMESTAMP DEFAULT now()
      - deleted_at TIMESTAMP NULL
      - UNIQUE(source, external_id)
      - Indexes: source, tournament_id, sportradar_id, betpawa_event_id, kickoff

   e. Create competitor_odds_snapshots table:
      - id BIGSERIAL PRIMARY KEY
      - competitor_event_id INTEGER NOT NULL REFERENCES competitor_events(id)
      - captured_at TIMESTAMP DEFAULT now()
      - scrape_run_id INTEGER NULL REFERENCES scrape_runs(id)
      - raw_response JSONB NULL
      - Indexes: competitor_event_id, captured_at

   f. Create competitor_market_odds table:
      - id BIGSERIAL PRIMARY KEY
      - snapshot_id BIGINT NOT NULL REFERENCES competitor_odds_snapshots(id)
      - betpawa_market_id VARCHAR(50) NOT NULL
      - betpawa_market_name VARCHAR(255) NOT NULL
      - line FLOAT NULL
      - handicap_type VARCHAR(50) NULL
      - handicap_home FLOAT NULL
      - handicap_away FLOAT NULL
      - outcomes JSONB NOT NULL
      - Indexes: snapshot_id, betpawa_market_id

3. In downgrade():
   - Drop tables in reverse order (competitor_market_odds, competitor_odds_snapshots, competitor_events, competitor_tournaments)
   - Drop batch_id column from scrape_runs
   - Drop scrape_batches table

Use op.create_table(), op.add_column(), op.create_index(), op.create_foreign_key() following existing migration patterns.
  </action>
  <verify>cd src && alembic check</verify>
  <done>Migration file created, alembic check passes (migration matches models)</done>
</task>

<task type="auto">
  <name>Task 3: Apply migration and verify schema</name>
  <files>None (database operation)</files>
  <action>
Apply the migration to create the new tables:

1. Run: `cd src && alembic upgrade head`

2. Verify all tables exist by running:
   ```python
   from src.db.engine import engine
   from sqlalchemy import inspect
   import asyncio

   async def check():
       async with engine.connect() as conn:
           def sync_inspect(connection):
               inspector = inspect(connection)
               tables = inspector.get_table_names()
               expected = ['scrape_batches', 'competitor_tournaments', 'competitor_events',
                          'competitor_odds_snapshots', 'competitor_market_odds']
               for t in expected:
                   assert t in tables, f"Missing table: {t}"
               print("All tables created successfully")
           await conn.run_sync(sync_inspect)

   asyncio.run(check())
   ```

3. Verify scrape_runs has batch_id column

Note: This will create tables in the existing database. If there's data you want to preserve, back it up first. Per phase context, fresh start is acceptable.
  </action>
  <verify>cd src && python -c "from sqlalchemy import inspect, create_engine; e = create_engine('postgresql://postgres:postgres@localhost/betpawa'); i = inspect(e); print([t for t in i.get_table_names() if 'competitor' in t or 'batch' in t])"</verify>
  <done>All new tables exist in database, migration applied successfully</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] ScrapeBatch model importable from src.db.models
- [ ] ScrapeRun.batch_id exists with proper FK
- [ ] Alembic migration file exists and is valid
- [ ] `alembic check` passes (no pending changes)
- [ ] All 6 new tables exist in database (scrape_batches, competitor_*)
- [ ] Phase 13 complete - schema ready for Phase 14 scraping
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Database schema matches models
- No migration errors
- Phase 13 complete
</success_criteria>

<output>
After completion, create `.planning/phases/13-database-schema-extension/13-02-SUMMARY.md`
</output>
