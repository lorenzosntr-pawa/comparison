---
phase: 07.2-scraping-performance
plan: 02
type: execute
---

<objective>
Add Server-Sent Events endpoint for live scraping progress and integrate with frontend.

Purpose: Enable real-time visibility into scraping progress in both terminal and web UI during execution.
Output: SSE endpoint streaming progress, orchestrator progress generator, frontend hook consuming SSE.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07.2-scraping-performance/07.2-CONTEXT.md
@.planning/phases/07.2-scraping-performance/07.2-RESEARCH.md

# Key source files:
@src/scraping/orchestrator.py
@src/scraping/schemas.py
@src/api/routes/scrape.py
@web/src/features/dashboard/hooks/use-scheduler.ts

**From research - SSE pattern:**
```python
@router.get("/scrape/stream")
async def stream_scrape_progress(request: Request):
    async def generate():
        async for progress in orchestrator.scrape_with_progress():
            if await request.is_disconnected():
                break
            yield f"data: {progress.model_dump_json()}\n\n"
    return StreamingResponse(generate(), media_type="text/event-stream", ...)
```

**Tech stack available:** FastAPI StreamingResponse, EventSource API in browser
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add ScrapeProgress schema and orchestrator progress generator</name>
  <files>src/scraping/schemas.py, src/scraping/orchestrator.py</files>
  <action>
1. In `src/scraping/schemas.py`, add ScrapeProgress model:
   ```python
   class ScrapeProgress(BaseModel):
       """Progress update during scraping."""
       platform: Platform | None = None  # None for overall updates
       phase: str  # "starting", "scraping", "storing", "completed", "failed"
       current: int  # Current platform index (0-based)
       total: int  # Total platforms
       events_count: int | None = None  # Events scraped so far for this platform
       message: str | None = None  # Human-readable status message
       timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
   ```

2. In `src/scraping/orchestrator.py`, add `scrape_with_progress()` async generator method:
   ```python
   async def scrape_with_progress(
       self,
       platforms: list[Platform] | None = None,
       sport_id: str | None = None,
       competition_id: str | None = None,
       timeout: float = 30.0,
       scrape_run_id: int | None = None,
       db: AsyncSession | None = None,
   ) -> AsyncGenerator[ScrapeProgress, None]:
       """Scrape all platforms, yielding progress updates."""
   ```

   Implementation approach:
   - Yield "starting" progress for overall scrape
   - For each platform, yield "scraping {platform}" progress before calling _scrape_platform()
   - After each platform completes, yield "completed {platform}" with events_count
   - At end, yield final "completed" with total events

   Use same scrape_all() logic internally but intersperse yields. Consider wrapping existing logic.

DO NOT change the existing `scrape_all()` method - it's used by scheduled jobs. Create new method.
Import AsyncGenerator from typing and datetime/timezone if not present.
  </action>
  <verify>
1. Python syntax check: `python -c "from src.scraping.orchestrator import ScrapingOrchestrator"`
2. Schema check: `python -c "from src.scraping.schemas import ScrapeProgress; print(ScrapeProgress.model_fields)"`
  </verify>
  <done>
- ScrapeProgress schema defined with platform, phase, counts, message, timestamp
- scrape_with_progress() async generator method added to orchestrator
- Method yields progress before/after each platform scrape
  </done>
</task>

<task type="auto">
  <name>Task 2: Add SSE streaming endpoint</name>
  <files>src/api/routes/scrape.py</files>
  <action>
1. Add new SSE endpoint GET `/scrape/stream`:
   ```python
   from fastapi.responses import StreamingResponse
   import json

   @router.get("/stream")
   async def stream_scrape(
       request: Request,
       db: AsyncSession = Depends(get_db),
       timeout: int = Query(default=300, ge=5, le=600),
   ):
       """Stream scrape progress via Server-Sent Events.

       Connect via EventSource in browser to receive real-time progress updates.
       Each event is a JSON object with platform, phase, counts, and message.
       """
       # Create ScrapeRun record
       scrape_run = ScrapeRun(status=ScrapeStatus.RUNNING, trigger="api-stream")
       db.add(scrape_run)
       await db.commit()
       await db.refresh(scrape_run)

       # Build orchestrator
       sportybet = SportyBetClient(request.app.state.sportybet_client)
       betpawa = BetPawaClient(request.app.state.betpawa_client)
       bet9ja = Bet9jaClient(request.app.state.bet9ja_client)
       orchestrator = ScrapingOrchestrator(sportybet, betpawa, bet9ja)

       async def event_generator():
           try:
               async for progress in orchestrator.scrape_with_progress(
                   timeout=float(timeout),
                   scrape_run_id=scrape_run.id,
                   db=db,
               ):
                   if await request.is_disconnected():
                       break
                   yield f"data: {progress.model_dump_json()}\n\n"
           except Exception as e:
               yield f"data: {json.dumps({'phase': 'failed', 'message': str(e)})}\n\n"
           finally:
               # Update ScrapeRun on completion
               scrape_run.status = ScrapeStatus.COMPLETED
               scrape_run.completed_at = datetime.utcnow()
               await db.commit()

       return StreamingResponse(
           event_generator(),
           media_type="text/event-stream",
           headers={
               "Cache-Control": "no-cache",
               "Connection": "keep-alive",
               "X-Accel-Buffering": "no",  # Disable nginx buffering
           },
       )
   ```

2. Place the endpoint BEFORE the `/{scrape_run_id}` route (specific paths before parameterized).

IMPORTANT: Check for client disconnection with `await request.is_disconnected()` to avoid orphaned generators.
Use proper SSE format: `data: {json}\n\n` (double newline is required).
  </action>
  <verify>
1. Start server: `cd src && python -m uvicorn api.main:app --reload`
2. Test with curl: `curl -N http://localhost:8000/api/scrape/stream`
3. Should see streaming JSON events like: `data: {"platform":"betpawa","phase":"scraping",...}`
4. Events should arrive in real-time as each platform completes
  </verify>
  <done>
- GET /api/scrape/stream endpoint returns StreamingResponse with text/event-stream
- Progress events stream in real-time during scrape
- Proper SSE format with data: prefix and double newline
- Client disconnection handled
  </done>
</task>

<task type="auto">
  <name>Task 3: Add frontend useScrapeProgress hook</name>
  <files>web/src/features/dashboard/hooks/use-scrape-progress.ts, web/src/features/dashboard/hooks/index.ts</files>
  <action>
1. Create `web/src/features/dashboard/hooks/use-scrape-progress.ts`:
   ```typescript
   import { useState, useCallback } from 'react'
   import { useQueryClient } from '@tanstack/react-query'

   export interface ScrapeProgress {
     platform: string | null
     phase: string
     current: number
     total: number
     events_count: number | null
     message: string | null
     timestamp: string
   }

   export function useScrapeProgress() {
     const queryClient = useQueryClient()
     const [isStreaming, setIsStreaming] = useState(false)
     const [progress, setProgress] = useState<ScrapeProgress | null>(null)
     const [error, setError] = useState<string | null>(null)

     const startScrape = useCallback(() => {
       setIsStreaming(true)
       setError(null)
       setProgress(null)

       const eventSource = new EventSource('/api/scrape/stream')

       eventSource.onmessage = (event) => {
         const data = JSON.parse(event.data) as ScrapeProgress
         setProgress(data)

         if (data.phase === 'completed' || data.phase === 'failed') {
           eventSource.close()
           setIsStreaming(false)
           // Invalidate queries to refresh data
           queryClient.invalidateQueries({ queryKey: ['scheduler-history'] })
           queryClient.invalidateQueries({ queryKey: ['events'] })
         }
       }

       eventSource.onerror = () => {
         eventSource.close()
         setIsStreaming(false)
         setError('Connection lost')
       }

       return () => {
         eventSource.close()
         setIsStreaming(false)
       }
     }, [queryClient])

     return { startScrape, isStreaming, progress, error }
   }
   ```

2. Add export to `web/src/features/dashboard/hooks/index.ts`:
   ```typescript
   export { useScrapeProgress } from './use-scrape-progress'
   export type { ScrapeProgress } from './use-scrape-progress'
   ```

The hook will be used in Plan 3 to add UI components. For now just verify it compiles.
  </action>
  <verify>
1. TypeScript check: `cd web && pnpm tsc --noEmit`
2. Import check: `cd web && pnpm exec tsx -e "import { useScrapeProgress } from './src/features/dashboard/hooks'; console.log('OK')"`
  </verify>
  <done>
- useScrapeProgress hook created with EventSource consumption
- Hook tracks isStreaming, progress, and error states
- Invalidates relevant queries on completion
- Exported from hooks index
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] ScrapeProgress schema defined correctly
- [ ] scrape_with_progress() yields progress updates
- [ ] GET /api/scrape/stream returns SSE events
- [ ] curl test shows streaming JSON progress
- [ ] useScrapeProgress hook compiles without errors
- [ ] No TypeScript/Python errors
</verification>

<success_criteria>

- SSE endpoint streams real-time progress during scrape
- Progress includes platform, phase, counts, timestamp
- Frontend hook ready to consume SSE stream
- Proper error handling and client disconnection detection
</success_criteria>

<output>
After completion, create `.planning/phases/07.2-scraping-performance/07.2-02-SUMMARY.md`:

# Phase 7.2 Plan 2: SSE Progress Streaming Summary

**[Substantive one-liner about what was accomplished]**

## Accomplishments
- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified
- `path/to/file` - Description

## Decisions Made
[Key decisions and rationale, or "None"]

## Issues Encountered
[Problems and resolutions, or "None"]

## Next Step
Ready for 07.2-03-PLAN.md (Frontend Scrape Runs UI)
</output>
