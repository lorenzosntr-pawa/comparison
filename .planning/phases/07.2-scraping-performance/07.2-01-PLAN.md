---
phase: 07.2-scraping-performance
plan: 01
type: execute
---

<objective>
Parallelize Bet9ja scraping and add per-platform timing metrics to achieve sub-2-minute scrape times.

Purpose: The ~5 minute scrape time is primarily due to sequential Bet9ja tournament fetching. Parallelizing this (matching the existing BetPawa pattern) will achieve target speed.
Output: Fast parallel Bet9ja scraping, per-platform timing stored in database, timing visible in API responses.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07.2-scraping-performance/07.2-CONTEXT.md
@.planning/phases/07.2-scraping-performance/07.2-RESEARCH.md
@.planning/phases/06.1-cross-platform-scraping/06.1-01-SUMMARY.md
@.planning/phases/07.1-complete-odds-pipeline/07.1-01-SUMMARY.md

# Key source files:
@src/scraping/orchestrator.py
@src/db/models/scrape.py
@src/api/routes/scrape.py
@src/api/schemas/scheduler.py

**Constraining decisions:**
- [Phase 6.1]: asyncio.Semaphore(10) pattern for parallel API fetching
- [Phase 7.1]: BetPawa parallel competition scraping pattern established

**Pattern to follow:** Lines 261-288 in orchestrator.py show the BetPawa parallel scraping pattern. Apply same pattern to Bet9ja (lines 587-634).
</context>

<tasks>

<task type="auto">
  <name>Task 1: Parallelize Bet9ja tournament scraping</name>
  <files>src/scraping/orchestrator.py</files>
  <action>
Refactor `_scrape_bet9ja()` method (lines 587-634) to use asyncio.gather with semaphore, matching the BetPawa pattern:

1. Create `_scrape_bet9ja_parallel()` helper method or refactor inline:
   - Use `asyncio.Semaphore(10)` to limit concurrent tournament requests
   - Replace the sequential `for tournament_id in tournament_ids:` loop with `asyncio.gather()`
   - Inside each coroutine: `async with semaphore:` then `await client.fetch_events(tournament_id)`
   - Reduce rate limit delay from 0.2s to 0.05s (same as BetPawa per-request delay)
   - Use `return_exceptions=True` to handle individual tournament failures gracefully

2. Keep the same event parsing logic (call `_parse_bet9ja_event()` for each event)

3. Log count of tournaments being scraped at start (already exists)

DO NOT change the `_parse_bet9ja_event()` method - it works correctly.
DO NOT change the overall scrape_all() flow - just the internal Bet9ja method.
  </action>
  <verify>
1. Run scrape and time it: `time curl -X POST http://localhost:8000/api/scrape -H "Content-Type: application/json"`
2. Should complete in under 2 minutes (down from ~5 min)
3. Check logs show parallel tournament discovery: "Discovered N Bet9ja football tournaments" followed by quick completion
4. Verify events still scraped correctly: response should show events_count > 0 for Bet9ja
  </verify>
  <done>
- Bet9ja scraping uses asyncio.gather with semaphore (matching BetPawa pattern)
- Full scrape completes in under 2 minutes
- All three platforms return events successfully
  </done>
</task>

<task type="auto">
  <name>Task 2: Add per-platform timing metrics to ScrapeRun</name>
  <files>src/db/models/scrape.py, src/api/routes/scrape.py, src/api/schemas/scheduler.py</files>
  <action>
1. In `src/db/models/scrape.py`, add timing fields to ScrapeRun model:
   ```python
   # Per-platform timing in milliseconds (JSON column for flexibility)
   platform_timings: Mapped[dict | None] = mapped_column(JSON, nullable=True)
   # Format: {"betpawa": {"duration_ms": 1234, "events_count": 40}, ...}
   ```

2. In `src/api/routes/scrape.py`, update `trigger_scrape()` to store platform timings:
   - After scrape completes, build platform_timings dict from result.platforms
   - Set `scrape_run.platform_timings = {p.platform.value: {"duration_ms": p.duration_ms, "events_count": p.events_count} for p in result.platforms if p.success}`

3. In `src/api/schemas/scheduler.py`, add platform_timings to ScrapeRunResponse and ScrapeStatusResponse:
   ```python
   platform_timings: dict[str, dict] | None = None
   ```

4. Update the GET `/scrape/{scrape_run_id}` endpoint to return platform_timings.

DO NOT create a new Alembic migration - the JSON column will be added on next migration.
Use SQLAlchemy's JSON type (already imported in other models).
  </action>
  <verify>
1. Trigger scrape: `curl -X POST http://localhost:8000/api/scrape`
2. Get scrape status: `curl http://localhost:8000/api/scrape/{id}` - should include platform_timings
3. Verify timings dict shows ms values for each platform: {"betpawa": {"duration_ms": N, "events_count": M}, ...}
  </verify>
  <done>
- ScrapeRun model has platform_timings JSON column
- Scrape endpoint stores timing per platform
- API response includes platform_timings with duration_ms and events_count per platform
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Full scrape completes in under 2 minutes (was ~5 min)
- [ ] Bet9ja uses parallel tournament fetching (check logs)
- [ ] platform_timings stored in database and returned in API
- [ ] All three platforms still return events successfully
- [ ] No TypeScript/Python errors
</verification>

<success_criteria>

- Scrape time reduced from ~5 min to under 2 min
- Per-platform timing metrics available in API responses
- Bet9ja scraping parallelized with semaphore pattern
- No regressions in event scraping functionality
</success_criteria>

<output>
After completion, create `.planning/phases/07.2-scraping-performance/07.2-01-SUMMARY.md`:

# Phase 7.2 Plan 1: Backend Performance & Timing Summary

**[Substantive one-liner about what was accomplished]**

## Accomplishments
- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified
- `path/to/file.ts` - Description

## Decisions Made
[Key decisions and rationale, or "None"]

## Issues Encountered
[Problems and resolutions, or "None"]

## Next Step
Ready for 07.2-02-PLAN.md (SSE Progress Streaming)
</output>
