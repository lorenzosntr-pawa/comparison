---
phase: 14-scraping-logging-workflow
plan: 01
type: execute
---

<objective>
Set up backend infrastructure for granular scraping state tracking and structured logging.

Purpose: Enable complete visibility into scrape workflow state with audit trail and structured logs for debugging.
Output: structlog configuration, ScrapePhase/PlatformStatus enums, ScrapePhaseLog model, enhanced ScrapeRun model, Alembic migration.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-scraping-logging-workflow/14-CONTEXT.md
@.planning/phases/14-scraping-logging-workflow/14-RESEARCH.md

# Key source files:
@src/scraping/schemas.py
@src/db/models/scrape.py

# Prior summaries (dependency graph):
@.planning/phases/07.2-scraping-performance/07.2-01-SUMMARY.md
@.planning/phases/08-scrape-runs-ui-improvements/08-FIX2-SUMMARY.md

**Tech stack available:**
- SQLAlchemy 2.x async
- Pydantic v2
- StrEnum (Python 3.11+ builtin)
- structlog (to be added)

**Established patterns:**
- StrEnum for status fields (type safety + string storage)
- Mapped[] + mapped_column() for all columns
- JSONB (via JSON type) for variable-structure data
- Async Alembic with async_engine_from_config

**Constraining decisions:**
- Phase 7.2: platform_timings stored as JSON column
- Phase 8-FIX2: SSE streaming saves platform_timings to DB
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add structlog configuration and phase enums</name>
  <files>src/scraping/logging.py, src/scraping/schemas.py, requirements.txt</files>
  <action>
1. Add structlog to requirements.txt (version 24.x)

2. Create src/scraping/logging.py with configure_logging() function:
   - Shared processors: merge_contextvars, add_log_level, TimeStamper(fmt="iso"), StackInfoRenderer
   - JSON output mode (production): add JSONRenderer
   - Console output mode (development): add ConsoleRenderer(colors=True)
   - Use structlog.make_filtering_bound_logger(logging.INFO)
   - Export logger = structlog.get_logger() for module use

3. Update src/scraping/schemas.py:
   - Add ScrapePhase(StrEnum): INITIALIZING, DISCOVERING, SCRAPING, MAPPING, STORING, COMPLETED, FAILED
   - Add PlatformStatus(StrEnum): PENDING, ACTIVE, COMPLETED, FAILED
   - These enums provide type-safe phase tracking (NOT string literals)

4. Initialize structlog in src/main.py lifespan:
   - Import configure_logging from src/scraping/logging
   - Call configure_logging(json_output=False) at startup (dev mode)
   - Note: Production deployments can set json_output=True via env var

Do NOT use generic phase strings - always use the ScrapePhase enum for validation.
  </action>
  <verify>python -c "from src.scraping.logging import configure_logging, logger; configure_logging(); logger.info('test')"</verify>
  <done>structlog configured, ScrapePhase and PlatformStatus enums defined, logging initialized at startup</done>
</task>

<task type="auto">
  <name>Task 2: Add ScrapePhaseLog model and enhance ScrapeRun</name>
  <files>src/db/models/scrape.py, src/db/models/__init__.py</files>
  <action>
1. Enhance ScrapeRun model in src/db/models/scrape.py:
   - Add current_phase: Mapped[str | None] = mapped_column(String(30), nullable=True)
   - Add current_platform: Mapped[str | None] = mapped_column(String(20), nullable=True)
   - Add platform_status: Mapped[dict | None] = mapped_column(JSON, nullable=True)
     Format: {"betpawa": "completed", "sportybet": "active", "bet9ja": "pending"}
   - Add relationship: phase_logs: Mapped[list["ScrapePhaseLog"]] = relationship(back_populates="scrape_run", cascade="all, delete-orphan")

2. Create ScrapePhaseLog model in src/db/models/scrape.py:
   - id: Mapped[int] = mapped_column(primary_key=True)
   - scrape_run_id: Mapped[int] = mapped_column(ForeignKey("scrape_runs.id"))
   - platform: Mapped[str | None] = mapped_column(String(20), nullable=True)
   - phase: Mapped[str] = mapped_column(String(30))
   - started_at: Mapped[datetime] = mapped_column(server_default=func.now())
   - ended_at: Mapped[datetime | None] = mapped_column(nullable=True)
   - events_processed: Mapped[int | None] = mapped_column(nullable=True)
   - message: Mapped[str | None] = mapped_column(Text, nullable=True)
   - error_details: Mapped[dict | None] = mapped_column(JSON, nullable=True)
   - Relationship: scrape_run: Mapped["ScrapeRun"] = relationship(back_populates="phase_logs")
   - Index on scrape_run_id for efficient queries

3. Update src/db/models/__init__.py to export ScrapePhaseLog

All new fields are NULLABLE - no data migration required for existing rows.
Do NOT change existing fields or break the existing ScrapeRun structure.
  </action>
  <verify>python -c "from src.db.models.scrape import ScrapeRun, ScrapePhaseLog; print('Models valid')"</verify>
  <done>ScrapeRun enhanced with current_phase, current_platform, platform_status fields; ScrapePhaseLog model created</done>
</task>

<task type="auto">
  <name>Task 3: Create Alembic migration for new fields</name>
  <files>alembic/versions/[new_migration].py</files>
  <action>
1. Generate migration:
   alembic revision --autogenerate -m "add_scrape_phase_tracking"

2. Review and adjust the generated migration:
   - Add columns to scrape_runs table:
     - current_phase (varchar(30), nullable)
     - current_platform (varchar(20), nullable)
     - platform_status (JSON, nullable)
   - Create scrape_phase_logs table with all columns from model
   - Create index idx_scrape_phase_logs_run_id on scrape_run_id

3. Run migration: alembic upgrade head

4. Verify migration applied:
   - Check scrape_runs table has new columns
   - Check scrape_phase_logs table exists

Do NOT modify existing columns or add constraints that would fail on existing data.
All new columns must be nullable for backward compatibility.
  </action>
  <verify>alembic current && python -c "from src.db.base import engine; import asyncio; from sqlalchemy import text; asyncio.run((lambda: None)())"</verify>
  <done>Migration created and applied, scrape_phase_logs table exists, scrape_runs has new columns</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pip install -r requirements.txt` installs structlog
- [ ] `python -c "from src.scraping.logging import configure_logging"` succeeds
- [ ] `python -c "from src.scraping.schemas import ScrapePhase, PlatformStatus"` succeeds
- [ ] `python -c "from src.db.models.scrape import ScrapePhaseLog"` succeeds
- [ ] `alembic current` shows latest migration as head
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- structlog installed and configured
- ScrapePhase and PlatformStatus enums available
- ScrapePhaseLog model ready for state history
- ScrapeRun model enhanced with phase tracking fields
- Database migration applied successfully
</success_criteria>

<output>
After completion, create `.planning/phases/14-scraping-logging-workflow/14-01-SUMMARY.md`
</output>
