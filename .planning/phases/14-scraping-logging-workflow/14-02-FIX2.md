---
phase: 14-scraping-logging-workflow
plan: 14-02-FIX2
type: fix
---

<objective>
Fix 2 UAT issues from plan 14-02.

Source: 14-02-ISSUES.md
Priority: 1 blocker, 1 major
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md

**Issues being fixed:**
@.planning/phases/14-scraping-logging-workflow/14-02-ISSUES.md

**Original plan for reference:**
@.planning/phases/14-scraping-logging-workflow/14-02-PLAN.md

**Key source files:**
@src/api/routes/scrape.py
@src/scraping/orchestrator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix UAT-003 - Decouple scrape execution from SSE connection lifecycle</name>
  <files>src/api/routes/scrape.py</files>
  <action>
The blocker: `scrape_with_progress()` runs inline with the SSE response. When client disconnects,
the entire async context (including DB session) is cancelled via `CancelledError`.

**Solution: Run scrape as background task, use pub/sub for SSE updates**

1. Modify `stream_scrape()` endpoint to:
   - Create scrape_run record
   - Spawn the scrape as a detached background task via `asyncio.create_task()`
   - The background task uses its own DB session (not the request's)
   - Use the existing `progress_registry` broadcaster to publish progress
   - SSE endpoint subscribes to the broadcaster (already implemented in `/runs/{id}/progress`)

2. Implementation approach:

```python
@router.get("/stream")
async def stream_scrape(
    request: Request,
    db: AsyncSession = Depends(get_db),
    timeout: int = Query(default=300, ge=5, le=600),
) -> StreamingResponse:
    """Stream scrape progress via Server-Sent Events."""
    # Create ScrapeRun record
    scrape_run = ScrapeRun(status=ScrapeStatus.RUNNING, trigger="api-stream")
    db.add(scrape_run)
    await db.commit()
    await db.refresh(scrape_run)
    scrape_run_id = scrape_run.id

    # Spawn scrape as background task (not tied to SSE connection)
    async def run_scrape_background():
        """Background task that runs independent of SSE connection."""
        from src.db.engine import async_session_factory

        async with async_session_factory() as bg_db:
            # Build orchestrator
            sportybet = SportyBetClient(request.app.state.sportybet_client)
            betpawa = BetPawaClient(request.app.state.betpawa_client)
            bet9ja = Bet9jaClient(request.app.state.bet9ja_client)
            orchestrator = ScrapingOrchestrator(sportybet, betpawa, bet9ja)

            # Create broadcaster for this scrape
            broadcaster = progress_registry.create_broadcaster(scrape_run_id)

            # Track metrics for final update
            platform_timings = {}
            total_events = 0
            failed_count = 0
            final_status = ScrapeStatus.COMPLETED

            try:
                async for progress in orchestrator.scrape_with_progress(
                    timeout=float(timeout),
                    scrape_run_id=scrape_run_id,
                    db=bg_db,
                ):
                    # Broadcast progress to any connected SSE clients
                    await broadcaster.broadcast(progress)

                    # Track metrics (same logic as before)
                    if progress.platform and progress.phase == "completed":
                        platform_timings[progress.platform.value] = {
                            "duration_ms": progress.duration_ms or 0,
                            "events_count": progress.events_count or 0,
                        }
                        total_events += progress.events_count or 0
                    elif progress.platform and progress.phase == "failed":
                        failed_count += 1

                    # Check final status
                    if progress.platform is None and progress.phase in ("completed", "failed"):
                        if progress.phase == "failed":
                            final_status = ScrapeStatus.FAILED
                        elif failed_count > 0 and len(platform_timings) > 0:
                            final_status = ScrapeStatus.PARTIAL
                        elif failed_count > 0:
                            final_status = ScrapeStatus.FAILED
            except Exception as e:
                final_status = ScrapeStatus.FAILED
                await broadcaster.broadcast(ScrapeProgress(
                    phase="failed", current=0, total=0, message=str(e)
                ))
            finally:
                # Update ScrapeRun with results (using background DB session)
                scrape_run_obj = await bg_db.get(ScrapeRun, scrape_run_id)
                if scrape_run_obj:
                    scrape_run_obj.status = final_status
                    scrape_run_obj.completed_at = datetime.utcnow()
                    scrape_run_obj.events_scraped = total_events
                    scrape_run_obj.events_failed = failed_count
                    scrape_run_obj.platform_timings = platform_timings if platform_timings else None
                    await bg_db.commit()

                # Signal completion and clean up
                await broadcaster.close()
                progress_registry.remove_broadcaster(scrape_run_id)

    # Start background task (fire-and-forget, survives SSE disconnect)
    asyncio.create_task(run_scrape_background())

    # Small delay to ensure broadcaster is registered
    await asyncio.sleep(0.1)

    # Get broadcaster and stream to client
    broadcaster = progress_registry.get_broadcaster(scrape_run_id)

    async def event_generator():
        """SSE stream that subscribes to progress updates."""
        if not broadcaster:
            yield f"data: {json.dumps({'phase': 'error', 'message': 'Failed to start scrape'})}\n\n"
            return

        try:
            async for progress in broadcaster.subscribe():
                if await request.is_disconnected():
                    break
                yield f"data: {progress.model_dump_json()}\n\n"
        except Exception as e:
            yield f"data: {json.dumps({'phase': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={...}
    )
```

3. Add `async_session_factory` export from `src/db/engine.py` if not already available.
   This allows background tasks to create their own DB sessions.

4. Update imports at top of scrape.py:
   - Add `from src.scraping.schemas import ScrapeProgress`

**Key principle:** The scrape job runs to completion regardless of SSE client state.
SSE is purely for observation, not execution.
  </action>
  <verify>
1. Start backend
2. Trigger scrape via dashboard
3. While scraping, close browser tab
4. Reopen browser, check scrape runs page
5. Verify: scrape completed successfully, data stored
  </verify>
  <done>Scrape continues running to completion even when SSE client disconnects</done>
</task>

<task type="auto">
  <name>Task 2: Fix UAT-004 - Pass fresh SportRadar IDs from BetPawa to SportyBet scrape</name>
  <files>src/scraping/orchestrator.py</files>
  <action>
The issue: SportyBet scraping queries the database for existing events with SportRadar IDs,
rather than using the fresh IDs from the current BetPawa scrape. This means we may be
fetching odds for stale events instead of the exact events just scraped from BetPawa.

**Solution: Collect SportRadar IDs during BetPawa scrape, pass to SportyBet/Bet9ja**

1. Modify `scrape_with_progress()` to track SportRadar IDs from BetPawa:

```python
async def scrape_with_progress(self, ...) -> AsyncGenerator[ScrapeProgress, None]:
    # ... existing setup ...

    # Collect SportRadar IDs from BetPawa for cross-platform lookup
    betpawa_sportradar_ids: list[str] = []

    for idx, platform in enumerate(target_platforms):
        # ... existing scraping logic ...

        try:
            events, duration_ms = await self._scrape_platform(
                platform, sport_id, competition_id, False, timeout, db,
                sportradar_ids=betpawa_sportradar_ids if platform != Platform.BETPAWA else None,
            )

            # After BetPawa scrape, extract SportRadar IDs for competitors
            if platform == Platform.BETPAWA:
                betpawa_sportradar_ids = [
                    e["sportradar_id"] for e in events
                    if e.get("sportradar_id")
                ]
                logger.info(f"Collected {len(betpawa_sportradar_ids)} SportRadar IDs from BetPawa")

            # ... rest of existing logic ...
```

2. Update `_scrape_platform()` signature to accept optional `sportradar_ids`:

```python
async def _scrape_platform(
    self,
    platform: Platform,
    sport_id: str | None,
    competition_id: str | None,
    include_data: bool,
    timeout: float,
    db: AsyncSession | None = None,
    sportradar_ids: list[str] | None = None,  # NEW: IDs to fetch for competitor platforms
) -> tuple[list[dict], int]:
```

3. Modify `_scrape_sportybet()` to accept and use the IDs:

```python
async def _scrape_sportybet(
    self,
    client: SportyBetClient,
    db: AsyncSession,
    sportradar_ids: list[str] | None = None,
) -> list[dict]:
    """Scrape events from SportyBet via SportRadar ID lookup.

    If sportradar_ids provided, uses those directly (from current BetPawa scrape).
    Otherwise, queries database for upcoming events (fallback for non-streaming scrape).
    """
    # Use provided IDs if available, otherwise query DB
    if sportradar_ids:
        ids_to_fetch = sportradar_ids
        logger.info(f"Using {len(ids_to_fetch)} fresh SportRadar IDs from BetPawa session")
    else:
        # Fallback: query DB for existing events (used by scrape_all, not scrape_with_progress)
        result = await db.execute(
            select(Event).where(
                Event.sportradar_id.isnot(None),
                Event.kickoff > datetime.now(timezone.utc).replace(tzinfo=None),
            )
        )
        db_events = result.scalars().all()
        ids_to_fetch = [e.sportradar_id for e in db_events]
        logger.info(f"Using {len(ids_to_fetch)} SportRadar IDs from database")

    # ... rest of method uses ids_to_fetch instead of db_events ...
```

4. The same pattern applies to `_scrape_bet9ja()` - pass SportRadar IDs for matching:

```python
async def _scrape_bet9ja(
    self,
    client: Bet9jaClient,
    db: AsyncSession,
    sportradar_ids: list[str] | None = None,
) -> list[dict]:
    """Scrape events from Bet9ja, filtering to SportRadar IDs if provided."""
    # ... fetch all events ...

    # Filter to only events matching provided SportRadar IDs (if given)
    if sportradar_ids:
        sportradar_set = set(sportradar_ids)
        events = [e for e in events if e.get("sportradar_id") in sportradar_set]
        logger.info(f"Filtered to {len(events)} events matching BetPawa session")

    return events
```

**Key principle:** Within a scrape session, use the SportRadar IDs we just collected
from BetPawa to ensure we're comparing the exact same events across all platforms.
  </action>
  <verify>
1. Run a scrape and observe logs
2. Look for log: "Using X fresh SportRadar IDs from BetPawa session"
3. Verify: SportyBet/Bet9ja scrape uses IDs from current session, not stale DB query
  </verify>
  <done>SportyBet and Bet9ja scraping use fresh SportRadar IDs from current BetPawa scrape session</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] SSE disconnect does NOT cancel scrape (blocker fix verified)
- [ ] Scrape runs complete successfully in background
- [ ] SportyBet scraping uses fresh IDs from BetPawa session
- [ ] Bet9ja scraping filtered to match BetPawa events
- [ ] Backend starts without errors
- [ ] Existing SSE streaming still works for clients that stay connected
</verification>

<success_criteria>
- UAT-003 fixed: Scrape continues even when SSE client disconnects
- UAT-004 fixed: Competitor scraping uses fresh session IDs from BetPawa
- All tests pass
- Ready for re-verification
</success_criteria>

<output>
After completion, create `.planning/phases/14-scraping-logging-workflow/14-02-FIX2-SUMMARY.md`
</output>
