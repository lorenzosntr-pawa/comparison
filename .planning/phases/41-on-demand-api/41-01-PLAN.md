---
phase: 41-on-demand-api
plan: 01
type: execute
domain: scraping
---

<objective>
Add single-event on-demand refresh endpoint POST /api/scrape/{sr_id}

Purpose: Enable manual refresh of a specific event's odds across all bookmakers without running a full scrape cycle. Useful for traders monitoring specific matches or refreshing stale data.
Output: Working API endpoint with request/response schemas, per-platform success/failure tracking.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase summaries (v1.7 Scraping Architecture)
@.planning/phases/38-sr-id-parallel-scraping/38-01-SUMMARY.md
@.planning/phases/39-batch-db-storage/39-01-SUMMARY.md
@.planning/phases/40-concurrency-tuning-metrics/40-01-SUMMARY.md

# Key source files
@src/api/routes/scrape.py
@src/scraping/event_coordinator.py
@src/scraping/schemas/coordinator.py
@src/api/schemas/scheduler.py

**Tech stack available:** EventCoordinator with parallel scraping, store_batch_results(), FastAPI patterns
**Established patterns:**
- EventTarget dataclass with platform_ids for API calls
- _scrape_event_all_platforms() for parallel platform scraping
- store_batch_results() for bulk DB inserts
- Pydantic schemas in src/api/schemas/

**Key decisions from prior phases:**
- Phase 38: platform_ids stores platform-specific IDs (BetPawa ID, sr:match: format, Bet9ja ID)
- Phase 38: Partial success (some platforms) = COMPLETED, total failure = FAILED
- Phase 39: EventScrapeStatus tracks per-event success/failure
- Phase 40: EventCoordinator.from_settings() for configurable initialization
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add single-event scrape response schema</name>
  <files>src/api/schemas/scheduler.py, src/api/schemas/__init__.py</files>
  <action>
Create Pydantic schemas for single-event scrape endpoint:

1. `SingleEventPlatformResult` - Per-platform result:
   - platform: str
   - success: bool
   - timing_ms: int | None
   - error: str | None
   - markets_count: int | None (number of markets scraped if success)

2. `SingleEventScrapeResponse` - Endpoint response:
   - sportradar_id: str
   - status: str ("completed" | "partial" | "failed")
   - platforms_scraped: list[str]
   - platforms_failed: list[str]
   - platform_results: list[SingleEventPlatformResult]
   - total_timing_ms: int
   - event_scrape_status_id: int | None (FK to EventScrapeStatus if created)

Export both from `src/api/schemas/__init__.py`.

Follow existing schema patterns (see ScrapeStatusResponse, EventMetricsByPlatform for reference).
  </action>
  <verify>python -c "from src.api.schemas import SingleEventScrapeResponse, SingleEventPlatformResult; print('Schemas OK')"</verify>
  <done>Schemas importable, fields match specification</done>
</task>

<task type="auto">
  <name>Task 2: Add POST /api/scrape/{sr_id} endpoint</name>
  <files>src/api/routes/scrape.py</files>
  <action>
Add new endpoint `POST /api/scrape/{sr_id}` that:

1. **Look up platform IDs:**
   - Query `Event` table by `sportradar_id` to get BetPawa event ID
   - Query `CompetitorEvent` table by `sportradar_id` to get Bet9ja event ID (where source='bet9ja')
   - Build SportyBet ID as `f"sr:match:{sr_id}"` (this is the format SportyBet API expects)
   - Collect available platforms (only include if platform_id found)

2. **Validate:**
   - Return 404 if no platform IDs found (event not in database)
   - At minimum, need at least one platform to scrape

3. **Build EventTarget:**
   ```python
   from src.scraping.schemas.coordinator import EventTarget, ScrapeStatus
   from datetime import datetime, timezone

   event = EventTarget(
       sr_id=sr_id,
       kickoff=datetime.now(timezone.utc),  # Not used for on-demand
       platforms=set(platform_ids.keys()),
       platform_ids=platform_ids,
       status=ScrapeStatus.PENDING,
   )
   ```

4. **Scrape:**
   - Get clients from `request.app.state`
   - Create semaphores dict: `{platform: asyncio.Semaphore(1) for platform in platform_ids}` (single concurrent request per platform for on-demand)
   - Call `coordinator._scrape_event_all_platforms(event, semaphores)` (make method public by adding `scrape_single_event()` wrapper, or just inline the logic)

   Actually, to avoid modifying EventCoordinator, inline the scraping logic:
   ```python
   async def scrape_platform(platform: str, platform_id: str):
       client = clients[platform]
       start = time.perf_counter()
       try:
           result = await client.fetch_event(platform_id)
           elapsed = int((time.perf_counter() - start) * 1000)
           return (platform, result, None, elapsed)
       except Exception as e:
           elapsed = int((time.perf_counter() - start) * 1000)
           return (platform, None, str(e), elapsed)

   tasks = [scrape_platform(p, pid) for p, pid in platform_ids.items()]
   results = await asyncio.gather(*tasks)
   ```

5. **Store results (optional - create EventScrapeStatus for tracking):**
   - Only store if scrape_run_id is provided (from optional query param)
   - For on-demand, can skip full storage and just return results
   - If storing, create single EventScrapeStatus record

6. **Build response:**
   - Aggregate platform results
   - Determine overall status: "completed" if all succeeded, "partial" if some, "failed" if none
   - Include per-platform details (timing, error, success)

**Endpoint signature:**
```python
@router.post("/{sr_id}", response_model=SingleEventScrapeResponse)
async def scrape_single_event(
    sr_id: str,
    request: Request,
    db: AsyncSession = Depends(get_db),
    store_result: bool = Query(default=False, description="Store result to EventScrapeStatus"),
) -> SingleEventScrapeResponse:
```

Add import for `time` at top of file if not present.
  </action>
  <verify>
1. Start dev server: cd src && uvicorn main:app --reload (in separate terminal)
2. Test with known SR ID from database:
   - curl -X POST http://localhost:8000/api/scrape/12345678 (replace with real SR ID)
   - Should return JSON with platform results
3. Test with unknown SR ID:
   - curl -X POST http://localhost:8000/api/scrape/99999999999
   - Should return 404
  </verify>
  <done>Endpoint returns per-platform scrape results for valid SR IDs, 404 for unknown</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `python -c "from src.api.schemas import SingleEventScrapeResponse"` succeeds
- [ ] `python -c "from src.api.routes.scrape import router"` succeeds
- [ ] Endpoint responds to POST /api/scrape/{sr_id} with valid JSON
- [ ] 404 returned for non-existent SR IDs
- [ ] Per-platform success/failure correctly reported
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Endpoint works for known events, returns 404 for unknown
- Per-platform timing and error details included in response
</success_criteria>

<output>
After completion, create `.planning/phases/41-on-demand-api/41-01-SUMMARY.md`:

# Phase 41 Plan 01: On-Demand API Summary

**[Substantive one-liner - what shipped]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `path/to/file.ts` - Description

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Phase Readiness

Ready for Phase 42: Validation & Cleanup
</output>
