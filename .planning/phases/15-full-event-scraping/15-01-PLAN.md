---
phase: 15-full-event-scraping
plan: 01
type: execute
---

<objective>
Create CompetitorEventScrapingService to scrape ALL football events from SportyBet and Bet9ja tournaments.

Purpose: Enable full palimpsest comparison by capturing all competitor events, not just those matching betpawa.
Output: Working service that scrapes all events from competitor_tournaments and stores in competitor_events with odds snapshots.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-full-event-scraping/15-CONTEXT.md
@.planning/phases/14-tournament-discovery-scraping/14-01-SUMMARY.md
@.planning/phases/13-database-schema-extension/13-02-SUMMARY.md

**Key files:**
@src/scraping/clients/sportybet.py
@src/scraping/clients/bet9ja.py
@src/scraping/tournament_discovery.py
@src/db/models/competitor.py

**Prior decisions:**
- Phase 13: CompetitorEvent, CompetitorOddsSnapshot, CompetitorMarketOdds models created
- Phase 14: TournamentDiscoveryService stores tournaments in competitor_tournaments
- SportRadar ID as primary matching key across platforms
- Metadata priority chain: betpawa > sportybet > bet9ja

**API Reference (from captured responses):**
- SportyBet events by tournament: POST /api/ng/factsCenter/pcEvents
  - Payload: `[{sportId: "sr:sport:1", marketId: "1,18,10,29,11,26,36,14", tournamentId: [["sr:tournament:17"]]}]`
  - Returns events with full market/odds data
- Bet9ja events by tournament: GET /desktop/feapi/PalimpsestAjax/GetEventsInGroupV2 (already implemented)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add fetch_events_by_tournament() to SportyBetClient</name>
  <files>src/scraping/clients/sportybet.py</files>
  <action>
Add new method `fetch_events_by_tournament(tournament_id: str, market_ids: str = "1,18,10,29,11,26,36,14")` that:
1. POSTs to `/api/ng/factsCenter/pcEvents`
2. Payload: `[{"sportId": "sr:sport:1", "marketId": market_ids, "tournamentId": [[tournament_id]]}]`
3. Headers: same as existing methods + `"content-type": "application/json"`
4. Returns list of event dicts from `data[0]["events"]`
5. Uses existing `@_retry` decorator
6. Raises ApiError if bizCode != 10000

Market IDs included cover main markets: 1=1X2, 18=O/U, 10=DC, 29=BTTS, 11=DNB, 26=HT/FT, 36=Correct Score, 14=HT 1X2.
  </action>
  <verify>Method exists and follows existing client patterns. Test import: `from src.scraping.clients.sportybet import SportyBetClient`</verify>
  <done>SportyBetClient has fetch_events_by_tournament() method that POSTs to pcEvents endpoint</done>
</task>

<task type="auto">
  <name>Task 2: Create CompetitorEventScrapingService</name>
  <files>src/scraping/competitor_events.py</files>
  <action>
Create new service class `CompetitorEventScrapingService` with:

1. **Dependencies:** SportyBetClient, Bet9jaClient

2. **Core methods:**
   - `scrape_sportybet_events(db: AsyncSession, scrape_run_id: int | None = None) -> dict`
     - Query competitor_tournaments WHERE source='sportybet' AND deleted_at IS NULL
     - For each tournament, call client.fetch_events_by_tournament(external_id)
     - Parse events and upsert to competitor_events
     - Create competitor_odds_snapshots with parsed markets
     - Return {"new": N, "updated": N, "snapshots": N, "markets": N}

   - `scrape_bet9ja_events(db: AsyncSession, scrape_run_id: int | None = None) -> dict`
     - Query competitor_tournaments WHERE source='bet9ja' AND deleted_at IS NULL
     - For each tournament, call client.fetch_events(external_id)
     - Parse events and upsert to competitor_events
     - Create competitor_odds_snapshots with parsed markets
     - Return {"new": N, "updated": N, "snapshots": N, "markets": N}

3. **Helper methods:**
   - `_upsert_competitor_event(db, source, tournament_id, event_data) -> CompetitorEvent`
     - Use source + external_id as unique key
     - Extract: sportradar_id, name, home_team, away_team, kickoff, external_id
     - Link betpawa_event_id if SR ID matches an Event record

   - `_parse_sportybet_event(event_data, tournament_id) -> dict`
     - Extract eventId → sportradar_id (remove 'sr:match:' prefix)
     - Extract homeTeamName, awayTeamName, estimateStartTime
     - Parse markets array for odds

   - `_parse_bet9ja_event(event_data, tournament_id) -> dict`
     - Extract EXTID → sportradar_id
     - Parse DS field for home/away teams
     - Parse STARTDATE for kickoff
     - Extract O dict for odds

4. **Market parsing (reuse patterns from orchestrator.py:1053-1148):**
   - `_parse_sportybet_markets(markets: list) -> list[CompetitorMarketOdds]`
   - `_parse_bet9ja_markets(odds_dict: dict) -> list[CompetitorMarketOdds]`
   - Use existing market mapping logic from orchestrator

5. **Concurrency:** Use semaphore (10 concurrent) for tournament fetching, similar to orchestrator patterns.

6. **Logging:** Use structlog, log tournament counts and event counts per platform.
  </action>
  <verify>`from src.scraping.competitor_events import CompetitorEventScrapingService` imports without error. Class has scrape_sportybet_events and scrape_bet9ja_events methods.</verify>
  <done>CompetitorEventScrapingService exists with full event scraping for both platforms, stores in competitor_events and competitor_odds_snapshots</done>
</task>

<task type="auto">
  <name>Task 3: Add API endpoint for competitor event scraping</name>
  <files>src/api/routes/scheduler.py, src/api/schemas/scheduler.py</files>
  <action>
Add POST /api/scheduler/scrape-competitor-events endpoint:

1. **Schema (scheduler.py):**
   ```python
   class CompetitorScrapeResult(BaseModel):
       platform: str
       new_events: int
       updated_events: int
       snapshots: int
       markets: int
       error: str | None = None

   class CompetitorScrapeResponse(BaseModel):
       sportybet: CompetitorScrapeResult
       bet9ja: CompetitorScrapeResult
       duration_ms: int
   ```

2. **Endpoint (scheduler.py):**
   - Create clients using existing factory pattern
   - Instantiate CompetitorEventScrapingService
   - Call scrape_sportybet_events() and scrape_bet9ja_events()
   - Handle partial failures (one platform can fail, other continues)
   - Return combined results

3. **Error handling:** Wrap each platform scrape in try/except, populate error field on failure.
  </action>
  <verify>curl -X POST http://localhost:8000/api/scheduler/scrape-competitor-events returns JSON with sportybet and bet9ja results</verify>
  <done>API endpoint exists and successfully scrapes competitor events to database</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] SportyBetClient.fetch_events_by_tournament() works (test with sr:tournament:17)
- [ ] CompetitorEventScrapingService scrapes both platforms
- [ ] competitor_events table has records from both sources
- [ ] competitor_odds_snapshots have market odds
- [ ] betpawa_event_id populated for matching events
- [ ] API endpoint returns successful response
- [ ] No TypeScript/Python errors
</verification>

<success_criteria>

- All tasks completed
- SportyBet and Bet9ja full event scraping functional
- Events stored in competitor_events with odds in competitor_odds_snapshots
- SR ID matching links competitor events to betpawa events
- API endpoint triggers competitor scraping
</success_criteria>

<output>
After completion, create `.planning/phases/15-full-event-scraping/15-01-SUMMARY.md`
</output>
