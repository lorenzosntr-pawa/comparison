---
phase: 14-tournament-discovery-scraping
plan: 01
type: execute
---

<objective>
Scrape full football tournament lists from SportyBet and Bet9ja APIs and store in CompetitorTournament model.

Purpose: Build complete picture of competitor tournament coverage as foundation for palimpsest comparison.
Output: TournamentDiscoveryService that populates competitor_tournaments table with ~200+ tournaments per platform.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-tournament-discovery-scraping/14-RESEARCH.md
@.planning/phases/14-tournament-discovery-scraping/14-CONTEXT.md
@.planning/phases/13-database-schema-extension/13-01-SUMMARY.md
@.planning/phases/13-database-schema-extension/13-02-SUMMARY.md

**Key files:**
@src/scraping/clients/sportybet.py
@src/scraping/clients/bet9ja.py
@src/db/models/competitor.py

**Tech stack available:**
- httpx + tenacity for HTTP with retry
- SQLAlchemy 2.0 async ORM
- CompetitorTournament, CompetitorSource models (Phase 13)

**Established patterns:**
- Retry decorator from base.py
- Response validation helpers
- Upsert via unique constraint check

**Constraining decisions:**
- [Phase 13-01]: sportradar_id nullable on CompetitorTournament (Bet9ja has no SR IDs at tournament level)
- [Phase 13-01]: source + external_id unique constraint for upsert
- [CONTEXT]: Football only, tournaments only (events in Phase 15)
- [CONTEXT]: Store separately per platform, don't try to match tournaments

**API structures (from RESEARCH):**
- SportyBet: GET /api/ng/factsCenter/popularAndSportList returns complete hierarchy
- Bet9ja: GET /desktop/feapi/PalimpsestAjax/GetSports returns D.PAL structure (already implemented)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add fetch_tournaments() to SportyBetClient</name>
  <files>src/scraping/clients/sportybet.py</files>
  <action>
Add new method `fetch_tournaments()` to SportyBetClient:

1. Endpoint: GET /api/ng/factsCenter/popularAndSportList
2. Parameters: sportId="sr:sport:1" (football), timeline="", productId="3", _t={timestamp_ms}
3. Follow existing fetch_event() pattern: use _retry decorator, handle NetworkError
4. Validate bizCode == 10000 (success)
5. Return full response dict (let caller navigate data.sportList.categories.tournaments)

Avoid: Parsing/filtering response in client - that's the service layer's job. Client just fetches and validates basic structure.
  </action>
  <verify>
```python
# Test in Python REPL or add quick test script
import asyncio
import httpx
from src.scraping.clients.sportybet import SportyBetClient

async def test():
    async with httpx.AsyncClient(timeout=30.0) as client:
        sporty = SportyBetClient(client)
        data = await sporty.fetch_tournaments()
        print(f"bizCode: {data.get('bizCode')}")
        sport_list = data.get('data', {}).get('sportList', [])
        if sport_list:
            categories = sport_list[0].get('categories', [])
            print(f"Categories: {len(categories)}")

asyncio.run(test())
```
Should print bizCode: 10000 and Categories: 50+
  </verify>
  <done>SportyBetClient.fetch_tournaments() returns valid tournament hierarchy, follows existing client patterns</done>
</task>

<task type="auto">
  <name>Task 2: Create TournamentDiscoveryService</name>
  <files>src/scraping/tournament_discovery.py</files>
  <action>
Create new service at src/scraping/tournament_discovery.py:

1. Class TournamentDiscoveryService with async methods:
   - discover_sportybet_tournaments(client: SportyBetClient, db: AsyncSession) -> tuple[int, int]
   - discover_bet9ja_tournaments(client: Bet9jaClient, db: AsyncSession) -> tuple[int, int]
   - discover_all(sportybet_client, bet9ja_client, db) -> dict with both results
   - _upsert_tournament(db, source, external_id, name, country_raw, sportradar_id=None) -> tuple[bool, bool]

2. SportyBet discovery logic:
   - Call client.fetch_tournaments()
   - Navigate: data.sportList[0].categories (where sportList[0].id == "sr:sport:1")
   - For each category: country_raw = category.name
   - For each tournament: external_id = tournament.id (e.g., "sr:tournament:17"), name = tournament.name
   - Extract sportradar_id from external_id: "sr:tournament:17" -> "17"

3. Bet9ja discovery logic:
   - Call client.fetch_sports()
   - Navigate: D.PAL.1.SG (Soccer subgroups)
   - For each subgroup: country_raw = SG_DESC
   - For each tournament (G entries): external_id = g_id, name = G_DESC
   - sportradar_id = None (Bet9ja has no SR IDs at tournament level)

4. Upsert logic:
   - Query by source + external_id (unique constraint)
   - If exists: update name, country_raw if changed, return (0, 1) for updated
   - If not exists: insert new, return (1, 0) for new
   - Commit after all upserts

5. Sport ID lookup:
   - Query sports table for name="Football" or seed it if missing
   - All tournaments get sport_id pointing to Football record

Avoid: Trying to match tournaments across platforms. Store separately, link later via event matching.
  </action>
  <verify>
```python
import asyncio
import httpx
from sqlalchemy import select, func
from src.db.engine import async_session
from src.db.models.competitor import CompetitorTournament, CompetitorSource
from src.scraping.clients.sportybet import SportyBetClient
from src.scraping.clients.bet9ja import Bet9jaClient
from src.scraping.tournament_discovery import TournamentDiscoveryService

async def test():
    async with httpx.AsyncClient(timeout=30.0) as http_client:
        async with async_session() as db:
            service = TournamentDiscoveryService()
            sporty = SportyBetClient(http_client)
            bet9ja = Bet9jaClient(http_client)

            new_s, upd_s = await service.discover_sportybet_tournaments(sporty, db)
            new_b, upd_b = await service.discover_bet9ja_tournaments(bet9ja, db)

            print(f"SportyBet: {new_s} new, {upd_s} updated")
            print(f"Bet9ja: {new_b} new, {upd_b} updated")

            # Verify counts
            result = await db.execute(
                select(CompetitorTournament.source, func.count())
                .group_by(CompetitorTournament.source)
            )
            for source, count in result:
                print(f"{source}: {count} tournaments")

asyncio.run(test())
```
Should show 100+ tournaments per source, running twice shows 0 new (upsert works)
  </verify>
  <done>TournamentDiscoveryService populates competitor_tournaments for both sources, upsert prevents duplicates</done>
</task>

<task type="auto">
  <name>Task 3: Add API endpoint for tournament discovery</name>
  <files>src/api/routes/scheduler.py</files>
  <action>
Add new endpoint to scheduler routes:

1. POST /api/scheduler/discover-tournaments
   - Triggers tournament discovery for both platforms
   - Returns JSON with discovery results: { sportybet: { new, updated }, bet9ja: { new, updated }, total_tournaments: N }

2. Implementation:
   - Import TournamentDiscoveryService
   - Create httpx.AsyncClient (or reuse if pattern exists)
   - Create SportyBetClient and Bet9jaClient
   - Call service.discover_all()
   - Query total tournament count for response
   - Add structlog logging for operation start/complete

3. Error handling:
   - If one platform fails, still return partial results
   - Log errors but don't fail entire request
   - Return error details in response for failed platform

Avoid: Over-engineering with scheduling integration. This phase just adds manual trigger endpoint. Scheduling can be added later if needed.
  </action>
  <verify>
```bash
# Start server
cd c:\Users\loren\Desktop\betpawa\comparison\mvp1
python -m uvicorn src.api.app:app --reload

# In another terminal
curl -X POST http://localhost:8000/api/scheduler/discover-tournaments | jq
```
Should return JSON like:
{
  "sportybet": { "new": 150, "updated": 0 },
  "bet9ja": { "new": 200, "updated": 0 },
  "total_tournaments": 350
}
  </verify>
  <done>POST /api/scheduler/discover-tournaments triggers discovery and returns results, logged with structlog</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `SportyBetClient.fetch_tournaments()` exists and returns valid data
- [ ] `TournamentDiscoveryService` discovers tournaments from both APIs
- [ ] `competitor_tournaments` table populated with both sources
- [ ] Running discovery twice doesn't create duplicates (upsert works)
- [ ] API endpoint `/api/scheduler/discover-tournaments` returns results
- [ ] Database query shows tournaments:
```sql
SELECT source, COUNT(*) FROM competitor_tournaments GROUP BY source;
-- Should show counts for both sportybet and bet9ja

SELECT * FROM competitor_tournaments WHERE source = 'sportybet' LIMIT 3;
-- Should have sportradar_id populated

SELECT * FROM competitor_tournaments WHERE source = 'bet9ja' LIMIT 3;
-- sportradar_id should be NULL
```
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Both platforms have 100+ football tournaments stored
- Upsert logic prevents duplicates on re-run
- API endpoint accessible and returns valid JSON
</success_criteria>

<output>
After completion, create `.planning/phases/14-tournament-discovery-scraping/14-01-SUMMARY.md`:

# Phase 14 Plan 1: Tournament Discovery Scraping Summary

**[Substantive one-liner - what shipped]**

## Accomplishments

- [Key outcomes]

## Files Created/Modified

- `path/to/file.ts` - Description

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Phase Readiness

[Ready state for Phase 15: Full Event Scraping]
</output>
