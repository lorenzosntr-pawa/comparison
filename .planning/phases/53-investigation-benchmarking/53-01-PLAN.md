---
phase: 53-investigation-benchmarking
plan: 01
type: execute
---

<objective>
Measure current scraping pipeline bottlenecks and establish baseline performance metrics for the v2.0 Real-Time Scraping Pipeline milestone.

Purpose: Quantify where time is spent across discovery, scraping, storage, and API serving so that subsequent phases (cache layer, async writes, concurrency tuning) target real bottlenecks rather than assumptions.
Output: Instrumented EventCoordinator with granular timing, benchmark script, and baseline report in `.planning/phases/53-investigation-benchmarking/`.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Key source files:
@src/scraping/event_coordinator.py
@src/api/routes/scrape.py
@src/api/routes/events.py
@src/scheduling/jobs.py
@src/scraping/broadcaster.py
@src/db/models/odds.py
@src/db/models/scrape.py

**Tech stack available:** Python 3.11+, FastAPI, SQLAlchemy 2.0 async, structlog, APScheduler, asyncio
**Established patterns:** Event-centric parallel scraping (v1.7), single-flush batch insert, factory method (EventCoordinator.from_settings()), SSE progress streaming with per-platform events

**Constraining decisions:**
- v1.7: EventCoordinator is the single orchestrator for all scraping
- v1.7: AsyncSession cannot be shared across concurrent asyncio tasks
- v1.7: Single-flush batch insert pattern (add all → flush → link FKs → commit)
- v1.5: SSE progress events already contain batch_scrape_ms, batch_store_ms, timing_ms per event

**v2.0 Constraints (from milestone planning):**
- Memory usage for cache must stay reasonable (<100MB for active events)
- Keep current fixed scrape interval schedule (no change to refresh timing)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add granular timing instrumentation to EventCoordinator</name>
  <files>src/scraping/event_coordinator.py</files>
  <action>
Add fine-grained timing measurement to the EventCoordinator pipeline phases that are NOT currently timed. The existing progress events already track `timing_ms` (per event), `batch_scrape_ms`, `batch_store_ms`, and `total_timing_ms`. What's missing:

**1. Discovery phase per-platform timing:**
In `discover_events()` (or the individual `_discover_*` methods), wrap each platform's discovery call with `time.perf_counter()` and include per-platform timing in the `DISCOVERY_COMPLETE` progress event dict. Add fields:
- `betpawa_discovery_ms: int`
- `sportybet_discovery_ms: int`
- `bet9ja_discovery_ms: int`
- `discovery_total_ms: int` (wall-clock for the parallel gather)

**2. Storage sub-phase breakdown:**
In `store_batch_results()`, add timing around the key sub-phases and include in the `BATCH_COMPLETE` progress event dict:
- `storage_lookups_ms: int` — time for pre-fetch queries (_get_bookmaker_ids, _get_event_ids_by_sr_ids, etc.)
- `storage_processing_ms: int` — time for event/snapshot/market object creation loop
- `storage_flush_ms: int` — time for the `db.flush()` call that gets snapshot IDs
- `storage_commit_ms: int` — time for the final `db.commit()`

Use `time.perf_counter()` for all timing. Convert to milliseconds (int). Do NOT change the method signatures or return types — just enrich the existing progress event dicts with additional fields. The broadcaster and SSE consumers will pass through unknown fields automatically since they serialize dicts.

**3. Per-event platform breakdown:**
In `_scrape_event_all_platforms()` or wherever individual platform results are collected, ensure the per-event timing includes which platform was slowest. Add to the `EVENT_SCRAPED` progress event:
- `platform_timings: dict[str, int]` — e.g. `{"betpawa": 234, "sportybet": 456, "bet9ja": 789}`

Keep all changes backward-compatible — these are additive fields on existing dict events.
  </action>
  <verify>
Run `python -c "from src.scraping.event_coordinator import EventCoordinator; print('Import OK')"` to verify no syntax errors. Grep for `perf_counter` in event_coordinator.py to confirm instrumentation was added. Count should be 8-12 usage sites (4 discovery + 4 storage + per-event platforms).
  </verify>
  <done>
EventCoordinator emits enriched progress events with: per-platform discovery timing, storage sub-phase breakdown (lookups/processing/flush/commit), and per-event platform timing breakdown. All existing progress events still work unchanged.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create benchmark script and produce baseline report</name>
  <files>scripts/benchmark_pipeline.py, .planning/phases/53-investigation-benchmarking/BENCHMARK-BASELINE.md</files>
  <action>
Create a benchmark script at `scripts/benchmark_pipeline.py` that:

**1. Runs a real scrape cycle** using the existing EventCoordinator and database:
- Import the app's database session factory, client factories, and EventCoordinator
- Create a one-off coordinator instance using current settings
- Run `run_full_cycle()` and collect ALL progress events into a list
- This is a real scrape against live APIs — same as a scheduled run

**2. Measures API response latency:**
After the scrape completes, measure response time for key API endpoints using httpx against the local FastAPI app:
- `GET /api/events` — event list with inline odds
- `GET /api/events/{id}` — single event detail with all markets
- `GET /api/scrape/runs` — scrape run history
Run each endpoint 5 times, record p50 and p95 latency.

For API latency measurement: if the FastAPI server isn't running, skip this section and note "API latency: server not running, measure manually". Use `httpx.AsyncClient` with a short timeout (10s) and catch connection errors gracefully.

**3. Measures memory usage:**
Use `tracemalloc` (stdlib) to snapshot memory before and after the scrape cycle. Record:
- Peak memory during scrape
- Memory delta (after - before)
- Current RSS via `resource.getrusage` (Unix) or `psutil.Process().memory_info()` if available (fall back gracefully)

**4. Produces a structured report** at `.planning/phases/53-investigation-benchmarking/BENCHMARK-BASELINE.md`:

```markdown
# Benchmark Baseline Report

**Date:** [timestamp]
**Events discovered:** [count]
**Batches processed:** [count]

## Discovery Phase
| Platform | Time (ms) | Events Found |
|----------|-----------|--------------|
| BetPawa  | X         | Y            |
| SportyBet| X         | Y            |
| Bet9ja   | X         | Y            |
| **Total (wall-clock)** | X | Y (merged) |

## Batch Processing
| Metric | Value |
|--------|-------|
| Batch count | N |
| Avg batch scrape time (ms) | X |
| Avg batch storage time (ms) | X |
| Storage % of total batch time | X% |
| Scrape % of total batch time | X% |

### Storage Breakdown (avg per batch)
| Sub-phase | Time (ms) | % of Storage |
|-----------|-----------|--------------|
| Lookups | X | X% |
| Processing | X | X% |
| Flush | X | X% |
| Commit | X | X% |

## Per-Event Scraping
| Metric | Value |
|--------|-------|
| Avg event scrape time (ms) | X |
| P50 event scrape time (ms) | X |
| P95 event scrape time (ms) | X |
| Slowest platform (most often) | [platform] |

### Platform Timing Distribution
| Platform | Avg (ms) | P50 (ms) | P95 (ms) |
|----------|----------|----------|----------|
| BetPawa  | X | X | X |
| SportyBet| X | X | X |
| Bet9ja   | X | X | X |

## API Response Latency
| Endpoint | P50 (ms) | P95 (ms) |
|----------|----------|----------|
| GET /api/events | X | X |
| GET /api/events/{id} | X | X |
| GET /api/scrape/runs | X | X |

## Memory
| Metric | Value |
|--------|-------|
| Peak memory (MB) | X |
| Memory delta (MB) | X |

## Bottleneck Analysis
[Auto-generated summary identifying where most time is spent]
- Discovery: X% of total
- Scraping: X% of total
- Storage: X% of total
- Storage breakdown: [which sub-phase dominates]
- Recommendation: [which v2.0 phase will have biggest impact]
```

The script should be runnable with: `python scripts/benchmark_pipeline.py`

Use `asyncio.run()` as the entry point. Import from the `src` package (adjust sys.path if needed). Handle errors gracefully — if a measurement fails, note it in the report rather than crashing.

The bottleneck analysis section should compute percentages and identify the dominant cost center, then map it to the v2.0 phase that addresses it:
- If storage dominates → Phase 55 (Async Write Pipeline) is highest priority
- If scraping dominates → Phase 56 (Concurrency Tuning) is highest priority
- If discovery dominates → optimize discovery in Phase 56
- If API latency is high → Phase 54 (Cache Layer) is highest priority
  </action>
  <verify>
Run `python scripts/benchmark_pipeline.py` and verify it produces a benchmark report at `.planning/phases/53-investigation-benchmarking/BENCHMARK-BASELINE.md`. The report should contain all sections with actual measurements (not placeholder values). Check that the script exits cleanly without errors.
  </verify>
  <done>
Benchmark script runs successfully, produces a complete baseline report with: discovery timing per platform, batch scrape/storage breakdown with storage sub-phases, per-event platform timing distribution (avg/p50/p95), API latency (if server running), memory measurements, and automated bottleneck analysis identifying which v2.0 phase will have the highest impact.
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] EventCoordinator imports and runs without errors
- [ ] Progress events contain new timing fields (discovery per-platform, storage sub-phases, platform timings)
- [ ] Benchmark script runs end-to-end
- [ ] BENCHMARK-BASELINE.md exists with real measurements
- [ ] Bottleneck analysis identifies the dominant cost center
- [ ] No regressions in existing scraping functionality
</verification>

<success_criteria>

- EventCoordinator enriched with granular timing instrumentation
- Benchmark script produces reproducible baseline measurements
- BENCHMARK-BASELINE.md documents current performance with quantified bottlenecks
- Bottleneck analysis maps findings to specific v2.0 phases
- Existing scraping pipeline unchanged in behavior (timing is additive)
</success_criteria>

<output>
After completion, create `.planning/phases/53-investigation-benchmarking/53-01-SUMMARY.md`
</output>
