---
phase: 60-investigation-schema-design
plan: 01
type: execute
---

<objective>
Analyze current snapshot lifecycle and design historical odds retention strategy.

Purpose: Understand how snapshots are currently created, updated, and deleted to design a historical retention model that enables odds trend visualization while maintaining storage efficiency.
Output: DISCOVERY.md documenting current behavior, storage analysis, and recommended schema/strategy for historical tracking.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Key source files:
@src/db/models/odds.py
@src/db/models/competitor.py
@src/caching/change_detection.py
@src/caching/odds_cache.py
@src/services/cleanup.py
@src/db/models/settings.py

# Prior phase summaries (v2.0 cache/write pipeline):
@.planning/phases/54-in-memory-cache/54-01-SUMMARY.md
@.planning/phases/55-async-write-pipeline/55-01-SUMMARY.md

**Tech stack available:** SQLAlchemy 2.0, PostgreSQL, Alembic, frozen dataclasses for cache
**Established patterns:** Change detection via normalized outcome tuples, async write queue, cache-before-persist
</context>

<tasks>

<task type="auto">
  <name>Task 1: Analyze current snapshot lifecycle with SQL diagnostics</name>
  <files>.planning/phases/60-investigation-schema-design/DISCOVERY.md</files>
  <action>
Run SQL diagnostics against the database to understand current snapshot behavior:

1. **Snapshot creation pattern:**
   - Count snapshots per event over time (are multiple snapshots kept or overwritten?)
   - Check if `last_confirmed_at` is being used (indicates unchanged snapshots)
   - Analyze ratio of new snapshots vs confirmed-only updates

2. **Storage analysis:**
   - Count total snapshots, market_odds records
   - Calculate average markets per snapshot
   - Estimate storage per event per day (with current scrape frequency)
   - Project storage for 7/30/90 day retention

3. **Change frequency analysis:**
   - What % of scrape cycles result in actual odds changes?
   - How often do odds change for an event (avg changes/day)?
   - Identify patterns (more changes closer to kickoff?)

4. **Current retention behavior:**
   - Verify cleanup deletes by captured_at
   - Check if any historical data survives cleanup
   - Document what is lost when cleanup runs

SQL queries to run (via psql or SQLAlchemy):
```sql
-- Snapshots per event (do we keep history?)
SELECT event_id, COUNT(*) as snapshot_count
FROM odds_snapshots
GROUP BY event_id
ORDER BY snapshot_count DESC
LIMIT 20;

-- Recent snapshots with last_confirmed_at
SELECT id, event_id, captured_at, last_confirmed_at
FROM odds_snapshots
WHERE last_confirmed_at IS NOT NULL
ORDER BY captured_at DESC
LIMIT 20;

-- Storage size estimates
SELECT
  COUNT(*) as total_snapshots,
  (SELECT COUNT(*) FROM market_odds) as total_markets,
  ROUND(AVG(market_count), 1) as avg_markets_per_snap
FROM (
  SELECT s.id, COUNT(m.id) as market_count
  FROM odds_snapshots s
  LEFT JOIN market_odds m ON m.snapshot_id = s.id
  GROUP BY s.id
) sub;

-- Snapshot creation by hour (change frequency)
SELECT
  DATE_TRUNC('hour', captured_at) as hour,
  COUNT(*) as new_snapshots
FROM odds_snapshots
WHERE captured_at > NOW() - INTERVAL '24 hours'
GROUP BY 1
ORDER BY 1;
```

Document findings in the first section of DISCOVERY.md.
  </action>
  <verify>DISCOVERY.md exists with "Current State Analysis" section containing SQL results and interpretation</verify>
  <done>SQL analysis complete with snapshot count, storage estimates, and change frequency data documented</done>
</task>

<task type="auto">
  <name>Task 2: Design historical retention strategy and schema changes</name>
  <files>.planning/phases/60-investigation-schema-design/DISCOVERY.md</files>
  <action>
Based on the analysis from Task 1, design the historical retention approach. Add to DISCOVERY.md:

**Design considerations:**

1. **Retention options analysis:**
   - Option A: Keep ALL snapshots (simple, high storage)
   - Option B: Keep snapshots at fixed intervals (hourly, every N minutes)
   - Option C: Keep snapshots only when odds change (current behavior, but don't overwrite)
   - Option D: Hybrid - keep changes + periodic samples

2. **Schema changes needed:**
   - Do we need new tables or can we use existing schema?
   - Index requirements for efficient historical queries
   - Partitioning strategy for large datasets
   - Consider: snapshot_type enum (CHANGE, PERIODIC, MANUAL)?

3. **API query patterns:**
   - Query: "Get odds history for event X, market Y, outcome Z over last 24h"
   - Query: "Get margin history for event X over time"
   - Query: "Get all snapshots for event X between times T1 and T2"
   - Design efficient indexes for these patterns

4. **Integration with existing systems:**
   - How does this affect change detection (keep detecting, but always store)?
   - How does this affect cache (cache only latest, DB has history)?
   - How does cleanup change (keep more history, need new retention config)?
   - How does this affect write queue (always INSERT, never just UPDATE)?

5. **Storage projections:**
   - Based on Task 1 data, estimate storage for:
     - 100 events × 5 min scrape × 7 days = X snapshots
     - With 50% change rate = Y snapshots
     - Calculate MB/GB requirements

**Recommended approach:**
Document the recommended strategy with rationale. Include:
- Schema changes (new columns, indexes, or tables)
- Migration approach
- Configuration options (historical_retention_hours setting?)
- Phase 61 implementation outline
  </action>
  <verify>DISCOVERY.md has "Design Recommendations" section with schema proposal, storage projections, and implementation plan</verify>
  <done>Historical retention strategy documented with clear schema proposal ready for Phase 61 implementation</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] DISCOVERY.md created with two main sections
- [ ] SQL analysis results documented with interpretation
- [ ] Storage projections calculated from real data
- [ ] Schema changes clearly specified
- [ ] Recommended approach justified with tradeoffs
- [ ] Phase 61 can proceed directly from this design
</verification>

<success_criteria>

- DISCOVERY.md exists with comprehensive analysis
- Current snapshot lifecycle fully documented
- Historical retention strategy designed with clear rationale
- Schema changes specified (tables, columns, indexes)
- Storage projections based on real data
- No code changes in this phase (design only)
</success_criteria>

<output>
After completion, create `.planning/phases/60-investigation-schema-design/60-01-SUMMARY.md`:

# Phase 60 Plan 01: Investigation & Schema Design Summary

**[One-liner describing key findings and recommended approach]**

## Accomplishments

- [Key finding 1 from SQL analysis]
- [Key finding 2 - storage/change frequency]
- [Recommended historical retention strategy]

## Files Created/Modified

- `.planning/phases/60-investigation-schema-design/DISCOVERY.md` - Full analysis and design

## Decisions Made

- [Key design decision with rationale]

## Issues Encountered

[Any blockers or concerns discovered, or "None"]

## Next Phase Readiness

- DISCOVERY.md provides complete specification for Phase 61 implementation
- Schema changes ready to implement
- Storage projections inform retention configuration
</output>
