---
phase: 38-sr-id-parallel-scraping
plan: 01
type: execute
---

<objective>
Implement simultaneous multi-bookmaker scraping with batch processing using the EventCoordinator.

Purpose: Enable the EventCoordinator to fetch full event odds from all platforms in parallel for each event, capturing odds at the same moment for accurate trader comparison.

Output: Working scrape_batch() and _scrape_event_all_platforms() methods that fetch odds simultaneously from BetPawa, SportyBet, and Bet9ja for each event in priority order.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase summaries (dependency graph)
@.planning/phases/36-investigation-architecture-design/36-01-SUMMARY.md
@.planning/phases/37-event-coordination-layer/37-01-SUMMARY.md

# Architecture design document
@.planning/phases/36-investigation-architecture-design/ARCHITECTURE-DESIGN.md

# Key files to modify
@src/scraping/schemas/coordinator.py
@src/scraping/event_coordinator.py

# Reference for existing patterns
@src/scraping/competitor_events.py (fetch-then-store pattern, client usage)
@src/scraping/clients/betpawa.py (fetch_event API)
@src/scraping/clients/sportybet.py (fetch_event API)
@src/scraping/clients/bet9ja.py (fetch_event API)

**Tech stack available:**
- asyncio.Semaphore for concurrency control
- structlog for logging
- dataclasses for EventTarget (mutable during scrape)

**Established patterns:**
- EventCoordinator for event-centric scraping coordination
- Priority queue with (urgency, kickoff, -coverage, not_has_betpawa)
- Parallel discovery with platform-specific semaphore limits
- Fetch-then-store: parallel API calls, sequential DB writes (DB storage is Phase 39)

**Constraining decisions:**
- Phase 36: Semaphore limits - BetPawa 50, SportyBet 50, Bet9ja 15 + 25ms delay
- Phase 36: Batch size of 50 events for optimal throughput
- Phase 36: Sequential batches but parallel platforms within each event
- Phase 37: EventTarget uses dataclass (mutable for status/results updates)

**Key discovery from Phase 38 planning:**
- BetPawa uses platform-specific event IDs (from event_data["id"]), not SR IDs
- SportyBet uses SR ID format "sr:match:{sr_id}" for fetch_event
- Bet9ja uses platform-specific event ID (from event_data["ID"]), not EXTID (which is SR ID)
- Current EventTarget only stores sr_id, not platform-specific IDs needed for fetch_event calls
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend EventTarget with platform IDs and update discovery</name>
  <files>src/scraping/schemas/coordinator.py, src/scraping/event_coordinator.py</files>
  <action>
**In coordinator.py:**
1. Add `platform_ids: dict[str, str] = field(default_factory=dict)` to EventTarget dataclass
   - Maps platform name -> platform-specific event ID for API calls
   - Example: {"betpawa": "32299257", "sportybet": "sr:match:61300947", "bet9ja": "707096003"}

**In event_coordinator.py:**

2. Update `_parse_betpawa_event()` to also return the BetPawa event ID:
   - Extract `event_id = str(event_data.get("id", ""))`
   - Return `{"sr_id": sr_id, "kickoff": kickoff, "platform_id": event_id}`

3. Update `_parse_sportybet_event()` to return full SR format as platform ID:
   - The eventId field IS the platform ID: "sr:match:12345678"
   - Return `{"sr_id": sr_id, "kickoff": kickoff, "platform_id": event_id}` (event_id before stripping prefix)

4. Update `_parse_bet9ja_event()` to return the Bet9ja event ID:
   - Extract `event_id = str(event_data.get("ID", ""))` (the "ID" field, NOT "EXTID")
   - Return `{"sr_id": sr_id, "kickoff": kickoff, "platform_id": event_id}`

5. Update `discover_events()` merge logic to store platform IDs:
   - When creating new EventTarget: `platform_ids={platform: event["platform_id"]}`
   - When updating existing: `self._event_map[sr_id].platform_ids[platform] = event["platform_id"]`

**Important:** Preserve existing functionality - discovery should still work the same way, just with additional platform_id data captured.
  </action>
  <verify>
```bash
# Verify no syntax errors
python -c "from src.scraping.schemas.coordinator import EventTarget, ScrapeStatus, ScrapeBatch; print('Schema OK')"
python -c "from src.scraping.event_coordinator import EventCoordinator; print('Coordinator OK')"

# Verify EventTarget has new field
python -c "from src.scraping.schemas.coordinator import EventTarget; from datetime import datetime; e = EventTarget(sr_id='123', kickoff=datetime.now()); print(f'platform_ids type: {type(e.platform_ids)}, value: {e.platform_ids}')"
```
  </verify>
  <done>
- EventTarget has platform_ids dict field
- All three parse methods return platform_id
- discover_events stores platform IDs in EventTarget
- No syntax/import errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement scrape_batch and parallel platform scraping</name>
  <files>src/scraping/event_coordinator.py</files>
  <action>
**Add platform semaphore configuration at class level:**
```python
# Platform concurrency limits (from Phase 36 rate limit investigation)
PLATFORM_SEMAPHORES = {
    "betpawa": 50,
    "sportybet": 50,
    "bet9ja": 15,
}
BET9JA_DELAY_MS = 25  # 25ms delay after each Bet9ja request
```

**Implement `_scrape_event_all_platforms()` method:**
```python
async def _scrape_event_all_platforms(
    self,
    event: EventTarget,
    semaphores: dict[str, asyncio.Semaphore],
) -> dict:
    """Scrape single event from all available platforms in parallel.

    Args:
        event: EventTarget with platforms and platform_ids populated
        semaphores: Dict of platform -> Semaphore for concurrency control

    Returns:
        Dict with keys: data (platform->result), errors (platform->error), timing_ms
    """
```
- For each platform in event.platforms:
  - Get platform_id from event.platform_ids[platform]
  - Use semaphore for concurrency control
  - Call client.fetch_event(platform_id)
  - For Bet9ja, add 25ms sleep after fetch
  - Catch exceptions and store in errors dict
- Use asyncio.gather with return_exceptions=False (handle exceptions in wrapper)
- Track timing with time.perf_counter()
- Return {data: {platform: result}, errors: {platform: error_str}, timing_ms: int}

**Implement `scrape_batch()` async generator method:**
```python
async def scrape_batch(
    self,
    batch: ScrapeBatch,
) -> AsyncGenerator[dict, None]:
    """Scrape all events in batch, yielding progress for each.

    For each event:
    1. Mark as IN_PROGRESS
    2. Yield EVENT_SCRAPING progress
    3. Scrape all platforms in parallel
    4. Update event.results and event.errors
    5. Mark as COMPLETED or FAILED
    6. Yield EVENT_SCRAPED progress

    Yields:
        Progress dicts for SSE streaming (event_type, sr_id, platforms, timing_ms)
    """
```
- Create semaphores dict at start of method (reuse across all events in batch)
- Iterate through batch.events sequentially (priority order)
- For each event: update status, yield progress, scrape, update results, yield progress
- Event is COMPLETED if no errors, FAILED if any errors

**Progress event structure:**
```python
# EVENT_SCRAPING
{"event_type": "EVENT_SCRAPING", "sr_id": "123", "platforms_pending": ["betpawa", "sportybet"]}

# EVENT_SCRAPED
{"event_type": "EVENT_SCRAPED", "sr_id": "123", "platforms_scraped": ["betpawa"], "platforms_failed": ["sportybet"], "timing_ms": 245}
```

**Important considerations:**
- Do NOT store to DB in this phase - that's Phase 39
- Just populate event.results with the raw API responses
- Handle missing platform_ids gracefully (skip platform, log warning)
- If platform_id is missing, add to errors: "No platform ID available"
  </action>
  <verify>
```bash
# Verify no syntax errors
python -c "from src.scraping.event_coordinator import EventCoordinator; print('Import OK')"

# Verify methods exist
python -c "from src.scraping.event_coordinator import EventCoordinator; print('scrape_batch' in dir(EventCoordinator), '_scrape_event_all_platforms' in dir(EventCoordinator))"

# Verify semaphore constants exist
python -c "from src.scraping.event_coordinator import PLATFORM_SEMAPHORES, BET9JA_DELAY_MS; print(f'Semaphores: {PLATFORM_SEMAPHORES}, Delay: {BET9JA_DELAY_MS}ms')"
```
  </verify>
  <done>
- PLATFORM_SEMAPHORES and BET9JA_DELAY_MS constants defined
- _scrape_event_all_platforms() implemented with semaphore control
- scrape_batch() async generator implemented
- Progress events yield correct structure
- Event status updated to IN_PROGRESS/COMPLETED/FAILED
- Event results and errors populated
- No syntax/import errors
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `python -c "from src.scraping.event_coordinator import EventCoordinator"` succeeds
- [ ] `python -c "from src.scraping.schemas.coordinator import EventTarget; e = EventTarget(sr_id='x', kickoff=__import__('datetime').datetime.now()); assert hasattr(e, 'platform_ids')"` succeeds
- [ ] EventTarget.platform_ids is a dict
- [ ] All three parse methods return platform_id key
- [ ] scrape_batch is an async generator (yields dicts)
- [ ] _scrape_event_all_platforms handles all three platforms
- [ ] Semaphore limits match Phase 36 design (50/50/15)
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- EventCoordinator can scrape events in parallel across platforms
- Progress events suitable for SSE streaming
</success_criteria>

<output>
After completion, create `.planning/phases/38-sr-id-parallel-scraping/38-01-SUMMARY.md`:

# Phase 38 Plan 01: SR ID Parallel Scraping Summary

**[Substantive one-liner]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `src/scraping/schemas/coordinator.py` - Added platform_ids field
- `src/scraping/event_coordinator.py` - Added scrape_batch, _scrape_event_all_platforms

## Decisions Made

[Key decisions and rationale]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Phase Readiness

Ready for Phase 39: Batch DB Storage
</output>
