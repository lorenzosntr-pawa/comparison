---
phase: 103-data-migration-validation
plan: 01
type: execute
---

<objective>
Reclaim disk space and apply retention policies to reduce database from 63 GB to target <15 GB.

Purpose: Complete the storage optimization by reclaiming space from dropped raw_response columns (33 GB) and applying 7-day retention to purge old data (~16 GB additional savings).
Output: Database reduced to under 15 GB with all features validated.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/100-investigation-analysis/100-01-SUMMARY.md
@.planning/phases/101-schema-implementation/101-01-SUMMARY.md
@.planning/phases/102-scraping-verification/102-01-SUMMARY.md
@scripts/profile_database.py

**Tech stack available:**
- PostgreSQL with VACUUM FULL for space reclamation
- Existing cleanup API (POST /api/cleanup/execute)
- Existing profiling script (scripts/profile_database.py)

**Constraining decisions:**
- [Phase 100]: raw_response columns are UNUSED after scraping - safe to remove (saves 33 GB)
- [Phase 100]: Combined strategy: remove raw_response + 7-day retention = 78% reduction
- [Phase 101]: raw_response columns dropped from schema via Alembic migration
- [Phase 102]: Scraping pipeline verified working after schema changes

**Expected results:**
- Phase 101 dropped columns: reclaim 33 GB
- 7-day retention: additional ~16 GB savings
- Final target: ~14 GB (under 15 GB target)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Reclaim space from dropped columns</name>
  <files>N/A (database operation)</files>
  <action>
Run VACUUM FULL on odds_snapshots and competitor_odds_snapshots tables to reclaim the disk space from the dropped raw_response columns. VACUUM FULL rewrites the entire table and reclaims all dead tuple space.

Execute via psql:
```sql
VACUUM FULL odds_snapshots;
VACUUM FULL competitor_odds_snapshots;
```

Note: VACUUM FULL requires exclusive lock - run during low usage. This is a one-time operation after the column drop.

After VACUUM, also run VACUUM FULL on dependent tables that may have bloat from the batch deletions coming in Task 2:
```sql
VACUUM FULL market_odds;
VACUUM FULL competitor_market_odds;
```
  </action>
  <verify>psql -c "SELECT pg_size_pretty(pg_total_relation_size('odds_snapshots'))" shows reduced size (should drop from ~24 GB to ~1-2 GB)</verify>
  <done>odds_snapshots and competitor_odds_snapshots tables have reclaimed space from dropped columns</done>
</task>

<task type="auto">
  <name>Task 2: Execute retention cleanup with 7-day policy</name>
  <files>N/A (API operation)</files>
  <action>
Call the existing cleanup API with 7-day retention to delete data older than 7 days.

First, preview what will be deleted:
```bash
curl -s http://localhost:8000/api/cleanup/preview?odds_days=7&match_days=7 | python -m json.tool
```

Then execute the cleanup:
```bash
curl -X POST http://localhost:8000/api/cleanup/execute \
  -H "Content-Type: application/json" \
  -d '{"odds_retention_days": 7, "match_retention_days": 7}'
```

The cleanup service handles deletion in correct FK order:
1. market_odds → odds_snapshots
2. competitor_market_odds → competitor_odds_snapshots
3. scrape_errors, scrape_phase_logs → scrape_runs → scrape_batches
4. event_bookmakers → events
5. competitor_events
6. orphaned tournaments

After cleanup completes, run VACUUM on all affected tables to reclaim space:
```sql
VACUUM FULL market_odds;
VACUUM FULL competitor_market_odds;
VACUUM FULL scrape_runs;
VACUUM FULL scrape_batches;
VACUUM FULL events;
VACUUM FULL competitor_events;
VACUUM FULL tournaments;
VACUUM FULL competitor_tournaments;
```

Note: Ensure the backend is running (python -m uvicorn src.api.app:app --host 0.0.0.0 --port 8000) before executing.
  </action>
  <verify>curl http://localhost:8000/api/cleanup/history shows successful cleanup run with counts</verify>
  <done>Cleanup executed with 7-day retention, all data older than 7 days deleted</done>
</task>

<task type="auto">
  <name>Task 3: Verify database size reduction</name>
  <files>scripts/profile_database.py</files>
  <action>
Run the existing profiling script to verify database size is under target:

```bash
cd backend && python scripts/profile_database.py
```

Expected results after both operations:
- odds_snapshots: ~24 GB → <2 GB (raw_response + retention)
- competitor_odds_snapshots: ~11 GB → <1 GB
- market_odds: ~22 GB → <3 GB (7-day retention)
- competitor_market_odds: ~5.6 GB → <1 GB
- Total: 63 GB → <15 GB (target)

If total exceeds 15 GB, document the breakdown and assess if additional retention tightening is needed.
  </action>
  <verify>Profile output shows total database size under 15 GB</verify>
  <done>Database size verified under 15 GB target, breakdown documented</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Database reduced from 63 GB to under 15 GB through space reclamation and retention cleanup</what-built>
  <how-to-verify>
    1. Run: cd backend && python -m uvicorn src.api.app:app --host 0.0.0.0 --port 8000
    2. Run: cd frontend && npm run dev
    3. Visit: http://localhost:5173/
    4. Test Odds Comparison page: Events display with odds data
    5. Test Event Details: Click an event, verify market odds show
    6. Test Historical Analysis: Navigate to page, verify tournament list loads
    7. Test Coverage page: Verify coverage stats display correctly
    8. Trigger a manual scrape: Verify new data comes in
    9. Check WebSocket connection: Should show "Connected" indicator
  </how-to-verify>
  <resume-signal>Type "approved" to complete phase, or describe any issues found</resume-signal>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] VACUUM FULL completed on snapshot tables
- [ ] Cleanup executed with 7-day retention
- [ ] Database size verified under 15 GB
- [ ] All UI features working (odds, events, history, coverage)
- [ ] Scraping pipeline operational
</verification>

<success_criteria>

- Database size reduced from 63 GB to under 15 GB
- All existing features continue working
- Scraping pipeline operational
- No data integrity issues
  </success_criteria>

<output>
After completion, create `.planning/phases/103-data-migration-validation/103-01-SUMMARY.md`:

# Phase 103 Plan 01: Data Migration & Validation Summary

**[One-liner: what was achieved]**

## Performance

- **Duration:** X min
- **Tasks:** 4 (3 auto + 1 checkpoint)

## Accomplishments

- Space reclaimed from dropped raw_response columns
- 7-day retention applied, old data purged
- Database reduced from 63 GB to X GB
- All features validated working

## Database Size Comparison

| Table | Before | After |
|-------|--------|-------|
| odds_snapshots | 24 GB | X GB |
| competitor_odds_snapshots | 11 GB | X GB |
| market_odds | 22 GB | X GB |
| competitor_market_odds | 5.6 GB | X GB |
| **Total** | **63 GB** | **X GB** |

## Decisions Made

[Any decisions during execution]

## Issues Encountered

[Any issues and resolutions]

## Next Phase Readiness

Ready for Phase 104: Monitoring & Prevention
</output>
